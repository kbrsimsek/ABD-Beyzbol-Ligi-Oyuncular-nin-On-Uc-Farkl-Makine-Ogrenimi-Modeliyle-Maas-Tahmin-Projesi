{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ABD Beyzbol Ligi Oyuncularının On Üç Farklı Makine Öğrenimi Modeliyle Maaş Tahmin Projesi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu projede, herhangi bir ABD Beyzbol Ligi oyuncusunun maaşını tahmin etmek için on üç farklı makine öğrenimi modeli kullanılacaktır.Aşağıda açıklanan Hitters verileri, beyzbol oyuncularının maaşlarını tahmin etmek için kullanılacaktır.Veriler \"https://www.kaggle.com\" adresinden alınacaktır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Açıklama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baglantı"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu veri seti, R-package ISLR'nin bir parçasıdır ve ilgili kitapta G. James ve diğerleri tarafından kullanılmaktadır. (2013) Ridge regresyonunun ve LASSO'nun R kullanılarak nasıl gerçekleştirildiğini göstermek için \"R'deki uygulamalarla İstatistiksel Öğrenmeye Giriş\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### İçerik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu veri seti orijinal olarak Carnegie Mellon Üniversitesi'nde tutulan StatLib kütüphanesinden alınmıştır.Bu, 1988 ASA Grafik Bölümü Poster Oturumunda kullanılan verilerin bir parçasıdır.Maaş verileri aslen Sports Illustrated, 20 Nisan 1987'den alınmıştır.1986 ve kariyer istatistikleri, Collier Books, Macmillan Publishing Company, New York tarafından yayınlanan 1987 Beyzbol Ansiklopedisi Güncellemesinden elde edildi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aşağıdaki 20 değişken üzerinde büyük lig oyuncularının 322 gözlemini içeren bir veri çerçevesi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AtBat 1986'da vuruş sayısı\n",
    "- Hits 1986'daki isabet sayısı\n",
    "- HmRun 1986'da ev sahibi koşu sayısı\n",
    "- Runs 1986 yılındaki koşu sayısı\n",
    "- RBI 1986 yılında atışla birlikte yapılan koşu sayısı\n",
    "- Walks 1986'daki yürüyüş sayısı\n",
    "- Years büyük liglerdeki yıl sayısı\n",
    "- CAtBat kariyeri boyunca vuruş sayısı\n",
    "- CHits kariyeri boyunca isabet sayısı\n",
    "- CHmRun kariyeri boyunca ev sahibi koşu sayısı\n",
    "- CRuns kariyeri boyunca koşu sayısı\n",
    "- CRBI kariyeri boyunca atışla birlikte yapılan koşu sayısı\n",
    "- CWalks kariyeri boyunca yaptığı yürüyüş sayısı\n",
    "- A ve N seviyelerine sahip A Ligi faktörü, 1986 sonunda oyuncunun ligini gösterir.\n",
    "- 1986 sonunda oyuncunun bölünmesini gösteren E ve W seviyelerine sahip Division A faktörü\n",
    "- PutOut'lar 1986'da dışarı çıkanların sayısı\n",
    "- Asist 1986'daki asist sayısı\n",
    "- Errors 1986'daki hata sayısı\n",
    "- Salary 1987 açılış gününde binlerce dolar yıllık maaş\n",
    "- NewLeague 1987'nin başında oyuncunun ligini gösteren A ve N seviyelerine sahip A faktörü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\toshiba\\anaconda3\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\toshiba\\anaconda3\\lib\\site-packages (from xgboost) (1.5.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\toshiba\\anaconda3\\lib\\site-packages (from xgboost) (1.18.5)\n",
      "Requirement already satisfied: lightgbm in c:\\users\\toshiba\\anaconda3\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\toshiba\\anaconda3\\lib\\site-packages (from lightgbm) (0.23.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\toshiba\\anaconda3\\lib\\site-packages (from lightgbm) (1.5.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\toshiba\\anaconda3\\lib\\site-packages (from lightgbm) (1.18.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\toshiba\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\toshiba\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (0.16.0)\n",
      "Requirement already satisfied: catboost in c:\\users\\toshiba\\anaconda3\\lib\\site-packages (0.24.2)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\toshiba\\anaconda3\\lib\\site-packages (from catboost) (1.0.5)\n",
      "Requirement already satisfied: six in c:\\users\\toshiba\\anaconda3\\lib\\site-packages (from catboost) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\toshiba\\anaconda3\\lib\\site-packages (from catboost) (1.18.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\toshiba\\anaconda3\\lib\\site-packages (from catboost) (1.5.0)\n",
      "Requirement already satisfied: graphviz in c:\\users\\toshiba\\anaconda3\\lib\\site-packages (from catboost) (0.14.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\toshiba\\anaconda3\\lib\\site-packages (from catboost) (3.2.2)\n",
      "Requirement already satisfied: plotly in c:\\users\\toshiba\\anaconda3\\lib\\site-packages (from catboost) (4.12.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\toshiba\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->catboost) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\toshiba\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\toshiba\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\toshiba\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\toshiba\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.2.0)\n",
      "Requirement already satisfied: retrying>=1.3.3 in c:\\users\\toshiba\\anaconda3\\lib\\site-packages (from plotly->catboost) (1.3.3)\n"
     ]
    }
   ],
   "source": [
    "# Gerekli algoritmaları yükleme\n",
    "\n",
    "!pip install xgboost\n",
    "!pip install lightgbm\n",
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gerekli kütüphaneleri yükleme \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn import neighbors\n",
    "from sklearn.neighbors import LocalOutlierFactor, KNeighborsRegressor\n",
    "from sklearn.preprocessing import scale, StandardScaler, RobustScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Kaggle sunucusundan Hitters verilerini okuma\n",
    "\n",
    "df = pd.read_csv(\"Hitters.csv\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veriyi Anlamak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>CHmRun</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>League</th>\n",
       "      <th>Division</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Errors</th>\n",
       "      <th>Salary</th>\n",
       "      <th>NewLeague</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>293</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>293</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>446</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>315</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>3449</td>\n",
       "      <td>835</td>\n",
       "      <td>69</td>\n",
       "      <td>321</td>\n",
       "      <td>414</td>\n",
       "      <td>375</td>\n",
       "      <td>N</td>\n",
       "      <td>W</td>\n",
       "      <td>632</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>475.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>479</td>\n",
       "      <td>130</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>1624</td>\n",
       "      <td>457</td>\n",
       "      <td>63</td>\n",
       "      <td>224</td>\n",
       "      <td>266</td>\n",
       "      <td>263</td>\n",
       "      <td>A</td>\n",
       "      <td>W</td>\n",
       "      <td>880</td>\n",
       "      <td>82</td>\n",
       "      <td>14</td>\n",
       "      <td>480.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>496</td>\n",
       "      <td>141</td>\n",
       "      <td>20</td>\n",
       "      <td>65</td>\n",
       "      <td>78</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>5628</td>\n",
       "      <td>1575</td>\n",
       "      <td>225</td>\n",
       "      <td>828</td>\n",
       "      <td>838</td>\n",
       "      <td>354</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>200</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>500.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321</td>\n",
       "      <td>87</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>396</td>\n",
       "      <td>101</td>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>46</td>\n",
       "      <td>33</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>805</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>91.5</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  CRuns  \\\n",
       "0    293    66      1    30   29     14      1     293     66       1     30   \n",
       "1    315    81      7    24   38     39     14    3449    835      69    321   \n",
       "2    479   130     18    66   72     76      3    1624    457      63    224   \n",
       "3    496   141     20    65   78     37     11    5628   1575     225    828   \n",
       "4    321    87     10    39   42     30      2     396    101      12     48   \n",
       "\n",
       "   CRBI  CWalks League Division  PutOuts  Assists  Errors  Salary NewLeague  \n",
       "0    29      14      A        E      446       33      20     NaN         A  \n",
       "1   414     375      N        W      632       43      10   475.0         N  \n",
       "2   266     263      A        W      880       82      14   480.0         A  \n",
       "3   838     354      N        E      200       11       3   500.0         N  \n",
       "4    46      33      N        E      805       40       4    91.5         N  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 322 entries, 0 to 321\n",
      "Data columns (total 20 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   AtBat      322 non-null    int64  \n",
      " 1   Hits       322 non-null    int64  \n",
      " 2   HmRun      322 non-null    int64  \n",
      " 3   Runs       322 non-null    int64  \n",
      " 4   RBI        322 non-null    int64  \n",
      " 5   Walks      322 non-null    int64  \n",
      " 6   Years      322 non-null    int64  \n",
      " 7   CAtBat     322 non-null    int64  \n",
      " 8   CHits      322 non-null    int64  \n",
      " 9   CHmRun     322 non-null    int64  \n",
      " 10  CRuns      322 non-null    int64  \n",
      " 11  CRBI       322 non-null    int64  \n",
      " 12  CWalks     322 non-null    int64  \n",
      " 13  League     322 non-null    object \n",
      " 14  Division   322 non-null    object \n",
      " 15  PutOuts    322 non-null    int64  \n",
      " 16  Assists    322 non-null    int64  \n",
      " 17  Errors     322 non-null    int64  \n",
      " 18  Salary     263 non-null    float64\n",
      " 19  NewLeague  322 non-null    object \n",
      "dtypes: float64(1), int64(16), object(3)\n",
      "memory usage: 50.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AtBat</th>\n",
       "      <td>322.0</td>\n",
       "      <td>380.928571</td>\n",
       "      <td>153.404981</td>\n",
       "      <td>16.0</td>\n",
       "      <td>255.25</td>\n",
       "      <td>379.5</td>\n",
       "      <td>512.00</td>\n",
       "      <td>687.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hits</th>\n",
       "      <td>322.0</td>\n",
       "      <td>101.024845</td>\n",
       "      <td>46.454741</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.00</td>\n",
       "      <td>96.0</td>\n",
       "      <td>137.00</td>\n",
       "      <td>238.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HmRun</th>\n",
       "      <td>322.0</td>\n",
       "      <td>10.770186</td>\n",
       "      <td>8.709037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.00</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Runs</th>\n",
       "      <td>322.0</td>\n",
       "      <td>50.909938</td>\n",
       "      <td>26.024095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.25</td>\n",
       "      <td>48.0</td>\n",
       "      <td>69.00</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RBI</th>\n",
       "      <td>322.0</td>\n",
       "      <td>48.027950</td>\n",
       "      <td>26.166895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.00</td>\n",
       "      <td>44.0</td>\n",
       "      <td>64.75</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Walks</th>\n",
       "      <td>322.0</td>\n",
       "      <td>38.742236</td>\n",
       "      <td>21.639327</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.00</td>\n",
       "      <td>35.0</td>\n",
       "      <td>53.00</td>\n",
       "      <td>105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Years</th>\n",
       "      <td>322.0</td>\n",
       "      <td>7.444099</td>\n",
       "      <td>4.926087</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CAtBat</th>\n",
       "      <td>322.0</td>\n",
       "      <td>2648.683230</td>\n",
       "      <td>2324.205870</td>\n",
       "      <td>19.0</td>\n",
       "      <td>816.75</td>\n",
       "      <td>1928.0</td>\n",
       "      <td>3924.25</td>\n",
       "      <td>14053.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHits</th>\n",
       "      <td>322.0</td>\n",
       "      <td>717.571429</td>\n",
       "      <td>654.472627</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.00</td>\n",
       "      <td>508.0</td>\n",
       "      <td>1059.25</td>\n",
       "      <td>4256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHmRun</th>\n",
       "      <td>322.0</td>\n",
       "      <td>69.490683</td>\n",
       "      <td>86.266061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.00</td>\n",
       "      <td>37.5</td>\n",
       "      <td>90.00</td>\n",
       "      <td>548.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRuns</th>\n",
       "      <td>322.0</td>\n",
       "      <td>358.795031</td>\n",
       "      <td>334.105886</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.25</td>\n",
       "      <td>247.0</td>\n",
       "      <td>526.25</td>\n",
       "      <td>2165.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRBI</th>\n",
       "      <td>322.0</td>\n",
       "      <td>330.118012</td>\n",
       "      <td>333.219617</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.75</td>\n",
       "      <td>220.5</td>\n",
       "      <td>426.25</td>\n",
       "      <td>1659.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWalks</th>\n",
       "      <td>322.0</td>\n",
       "      <td>260.239130</td>\n",
       "      <td>267.058085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67.25</td>\n",
       "      <td>170.5</td>\n",
       "      <td>339.25</td>\n",
       "      <td>1566.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PutOuts</th>\n",
       "      <td>322.0</td>\n",
       "      <td>288.937888</td>\n",
       "      <td>280.704614</td>\n",
       "      <td>0.0</td>\n",
       "      <td>109.25</td>\n",
       "      <td>212.0</td>\n",
       "      <td>325.00</td>\n",
       "      <td>1378.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Assists</th>\n",
       "      <td>322.0</td>\n",
       "      <td>106.913043</td>\n",
       "      <td>136.854876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>39.5</td>\n",
       "      <td>166.00</td>\n",
       "      <td>492.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Errors</th>\n",
       "      <td>322.0</td>\n",
       "      <td>8.040373</td>\n",
       "      <td>6.368359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Salary</th>\n",
       "      <td>263.0</td>\n",
       "      <td>535.925882</td>\n",
       "      <td>451.118681</td>\n",
       "      <td>67.5</td>\n",
       "      <td>190.00</td>\n",
       "      <td>425.0</td>\n",
       "      <td>750.00</td>\n",
       "      <td>2460.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count         mean          std   min     25%     50%      75%  \\\n",
       "AtBat    322.0   380.928571   153.404981  16.0  255.25   379.5   512.00   \n",
       "Hits     322.0   101.024845    46.454741   1.0   64.00    96.0   137.00   \n",
       "HmRun    322.0    10.770186     8.709037   0.0    4.00     8.0    16.00   \n",
       "Runs     322.0    50.909938    26.024095   0.0   30.25    48.0    69.00   \n",
       "RBI      322.0    48.027950    26.166895   0.0   28.00    44.0    64.75   \n",
       "Walks    322.0    38.742236    21.639327   0.0   22.00    35.0    53.00   \n",
       "Years    322.0     7.444099     4.926087   1.0    4.00     6.0    11.00   \n",
       "CAtBat   322.0  2648.683230  2324.205870  19.0  816.75  1928.0  3924.25   \n",
       "CHits    322.0   717.571429   654.472627   4.0  209.00   508.0  1059.25   \n",
       "CHmRun   322.0    69.490683    86.266061   0.0   14.00    37.5    90.00   \n",
       "CRuns    322.0   358.795031   334.105886   1.0  100.25   247.0   526.25   \n",
       "CRBI     322.0   330.118012   333.219617   0.0   88.75   220.5   426.25   \n",
       "CWalks   322.0   260.239130   267.058085   0.0   67.25   170.5   339.25   \n",
       "PutOuts  322.0   288.937888   280.704614   0.0  109.25   212.0   325.00   \n",
       "Assists  322.0   106.913043   136.854876   0.0    7.00    39.5   166.00   \n",
       "Errors   322.0     8.040373     6.368359   0.0    3.00     6.0    11.00   \n",
       "Salary   263.0   535.925882   451.118681  67.5  190.00   425.0   750.00   \n",
       "\n",
       "             max  \n",
       "AtBat      687.0  \n",
       "Hits       238.0  \n",
       "HmRun       40.0  \n",
       "Runs       130.0  \n",
       "RBI        121.0  \n",
       "Walks      105.0  \n",
       "Years       24.0  \n",
       "CAtBat   14053.0  \n",
       "CHits     4256.0  \n",
       "CHmRun     548.0  \n",
       "CRuns     2165.0  \n",
       "CRBI      1659.0  \n",
       "CWalks    1566.0  \n",
       "PutOuts   1378.0  \n",
       "Assists    492.0  \n",
       "Errors      32.0  \n",
       "Salary    2460.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(322, 20)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtBat         0\n",
       "Hits          0\n",
       "HmRun         0\n",
       "Runs          0\n",
       "RBI           0\n",
       "Walks         0\n",
       "Years         0\n",
       "CAtBat        0\n",
       "CHits         0\n",
       "CHmRun        0\n",
       "CRuns         0\n",
       "CRBI          0\n",
       "CWalks        0\n",
       "League        0\n",
       "Division      0\n",
       "PutOuts       0\n",
       "Assists       0\n",
       "Errors        0\n",
       "Salary       59\n",
       "NewLeague     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Eksik değerleri bulma\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-03ca40eb8aff>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-03ca40eb8aff>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    conda install -c conda-forge/label/cf202003 missingno\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### Eksik değerleri görselleştirmek için aşağıdaki paketi yüklemem gerekiyor\n",
    "\n",
    "conda install -c conda-forge/label/cf202003 missingno\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Eksik değerleri görselleştirme\n",
    "\n",
    "import missingno as msno\n",
    "msno.bar(df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Özellikler arasında 0,5'ten büyük korelasyon değerleri (>0,5 olduğu için sadece 0,5'in üzerindeki değerleri görebiliyorum)\n",
    "\n",
    "correlation_matrix = df.corr().round(2)\n",
    "filtre=np.abs(correlation_matrix['Salary'])>0.50\n",
    "corr_features=correlation_matrix.columns[filtre].tolist()\n",
    "sns.clustermap(df[corr_features].corr(),annot=True,fmt=\".2f\")\n",
    "plt.title('Correlation btw features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bazı değişkenler arasında çok yüksek korelasyon olmasına rağmen hiçbir şey yapmayacağım. Normalde bu sorun çözülmelidir.\n",
    "# Burada eksik değerleri sileceğim\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(263, 20)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>CHmRun</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>League</th>\n",
       "      <th>Division</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Errors</th>\n",
       "      <th>Salary</th>\n",
       "      <th>NewLeague</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>495</td>\n",
       "      <td>151</td>\n",
       "      <td>17</td>\n",
       "      <td>61</td>\n",
       "      <td>84</td>\n",
       "      <td>78</td>\n",
       "      <td>10</td>\n",
       "      <td>5624</td>\n",
       "      <td>1679</td>\n",
       "      <td>275</td>\n",
       "      <td>884</td>\n",
       "      <td>1015</td>\n",
       "      <td>709</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>1045</td>\n",
       "      <td>88</td>\n",
       "      <td>13</td>\n",
       "      <td>2460.000</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>618</td>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "      <td>98</td>\n",
       "      <td>110</td>\n",
       "      <td>62</td>\n",
       "      <td>13</td>\n",
       "      <td>7127</td>\n",
       "      <td>2163</td>\n",
       "      <td>351</td>\n",
       "      <td>1104</td>\n",
       "      <td>1289</td>\n",
       "      <td>564</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>330</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2412.500</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>78</td>\n",
       "      <td>220</td>\n",
       "      <td>6</td>\n",
       "      <td>2127.333</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>677</td>\n",
       "      <td>238</td>\n",
       "      <td>31</td>\n",
       "      <td>117</td>\n",
       "      <td>113</td>\n",
       "      <td>53</td>\n",
       "      <td>5</td>\n",
       "      <td>2223</td>\n",
       "      <td>737</td>\n",
       "      <td>93</td>\n",
       "      <td>349</td>\n",
       "      <td>401</td>\n",
       "      <td>171</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>1377</td>\n",
       "      <td>100</td>\n",
       "      <td>6</td>\n",
       "      <td>1975.000</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>514</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>54</td>\n",
       "      <td>79</td>\n",
       "      <td>9</td>\n",
       "      <td>4739</td>\n",
       "      <td>1169</td>\n",
       "      <td>13</td>\n",
       "      <td>583</td>\n",
       "      <td>374</td>\n",
       "      <td>528</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>229</td>\n",
       "      <td>453</td>\n",
       "      <td>15</td>\n",
       "      <td>1940.000</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  \\\n",
       "100    495   151     17    61   84     78     10    5624   1679     275   \n",
       "163    618   200     20    98  110     62     13    7127   2163     351   \n",
       "217     20     1      0     0    0      0      2      41      9       2   \n",
       "82     677   238     31   117  113     53      5    2223    737      93   \n",
       "229    514   144      0    67   54     79      9    4739   1169      13   \n",
       "\n",
       "     CRuns  CRBI  CWalks League Division  PutOuts  Assists  Errors    Salary  \\\n",
       "100    884  1015     709      A        E     1045       88      13  2460.000   \n",
       "163   1104  1289     564      A        E      330       16       8  2412.500   \n",
       "217      6     7       4      N        E       78      220       6  2127.333   \n",
       "82     349   401     171      A        E     1377      100       6  1975.000   \n",
       "229    583   374     528      N        E      229      453      15  1940.000   \n",
       "\n",
       "    NewLeague  \n",
       "100         A  \n",
       "163         A  \n",
       "217         N  \n",
       "82          A  \n",
       "229         N  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values('Salary', ascending = False).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A    139\n",
       "N    124\n",
       "Name: League, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 kategorik değişkenim var\n",
    "\n",
    "df['League'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A    141\n",
       "N    122\n",
       "Name: NewLeague, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['NewLeague'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "W    134\n",
       "E    129\n",
       "Name: Division, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Division'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nominal değişkenleri tek bir sıcak kodlama yöntemiyle dönüştürme. Normalde, kukla değişkenler için etiket kodlama değişkeni uygulanabilir. 3 veya daha fazla kategoriye sahip nominal değişkenler için bir sıcak kodlama uygundur\n",
    "\n",
    "df = pd.get_dummies(df, columns = ['League', 'Division', 'NewLeague'], drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.22764656, -4.12325749, -3.14372723, -2.46482586, -2.37903838,\n",
       "       -1.93271815, -1.92868899, -1.90888428, -1.6775256 , -1.66536304,\n",
       "       -1.62626849, -1.48361164, -1.48216262, -1.44345727, -1.43603775,\n",
       "       -1.42239393, -1.40734739, -1.38353101, -1.37252134, -1.3532077 ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aykırı değerleri tespit etmek için LocalOutlierFactor kullanacağım. 20 ve 'auto' varsayılan değerlerini kullanacağım.\n",
    "\n",
    "clf=LocalOutlierFactor(n_neighbors=20, contamination='auto')\n",
    "clf.fit_predict(df)\n",
    "df_scores=clf.negative_outlier_factor_\n",
    "df_scores= np.sort(df_scores)\n",
    "df_scores[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mLocalOutlierFactor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0malgorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mleaf_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'minkowski'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmetric_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcontamination\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnovelty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Unsupervised Outlier Detection using Local Outlier Factor (LOF)\n",
       "\n",
       "The anomaly score of each sample is called Local Outlier Factor.\n",
       "It measures the local deviation of density of a given sample with\n",
       "respect to its neighbors.\n",
       "It is local in that the anomaly score depends on how isolated the object\n",
       "is with respect to the surrounding neighborhood.\n",
       "More precisely, locality is given by k-nearest neighbors, whose distance\n",
       "is used to estimate the local density.\n",
       "By comparing the local density of a sample to the local densities of\n",
       "its neighbors, one can identify samples that have a substantially lower\n",
       "density than their neighbors. These are considered outliers.\n",
       "\n",
       ".. versionadded:: 0.19\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "n_neighbors : int, default=20\n",
       "    Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
       "    If n_neighbors is larger than the number of samples provided,\n",
       "    all samples will be used.\n",
       "\n",
       "algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
       "    Algorithm used to compute the nearest neighbors:\n",
       "\n",
       "    - 'ball_tree' will use :class:`BallTree`\n",
       "    - 'kd_tree' will use :class:`KDTree`\n",
       "    - 'brute' will use a brute-force search.\n",
       "    - 'auto' will attempt to decide the most appropriate algorithm\n",
       "      based on the values passed to :meth:`fit` method.\n",
       "\n",
       "    Note: fitting on sparse input will override the setting of\n",
       "    this parameter, using brute force.\n",
       "\n",
       "leaf_size : int, default=30\n",
       "    Leaf size passed to :class:`BallTree` or :class:`KDTree`. This can\n",
       "    affect the speed of the construction and query, as well as the memory\n",
       "    required to store the tree. The optimal value depends on the\n",
       "    nature of the problem.\n",
       "\n",
       "metric : str or callable, default='minkowski'\n",
       "    metric used for the distance computation. Any metric from scikit-learn\n",
       "    or scipy.spatial.distance can be used.\n",
       "\n",
       "    If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
       "    must be square. X may be a sparse matrix, in which case only \"nonzero\"\n",
       "    elements may be considered neighbors.\n",
       "\n",
       "    If metric is a callable function, it is called on each\n",
       "    pair of instances (rows) and the resulting value recorded. The callable\n",
       "    should take two arrays as input and return one value indicating the\n",
       "    distance between them. This works for Scipy's metrics, but is less\n",
       "    efficient than passing the metric name as a string.\n",
       "\n",
       "    Valid values for metric are:\n",
       "\n",
       "    - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
       "      'manhattan']\n",
       "\n",
       "    - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
       "      'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
       "      'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
       "      'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
       "      'yule']\n",
       "\n",
       "    See the documentation for scipy.spatial.distance for details on these\n",
       "    metrics:\n",
       "    https://docs.scipy.org/doc/scipy/reference/spatial.distance.html\n",
       "\n",
       "p : int, default=2\n",
       "    Parameter for the Minkowski metric from\n",
       "    :func:`sklearn.metrics.pairwise.pairwise_distances`. When p = 1, this\n",
       "    is equivalent to using manhattan_distance (l1), and euclidean_distance\n",
       "    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
       "\n",
       "metric_params : dict, default=None\n",
       "    Additional keyword arguments for the metric function.\n",
       "\n",
       "contamination : 'auto' or float, default='auto'\n",
       "    The amount of contamination of the data set, i.e. the proportion\n",
       "    of outliers in the data set. When fitting this is used to define the\n",
       "    threshold on the scores of the samples.\n",
       "\n",
       "    - if 'auto', the threshold is determined as in the\n",
       "      original paper,\n",
       "    - if a float, the contamination should be in the range [0, 0.5].\n",
       "\n",
       "    .. versionchanged:: 0.22\n",
       "       The default value of ``contamination`` changed from 0.1\n",
       "       to ``'auto'``.\n",
       "\n",
       "novelty : bool, default=False\n",
       "    By default, LocalOutlierFactor is only meant to be used for outlier\n",
       "    detection (novelty=False). Set novelty to True if you want to use\n",
       "    LocalOutlierFactor for novelty detection. In this case be aware that\n",
       "    that you should only use predict, decision_function and score_samples\n",
       "    on new unseen data and not on the training set.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "n_jobs : int, default=None\n",
       "    The number of parallel jobs to run for neighbors search.\n",
       "    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
       "    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
       "    for more details.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "negative_outlier_factor_ : ndarray of shape (n_samples,)\n",
       "    The opposite LOF of the training samples. The higher, the more normal.\n",
       "    Inliers tend to have a LOF score close to 1\n",
       "    (``negative_outlier_factor_`` close to -1), while outliers tend to have\n",
       "    a larger LOF score.\n",
       "\n",
       "    The local outlier factor (LOF) of a sample captures its\n",
       "    supposed 'degree of abnormality'.\n",
       "    It is the average of the ratio of the local reachability density of\n",
       "    a sample and those of its k-nearest neighbors.\n",
       "\n",
       "n_neighbors_ : int\n",
       "    The actual number of neighbors used for :meth:`kneighbors` queries.\n",
       "\n",
       "offset_ : float\n",
       "    Offset used to obtain binary labels from the raw scores.\n",
       "    Observations having a negative_outlier_factor smaller than `offset_`\n",
       "    are detected as abnormal.\n",
       "    The offset is set to -1.5 (inliers score around -1), except when a\n",
       "    contamination parameter different than \"auto\" is provided. In that\n",
       "    case, the offset is defined in such a way we obtain the expected\n",
       "    number of outliers in training.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> from sklearn.neighbors import LocalOutlierFactor\n",
       ">>> X = [[-1.1], [0.2], [101.1], [0.3]]\n",
       ">>> clf = LocalOutlierFactor(n_neighbors=2)\n",
       ">>> clf.fit_predict(X)\n",
       "array([ 1,  1, -1,  1])\n",
       ">>> clf.negative_outlier_factor_\n",
       "array([ -0.9821...,  -1.0370..., -73.3697...,  -0.9821...])\n",
       "\n",
       "References\n",
       "----------\n",
       ".. [1] Breunig, M. M., Kriegel, H. P., Ng, R. T., & Sander, J. (2000, May).\n",
       "       LOF: identifying density-based local outliers. In ACM sigmod record.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\toshiba\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_lof.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Beş değerden sonraki değerler yakın düşerken 5. değeri eşik olarak alacağım\n",
    "# Ancak ilk başta bu durumu aykırı değerlerle ilgili olarak görselleştireceğim.\n",
    "\n",
    "sns.boxplot(df_scores);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.9327181498464339\n"
     ]
    }
   ],
   "source": [
    "threshold=np.sort(df_scores)[5]\n",
    "print(threshold)\n",
    "df = df.loc[df_scores > threshold]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(257, 20)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>CHmRun</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>298</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>509</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "      <td>121</td>\n",
       "      <td>283</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>323</td>\n",
       "      <td>81</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>341</td>\n",
       "      <td>86</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>34</td>\n",
       "      <td>8</td>\n",
       "      <td>143</td>\n",
       "      <td>290</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>401</td>\n",
       "      <td>92</td>\n",
       "      <td>17</td>\n",
       "      <td>49</td>\n",
       "      <td>66</td>\n",
       "      <td>65</td>\n",
       "      <td>13</td>\n",
       "      <td>5206</td>\n",
       "      <td>1332</td>\n",
       "      <td>253</td>\n",
       "      <td>784</td>\n",
       "      <td>890</td>\n",
       "      <td>866</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>574</td>\n",
       "      <td>159</td>\n",
       "      <td>21</td>\n",
       "      <td>107</td>\n",
       "      <td>75</td>\n",
       "      <td>59</td>\n",
       "      <td>10</td>\n",
       "      <td>4631</td>\n",
       "      <td>1300</td>\n",
       "      <td>90</td>\n",
       "      <td>702</td>\n",
       "      <td>504</td>\n",
       "      <td>488</td>\n",
       "      <td>238</td>\n",
       "      <td>445</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>202</td>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "      <td>1876</td>\n",
       "      <td>467</td>\n",
       "      <td>15</td>\n",
       "      <td>192</td>\n",
       "      <td>186</td>\n",
       "      <td>161</td>\n",
       "      <td>304</td>\n",
       "      <td>45</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  CRuns  \\\n",
       "0    298    73      0    24   24      7      3     509    108       0     41   \n",
       "1    323    81      6    26   32      8      2     341     86       6     32   \n",
       "2    401    92     17    49   66     65     13    5206   1332     253    784   \n",
       "3    574   159     21   107   75     59     10    4631   1300      90    702   \n",
       "4    202    53      4    31   26     27      9    1876    467      15    192   \n",
       "\n",
       "   CRBI  CWalks  PutOuts  Assists  Errors  \n",
       "0    37      12      121      283       9  \n",
       "1    34       8      143      290      19  \n",
       "2   890     866        0        0       0  \n",
       "3   504     488      238      445      22  \n",
       "4   186     161      304       45      11  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardization\n",
    "# Aşağıdaki satırlarda bazı işlemler yapacağım.\n",
    "# Maaş benim bağımlı değişkenim, diğerleri kukla değişkenler. İlk başta onları bağımsız değişken kümemden (X) çıkaracağım.\n",
    "# Sonunda tüm bağımsız değişkenleri birleştireceğim\n",
    "\n",
    "df_X=df.drop(['Salary','League_N','Division_W','NewLeague_N'], axis=1)\n",
    "df_X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>CHmRun</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.718286</td>\n",
       "      <td>-0.772974</td>\n",
       "      <td>-1.328611</td>\n",
       "      <td>-1.207896</td>\n",
       "      <td>-1.064777</td>\n",
       "      <td>-1.568744</td>\n",
       "      <td>-0.902646</td>\n",
       "      <td>-0.939501</td>\n",
       "      <td>-0.947471</td>\n",
       "      <td>-0.843844</td>\n",
       "      <td>-0.967966</td>\n",
       "      <td>-0.907090</td>\n",
       "      <td>-0.939295</td>\n",
       "      <td>-0.596425</td>\n",
       "      <td>1.133526</td>\n",
       "      <td>0.068743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.548505</td>\n",
       "      <td>-0.595465</td>\n",
       "      <td>-0.644793</td>\n",
       "      <td>-1.129693</td>\n",
       "      <td>-0.755745</td>\n",
       "      <td>-1.522804</td>\n",
       "      <td>-1.111826</td>\n",
       "      <td>-1.012932</td>\n",
       "      <td>-0.981368</td>\n",
       "      <td>-0.770829</td>\n",
       "      <td>-0.995101</td>\n",
       "      <td>-0.916355</td>\n",
       "      <td>-0.954354</td>\n",
       "      <td>-0.517052</td>\n",
       "      <td>1.181824</td>\n",
       "      <td>1.591756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.018788</td>\n",
       "      <td>-0.351391</td>\n",
       "      <td>0.608873</td>\n",
       "      <td>-0.230351</td>\n",
       "      <td>0.557640</td>\n",
       "      <td>1.095761</td>\n",
       "      <td>1.189149</td>\n",
       "      <td>1.113530</td>\n",
       "      <td>0.938454</td>\n",
       "      <td>2.234972</td>\n",
       "      <td>1.272122</td>\n",
       "      <td>1.727355</td>\n",
       "      <td>2.275692</td>\n",
       "      <td>-1.032980</td>\n",
       "      <td>-0.819115</td>\n",
       "      <td>-1.301969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.156096</td>\n",
       "      <td>1.135243</td>\n",
       "      <td>1.064751</td>\n",
       "      <td>2.037555</td>\n",
       "      <td>0.905301</td>\n",
       "      <td>0.820123</td>\n",
       "      <td>0.561610</td>\n",
       "      <td>0.862201</td>\n",
       "      <td>0.889148</td>\n",
       "      <td>0.251387</td>\n",
       "      <td>1.024898</td>\n",
       "      <td>0.535215</td>\n",
       "      <td>0.852665</td>\n",
       "      <td>-0.174302</td>\n",
       "      <td>2.251292</td>\n",
       "      <td>2.048660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.370245</td>\n",
       "      <td>-1.216745</td>\n",
       "      <td>-0.872732</td>\n",
       "      <td>-0.934184</td>\n",
       "      <td>-0.987519</td>\n",
       "      <td>-0.649949</td>\n",
       "      <td>0.352431</td>\n",
       "      <td>-0.341993</td>\n",
       "      <td>-0.394328</td>\n",
       "      <td>-0.661306</td>\n",
       "      <td>-0.512713</td>\n",
       "      <td>-0.446911</td>\n",
       "      <td>-0.378366</td>\n",
       "      <td>0.063819</td>\n",
       "      <td>-0.508625</td>\n",
       "      <td>0.373346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      AtBat      Hits     HmRun      Runs       RBI     Walks     Years  \\\n",
       "0 -0.718286 -0.772974 -1.328611 -1.207896 -1.064777 -1.568744 -0.902646   \n",
       "1 -0.548505 -0.595465 -0.644793 -1.129693 -0.755745 -1.522804 -1.111826   \n",
       "2 -0.018788 -0.351391  0.608873 -0.230351  0.557640  1.095761  1.189149   \n",
       "3  1.156096  1.135243  1.064751  2.037555  0.905301  0.820123  0.561610   \n",
       "4 -1.370245 -1.216745 -0.872732 -0.934184 -0.987519 -0.649949  0.352431   \n",
       "\n",
       "     CAtBat     CHits    CHmRun     CRuns      CRBI    CWalks   PutOuts  \\\n",
       "0 -0.939501 -0.947471 -0.843844 -0.967966 -0.907090 -0.939295 -0.596425   \n",
       "1 -1.012932 -0.981368 -0.770829 -0.995101 -0.916355 -0.954354 -0.517052   \n",
       "2  1.113530  0.938454  2.234972  1.272122  1.727355  2.275692 -1.032980   \n",
       "3  0.862201  0.889148  0.251387  1.024898  0.535215  0.852665 -0.174302   \n",
       "4 -0.341993 -0.394328 -0.661306 -0.512713 -0.446911 -0.378366  0.063819   \n",
       "\n",
       "    Assists    Errors  \n",
       "0  1.133526  0.068743  \n",
       "1  1.181824  1.591756  \n",
       "2 -0.819115 -1.301969  \n",
       "3  2.251292  2.048660  \n",
       "4 -0.508625  0.373346  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaled_cols=StandardScaler().fit_transform(df_X)\n",
    "\n",
    "\n",
    "\n",
    "scaled_cols=pd.DataFrame(scaled_cols, columns=df_X.columns)\n",
    "scaled_cols.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>League_N</th>\n",
       "      <th>Division_W</th>\n",
       "      <th>NewLeague_N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   League_N  Division_W  NewLeague_N\n",
       "0         0           1            0\n",
       "1         1           1            1\n",
       "2         0           0            0\n",
       "3         0           0            0\n",
       "4         1           1            1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_df=df.loc[:, \"League_N\":\"NewLeague_N\"]\n",
    "cat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Salary=pd.DataFrame(df['Salary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Salary</th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>CHmRun</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Errors</th>\n",
       "      <th>League_N</th>\n",
       "      <th>Division_W</th>\n",
       "      <th>NewLeague_N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.000</td>\n",
       "      <td>-0.718286</td>\n",
       "      <td>-0.772974</td>\n",
       "      <td>-1.328611</td>\n",
       "      <td>-1.207896</td>\n",
       "      <td>-1.064777</td>\n",
       "      <td>-1.568744</td>\n",
       "      <td>-0.902646</td>\n",
       "      <td>-0.939501</td>\n",
       "      <td>-0.947471</td>\n",
       "      <td>-0.843844</td>\n",
       "      <td>-0.967966</td>\n",
       "      <td>-0.907090</td>\n",
       "      <td>-0.939295</td>\n",
       "      <td>-0.596425</td>\n",
       "      <td>1.133526</td>\n",
       "      <td>0.068743</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75.000</td>\n",
       "      <td>-0.548505</td>\n",
       "      <td>-0.595465</td>\n",
       "      <td>-0.644793</td>\n",
       "      <td>-1.129693</td>\n",
       "      <td>-0.755745</td>\n",
       "      <td>-1.522804</td>\n",
       "      <td>-1.111826</td>\n",
       "      <td>-1.012932</td>\n",
       "      <td>-0.981368</td>\n",
       "      <td>-0.770829</td>\n",
       "      <td>-0.995101</td>\n",
       "      <td>-0.916355</td>\n",
       "      <td>-0.954354</td>\n",
       "      <td>-0.517052</td>\n",
       "      <td>1.181824</td>\n",
       "      <td>1.591756</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1100.000</td>\n",
       "      <td>-0.018788</td>\n",
       "      <td>-0.351391</td>\n",
       "      <td>0.608873</td>\n",
       "      <td>-0.230351</td>\n",
       "      <td>0.557640</td>\n",
       "      <td>1.095761</td>\n",
       "      <td>1.189149</td>\n",
       "      <td>1.113530</td>\n",
       "      <td>0.938454</td>\n",
       "      <td>2.234972</td>\n",
       "      <td>1.272122</td>\n",
       "      <td>1.727355</td>\n",
       "      <td>2.275692</td>\n",
       "      <td>-1.032980</td>\n",
       "      <td>-0.819115</td>\n",
       "      <td>-1.301969</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>517.143</td>\n",
       "      <td>1.156096</td>\n",
       "      <td>1.135243</td>\n",
       "      <td>1.064751</td>\n",
       "      <td>2.037555</td>\n",
       "      <td>0.905301</td>\n",
       "      <td>0.820123</td>\n",
       "      <td>0.561610</td>\n",
       "      <td>0.862201</td>\n",
       "      <td>0.889148</td>\n",
       "      <td>0.251387</td>\n",
       "      <td>1.024898</td>\n",
       "      <td>0.535215</td>\n",
       "      <td>0.852665</td>\n",
       "      <td>-0.174302</td>\n",
       "      <td>2.251292</td>\n",
       "      <td>2.048660</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>512.500</td>\n",
       "      <td>-1.370245</td>\n",
       "      <td>-1.216745</td>\n",
       "      <td>-0.872732</td>\n",
       "      <td>-0.934184</td>\n",
       "      <td>-0.987519</td>\n",
       "      <td>-0.649949</td>\n",
       "      <td>0.352431</td>\n",
       "      <td>-0.341993</td>\n",
       "      <td>-0.394328</td>\n",
       "      <td>-0.661306</td>\n",
       "      <td>-0.512713</td>\n",
       "      <td>-0.446911</td>\n",
       "      <td>-0.378366</td>\n",
       "      <td>0.063819</td>\n",
       "      <td>-0.508625</td>\n",
       "      <td>0.373346</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Salary     AtBat      Hits     HmRun      Runs       RBI     Walks  \\\n",
       "0   100.000 -0.718286 -0.772974 -1.328611 -1.207896 -1.064777 -1.568744   \n",
       "1    75.000 -0.548505 -0.595465 -0.644793 -1.129693 -0.755745 -1.522804   \n",
       "2  1100.000 -0.018788 -0.351391  0.608873 -0.230351  0.557640  1.095761   \n",
       "3   517.143  1.156096  1.135243  1.064751  2.037555  0.905301  0.820123   \n",
       "4   512.500 -1.370245 -1.216745 -0.872732 -0.934184 -0.987519 -0.649949   \n",
       "\n",
       "      Years    CAtBat     CHits    CHmRun     CRuns      CRBI    CWalks  \\\n",
       "0 -0.902646 -0.939501 -0.947471 -0.843844 -0.967966 -0.907090 -0.939295   \n",
       "1 -1.111826 -1.012932 -0.981368 -0.770829 -0.995101 -0.916355 -0.954354   \n",
       "2  1.189149  1.113530  0.938454  2.234972  1.272122  1.727355  2.275692   \n",
       "3  0.561610  0.862201  0.889148  0.251387  1.024898  0.535215  0.852665   \n",
       "4  0.352431 -0.341993 -0.394328 -0.661306 -0.512713 -0.446911 -0.378366   \n",
       "\n",
       "    PutOuts   Assists    Errors  League_N  Division_W  NewLeague_N  \n",
       "0 -0.596425  1.133526  0.068743         0           1            0  \n",
       "1 -0.517052  1.181824  1.591756         1           1            1  \n",
       "2 -1.032980 -0.819115 -1.301969         0           0            0  \n",
       "3 -0.174302  2.251292  2.048660         0           0            0  \n",
       "4  0.063819 -0.508625  0.373346         1           1            1  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.concat([Salary,scaled_cols, cat_df], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bağımlı değişken y = Maaş, bağımsız değişkenler x = maaşsız değişkenler\n",
    "\n",
    "y = df['Salary']\n",
    "X = df.drop('Salary', axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>CHmRun</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Errors</th>\n",
       "      <th>League_N</th>\n",
       "      <th>Division_W</th>\n",
       "      <th>NewLeague_N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.718286</td>\n",
       "      <td>-0.772974</td>\n",
       "      <td>-1.328611</td>\n",
       "      <td>-1.207896</td>\n",
       "      <td>-1.064777</td>\n",
       "      <td>-1.568744</td>\n",
       "      <td>-0.902646</td>\n",
       "      <td>-0.939501</td>\n",
       "      <td>-0.947471</td>\n",
       "      <td>-0.843844</td>\n",
       "      <td>-0.967966</td>\n",
       "      <td>-0.907090</td>\n",
       "      <td>-0.939295</td>\n",
       "      <td>-0.596425</td>\n",
       "      <td>1.133526</td>\n",
       "      <td>0.068743</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.548505</td>\n",
       "      <td>-0.595465</td>\n",
       "      <td>-0.644793</td>\n",
       "      <td>-1.129693</td>\n",
       "      <td>-0.755745</td>\n",
       "      <td>-1.522804</td>\n",
       "      <td>-1.111826</td>\n",
       "      <td>-1.012932</td>\n",
       "      <td>-0.981368</td>\n",
       "      <td>-0.770829</td>\n",
       "      <td>-0.995101</td>\n",
       "      <td>-0.916355</td>\n",
       "      <td>-0.954354</td>\n",
       "      <td>-0.517052</td>\n",
       "      <td>1.181824</td>\n",
       "      <td>1.591756</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.018788</td>\n",
       "      <td>-0.351391</td>\n",
       "      <td>0.608873</td>\n",
       "      <td>-0.230351</td>\n",
       "      <td>0.557640</td>\n",
       "      <td>1.095761</td>\n",
       "      <td>1.189149</td>\n",
       "      <td>1.113530</td>\n",
       "      <td>0.938454</td>\n",
       "      <td>2.234972</td>\n",
       "      <td>1.272122</td>\n",
       "      <td>1.727355</td>\n",
       "      <td>2.275692</td>\n",
       "      <td>-1.032980</td>\n",
       "      <td>-0.819115</td>\n",
       "      <td>-1.301969</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.156096</td>\n",
       "      <td>1.135243</td>\n",
       "      <td>1.064751</td>\n",
       "      <td>2.037555</td>\n",
       "      <td>0.905301</td>\n",
       "      <td>0.820123</td>\n",
       "      <td>0.561610</td>\n",
       "      <td>0.862201</td>\n",
       "      <td>0.889148</td>\n",
       "      <td>0.251387</td>\n",
       "      <td>1.024898</td>\n",
       "      <td>0.535215</td>\n",
       "      <td>0.852665</td>\n",
       "      <td>-0.174302</td>\n",
       "      <td>2.251292</td>\n",
       "      <td>2.048660</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.370245</td>\n",
       "      <td>-1.216745</td>\n",
       "      <td>-0.872732</td>\n",
       "      <td>-0.934184</td>\n",
       "      <td>-0.987519</td>\n",
       "      <td>-0.649949</td>\n",
       "      <td>0.352431</td>\n",
       "      <td>-0.341993</td>\n",
       "      <td>-0.394328</td>\n",
       "      <td>-0.661306</td>\n",
       "      <td>-0.512713</td>\n",
       "      <td>-0.446911</td>\n",
       "      <td>-0.378366</td>\n",
       "      <td>0.063819</td>\n",
       "      <td>-0.508625</td>\n",
       "      <td>0.373346</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0.633171</td>\n",
       "      <td>0.425209</td>\n",
       "      <td>-0.530823</td>\n",
       "      <td>0.395278</td>\n",
       "      <td>-0.137681</td>\n",
       "      <td>-0.190552</td>\n",
       "      <td>-0.484287</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>0.127999</td>\n",
       "      <td>-0.454429</td>\n",
       "      <td>0.051078</td>\n",
       "      <td>-0.060856</td>\n",
       "      <td>-0.464953</td>\n",
       "      <td>0.139585</td>\n",
       "      <td>-0.757017</td>\n",
       "      <td>-0.845065</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>0.599215</td>\n",
       "      <td>0.624906</td>\n",
       "      <td>-0.758763</td>\n",
       "      <td>0.825398</td>\n",
       "      <td>-0.060424</td>\n",
       "      <td>2.428014</td>\n",
       "      <td>0.979969</td>\n",
       "      <td>1.246844</td>\n",
       "      <td>1.214255</td>\n",
       "      <td>-0.369244</td>\n",
       "      <td>1.612808</td>\n",
       "      <td>0.371527</td>\n",
       "      <td>2.309574</td>\n",
       "      <td>0.096290</td>\n",
       "      <td>1.809706</td>\n",
       "      <td>1.744058</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0.483764</td>\n",
       "      <td>0.403020</td>\n",
       "      <td>-0.986702</td>\n",
       "      <td>0.238871</td>\n",
       "      <td>-0.330826</td>\n",
       "      <td>0.498545</td>\n",
       "      <td>-0.275108</td>\n",
       "      <td>-0.418922</td>\n",
       "      <td>-0.446715</td>\n",
       "      <td>-0.758659</td>\n",
       "      <td>-0.437340</td>\n",
       "      <td>-0.734137</td>\n",
       "      <td>-0.434836</td>\n",
       "      <td>-0.899488</td>\n",
       "      <td>-0.039439</td>\n",
       "      <td>-0.235860</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>1.149305</td>\n",
       "      <td>0.802414</td>\n",
       "      <td>-0.302884</td>\n",
       "      <td>1.177315</td>\n",
       "      <td>0.325866</td>\n",
       "      <td>1.692978</td>\n",
       "      <td>0.143251</td>\n",
       "      <td>0.235845</td>\n",
       "      <td>0.206579</td>\n",
       "      <td>0.336572</td>\n",
       "      <td>0.325436</td>\n",
       "      <td>0.275785</td>\n",
       "      <td>0.265384</td>\n",
       "      <td>3.707790</td>\n",
       "      <td>0.084757</td>\n",
       "      <td>0.525647</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>1.543197</td>\n",
       "      <td>1.379317</td>\n",
       "      <td>-0.302884</td>\n",
       "      <td>0.864500</td>\n",
       "      <td>-0.292197</td>\n",
       "      <td>-0.466190</td>\n",
       "      <td>0.770790</td>\n",
       "      <td>0.983276</td>\n",
       "      <td>1.131052</td>\n",
       "      <td>-0.478767</td>\n",
       "      <td>1.244987</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>-0.047080</td>\n",
       "      <td>0.439040</td>\n",
       "      <td>-0.791516</td>\n",
       "      <td>-0.845065</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>257 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        AtBat      Hits     HmRun      Runs       RBI     Walks     Years  \\\n",
       "0   -0.718286 -0.772974 -1.328611 -1.207896 -1.064777 -1.568744 -0.902646   \n",
       "1   -0.548505 -0.595465 -0.644793 -1.129693 -0.755745 -1.522804 -1.111826   \n",
       "2   -0.018788 -0.351391  0.608873 -0.230351  0.557640  1.095761  1.189149   \n",
       "3    1.156096  1.135243  1.064751  2.037555  0.905301  0.820123  0.561610   \n",
       "4   -1.370245 -1.216745 -0.872732 -0.934184 -0.987519 -0.649949  0.352431   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "252  0.633171  0.425209 -0.530823  0.395278 -0.137681 -0.190552 -0.484287   \n",
       "253  0.599215  0.624906 -0.758763  0.825398 -0.060424  2.428014  0.979969   \n",
       "254  0.483764  0.403020 -0.986702  0.238871 -0.330826  0.498545 -0.275108   \n",
       "255  1.149305  0.802414 -0.302884  1.177315  0.325866  1.692978  0.143251   \n",
       "256  1.543197  1.379317 -0.302884  0.864500 -0.292197 -0.466190  0.770790   \n",
       "\n",
       "       CAtBat     CHits    CHmRun     CRuns      CRBI    CWalks   PutOuts  \\\n",
       "0   -0.939501 -0.947471 -0.843844 -0.967966 -0.907090 -0.939295 -0.596425   \n",
       "1   -1.012932 -0.981368 -0.770829 -0.995101 -0.916355 -0.954354 -0.517052   \n",
       "2    1.113530  0.938454  2.234972  1.272122  1.727355  2.275692 -1.032980   \n",
       "3    0.862201  0.889148  0.251387  1.024898  0.535215  0.852665 -0.174302   \n",
       "4   -0.341993 -0.394328 -0.661306 -0.512713 -0.446911 -0.378366  0.063819   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "252  0.019484  0.127999 -0.454429  0.051078 -0.060856 -0.464953  0.139585   \n",
       "253  1.246844  1.214255 -0.369244  1.612808  0.371527  2.309574  0.096290   \n",
       "254 -0.418922 -0.446715 -0.758659 -0.437340 -0.734137 -0.434836 -0.899488   \n",
       "255  0.235845  0.206579  0.336572  0.325436  0.275785  0.265384  3.707790   \n",
       "256  0.983276  1.131052 -0.478767  1.244987  0.081213 -0.047080  0.439040   \n",
       "\n",
       "      Assists    Errors  League_N  Division_W  NewLeague_N  \n",
       "0    1.133526  0.068743         0           1            0  \n",
       "1    1.181824  1.591756         1           1            1  \n",
       "2   -0.819115 -1.301969         0           0            0  \n",
       "3    2.251292  2.048660         0           0            0  \n",
       "4   -0.508625  0.373346         1           1            1  \n",
       "..        ...       ...       ...         ...          ...  \n",
       "252 -0.757017 -0.845065         1           0            1  \n",
       "253  1.809706  1.744058         0           0            0  \n",
       "254 -0.039439 -0.235860         0           1            0  \n",
       "255  0.084757  0.525647         0           0            0  \n",
       "256 -0.791516 -0.845065         0           1            0  \n",
       "\n",
       "[257 rows x 19 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       100.000\n",
       "1        75.000\n",
       "2      1100.000\n",
       "3       517.143\n",
       "4       512.500\n",
       "         ...   \n",
       "252     700.000\n",
       "253     875.000\n",
       "254     385.000\n",
       "255     960.000\n",
       "256    1000.000\n",
       "Name: Salary, Length: 257, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "539.2295992217898"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model sonuçlarımızı tahmin edilen değişkenin (y) ortalama değerine göre değerlendireceğiz.\n",
    "\n",
    "y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELLEME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ve test ayırma süreci ve train ve test boyutunun belirlenmesi\n",
    "# Modelleri karşılaştırmak için tüm modeller için test boyutu verilerin %20'si ve rastgele durum 46 olacaktır.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315.28976474517196"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridreg = Ridge()\n",
    "model = ridreg.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "df_ridreg_rmse = np.sqrt(mean_squared_error(y_test,y_pred))\n",
    "df_ridreg_rmse "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318.6803120461422"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasreg = Lasso()\n",
    "model = lasreg.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "df_lasreg_rmse = np.sqrt(mean_squared_error(y_test,y_pred))\n",
    "df_lasreg_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302.78610988935907"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enet = ElasticNet()\n",
    "model = enet.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "df_enet_rmse = np.sqrt(mean_squared_error(y_test,y_pred))\n",
    "df_enet_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN (K-Nearest Neighbors )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315.07079037040967"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsRegressor()\n",
    "knn_model = knn.fit(X_train, y_train)\n",
    "y_pred = knn_model.predict(X_test)\n",
    "df_knn_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "df_knn_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR (support Vector Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278.85157251429393"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr = SVR(\"linear\")\n",
    "svr_model = svr.fit(X_train, y_train)\n",
    "y_pred = svr_model.predict(X_test)\n",
    "df_svr_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "df_svr_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP (Multilayer Perceptron)\n",
    "One of the Artificial Neural Network Models (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514.898354766454"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPRegressor()\n",
    "mlp_model = mlp.fit(X_train, y_train)\n",
    "y_pred = mlp_model.predict(X_test)\n",
    "df_mlp_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "df_mlp_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CART (Classification and Regression Trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "428.1806398479681"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart = DecisionTreeRegressor()\n",
    "cart_model = cart.fit(X_train, y_train)\n",
    "y_pred = cart_model.predict(X_test)\n",
    "df_cart_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "df_cart_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284.5394676691895"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf_model = rf.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "df_rf_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "df_rf_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBM (Gradient Boosting Machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267.3247955953076"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm = GradientBoostingRegressor()\n",
    "gbm_model = gbm.fit(X_train, y_train)\n",
    "y_pred = gbm_model.predict(X_test)\n",
    "df_gbm_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "df_gbm_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost (Extreme Gradient Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324.3984254581684"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBRegressor()\n",
    "xgb_model = xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "df_xgb_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "df_xgb_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276.06253566372055"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm = LGBMRegressor()\n",
    "lgbm_model = lgbm.fit(X_train, y_train)\n",
    "y_pred = lgbm_model.predict(X_test)\n",
    "df_lgbm_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "df_lgbm_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost (Category Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.029437\n",
      "0:\tlearn: 471.6018354\ttotal: 316ms\tremaining: 5m 15s\n",
      "1:\tlearn: 464.5806381\ttotal: 322ms\tremaining: 2m 40s\n",
      "2:\tlearn: 458.0766392\ttotal: 327ms\tremaining: 1m 48s\n",
      "3:\tlearn: 451.8490791\ttotal: 332ms\tremaining: 1m 22s\n",
      "4:\tlearn: 445.6296077\ttotal: 337ms\tremaining: 1m 7s\n",
      "5:\tlearn: 440.0957215\ttotal: 341ms\tremaining: 56.5s\n",
      "6:\tlearn: 435.1083799\ttotal: 344ms\tremaining: 48.8s\n",
      "7:\tlearn: 429.2321315\ttotal: 347ms\tremaining: 43.1s\n",
      "8:\tlearn: 423.0557357\ttotal: 350ms\tremaining: 38.6s\n",
      "9:\tlearn: 417.7807447\ttotal: 354ms\tremaining: 35s\n",
      "10:\tlearn: 412.8369072\ttotal: 357ms\tremaining: 32.1s\n",
      "11:\tlearn: 408.4064884\ttotal: 360ms\tremaining: 29.7s\n",
      "12:\tlearn: 402.6927357\ttotal: 364ms\tremaining: 27.6s\n",
      "13:\tlearn: 397.9132016\ttotal: 367ms\tremaining: 25.8s\n",
      "14:\tlearn: 393.2047282\ttotal: 369ms\tremaining: 24.2s\n",
      "15:\tlearn: 388.3594596\ttotal: 371ms\tremaining: 22.8s\n",
      "16:\tlearn: 384.0940179\ttotal: 372ms\tremaining: 21.5s\n",
      "17:\tlearn: 379.6927458\ttotal: 374ms\tremaining: 20.4s\n",
      "18:\tlearn: 375.5546320\ttotal: 376ms\tremaining: 19.4s\n",
      "19:\tlearn: 371.5199148\ttotal: 378ms\tremaining: 18.5s\n",
      "20:\tlearn: 367.5220047\ttotal: 380ms\tremaining: 17.7s\n",
      "21:\tlearn: 363.2025853\ttotal: 382ms\tremaining: 17s\n",
      "22:\tlearn: 359.2983065\ttotal: 384ms\tremaining: 16.3s\n",
      "23:\tlearn: 355.3604662\ttotal: 385ms\tremaining: 15.7s\n",
      "24:\tlearn: 352.0826433\ttotal: 387ms\tremaining: 15.1s\n",
      "25:\tlearn: 349.2424049\ttotal: 389ms\tremaining: 14.6s\n",
      "26:\tlearn: 346.1560184\ttotal: 391ms\tremaining: 14.1s\n",
      "27:\tlearn: 343.6779480\ttotal: 393ms\tremaining: 13.6s\n",
      "28:\tlearn: 340.0519365\ttotal: 395ms\tremaining: 13.2s\n",
      "29:\tlearn: 335.9618689\ttotal: 396ms\tremaining: 12.8s\n",
      "30:\tlearn: 332.5771734\ttotal: 398ms\tremaining: 12.4s\n",
      "31:\tlearn: 329.2915327\ttotal: 399ms\tremaining: 12.1s\n",
      "32:\tlearn: 326.0814775\ttotal: 401ms\tremaining: 11.7s\n",
      "33:\tlearn: 323.1653955\ttotal: 402ms\tremaining: 11.4s\n",
      "34:\tlearn: 320.2560161\ttotal: 404ms\tremaining: 11.1s\n",
      "35:\tlearn: 317.3812458\ttotal: 405ms\tremaining: 10.9s\n",
      "36:\tlearn: 314.3862135\ttotal: 407ms\tremaining: 10.6s\n",
      "37:\tlearn: 311.8151615\ttotal: 408ms\tremaining: 10.3s\n",
      "38:\tlearn: 309.1839468\ttotal: 410ms\tremaining: 10.1s\n",
      "39:\tlearn: 306.5948143\ttotal: 411ms\tremaining: 9.87s\n",
      "40:\tlearn: 303.6738706\ttotal: 413ms\tremaining: 9.66s\n",
      "41:\tlearn: 301.1102592\ttotal: 414ms\tremaining: 9.45s\n",
      "42:\tlearn: 299.2185046\ttotal: 416ms\tremaining: 9.26s\n",
      "43:\tlearn: 296.1632283\ttotal: 417ms\tremaining: 9.07s\n",
      "44:\tlearn: 293.6414736\ttotal: 419ms\tremaining: 8.89s\n",
      "45:\tlearn: 290.7288702\ttotal: 420ms\tremaining: 8.72s\n",
      "46:\tlearn: 288.5031425\ttotal: 422ms\tremaining: 8.56s\n",
      "47:\tlearn: 286.5721372\ttotal: 424ms\tremaining: 8.4s\n",
      "48:\tlearn: 284.5425133\ttotal: 425ms\tremaining: 8.25s\n",
      "49:\tlearn: 282.2610849\ttotal: 427ms\tremaining: 8.11s\n",
      "50:\tlearn: 280.2692548\ttotal: 428ms\tremaining: 7.97s\n",
      "51:\tlearn: 278.2001415\ttotal: 430ms\tremaining: 7.83s\n",
      "52:\tlearn: 275.6296465\ttotal: 431ms\tremaining: 7.7s\n",
      "53:\tlearn: 273.2771292\ttotal: 433ms\tremaining: 7.58s\n",
      "54:\tlearn: 271.9497062\ttotal: 434ms\tremaining: 7.46s\n",
      "55:\tlearn: 269.9894707\ttotal: 436ms\tremaining: 7.34s\n",
      "56:\tlearn: 267.8308134\ttotal: 437ms\tremaining: 7.23s\n",
      "57:\tlearn: 265.5257892\ttotal: 439ms\tremaining: 7.13s\n",
      "58:\tlearn: 263.7991677\ttotal: 440ms\tremaining: 7.02s\n",
      "59:\tlearn: 261.9063088\ttotal: 442ms\tremaining: 6.92s\n",
      "60:\tlearn: 260.3390844\ttotal: 443ms\tremaining: 6.82s\n",
      "61:\tlearn: 258.6999250\ttotal: 445ms\tremaining: 6.73s\n",
      "62:\tlearn: 256.6915896\ttotal: 446ms\tremaining: 6.64s\n",
      "63:\tlearn: 254.6936244\ttotal: 448ms\tremaining: 6.55s\n",
      "64:\tlearn: 252.6856418\ttotal: 449ms\tremaining: 6.46s\n",
      "65:\tlearn: 250.5866366\ttotal: 451ms\tremaining: 6.38s\n",
      "66:\tlearn: 249.1889464\ttotal: 452ms\tremaining: 6.3s\n",
      "67:\tlearn: 247.8041020\ttotal: 454ms\tremaining: 6.22s\n",
      "68:\tlearn: 245.7509029\ttotal: 456ms\tremaining: 6.15s\n",
      "69:\tlearn: 244.1179557\ttotal: 457ms\tremaining: 6.07s\n",
      "70:\tlearn: 242.7589080\ttotal: 459ms\tremaining: 6s\n",
      "71:\tlearn: 240.9886039\ttotal: 460ms\tremaining: 5.93s\n",
      "72:\tlearn: 239.4434678\ttotal: 462ms\tremaining: 5.86s\n",
      "73:\tlearn: 238.7811080\ttotal: 463ms\tremaining: 5.8s\n",
      "74:\tlearn: 237.4316034\ttotal: 465ms\tremaining: 5.73s\n",
      "75:\tlearn: 236.0644426\ttotal: 466ms\tremaining: 5.67s\n",
      "76:\tlearn: 234.6439422\ttotal: 468ms\tremaining: 5.61s\n",
      "77:\tlearn: 233.6687623\ttotal: 470ms\tremaining: 5.55s\n",
      "78:\tlearn: 232.4671312\ttotal: 472ms\tremaining: 5.5s\n",
      "79:\tlearn: 231.2479518\ttotal: 473ms\tremaining: 5.44s\n",
      "80:\tlearn: 229.9284246\ttotal: 475ms\tremaining: 5.39s\n",
      "81:\tlearn: 229.3240496\ttotal: 477ms\tremaining: 5.34s\n",
      "82:\tlearn: 228.9434990\ttotal: 478ms\tremaining: 5.29s\n",
      "83:\tlearn: 227.4504725\ttotal: 480ms\tremaining: 5.24s\n",
      "84:\tlearn: 225.6780744\ttotal: 482ms\tremaining: 5.19s\n",
      "85:\tlearn: 224.0843646\ttotal: 484ms\tremaining: 5.15s\n",
      "86:\tlearn: 223.3918498\ttotal: 486ms\tremaining: 5.1s\n",
      "87:\tlearn: 222.3638695\ttotal: 488ms\tremaining: 5.05s\n",
      "88:\tlearn: 221.0627436\ttotal: 489ms\tremaining: 5.01s\n",
      "89:\tlearn: 219.8445897\ttotal: 491ms\tremaining: 4.97s\n",
      "90:\tlearn: 218.7619005\ttotal: 493ms\tremaining: 4.92s\n",
      "91:\tlearn: 217.9796101\ttotal: 495ms\tremaining: 4.88s\n",
      "92:\tlearn: 217.0833545\ttotal: 496ms\tremaining: 4.84s\n",
      "93:\tlearn: 215.9639398\ttotal: 498ms\tremaining: 4.8s\n",
      "94:\tlearn: 215.0447815\ttotal: 500ms\tremaining: 4.76s\n",
      "95:\tlearn: 213.8057012\ttotal: 501ms\tremaining: 4.72s\n",
      "96:\tlearn: 212.7690954\ttotal: 502ms\tremaining: 4.67s\n",
      "97:\tlearn: 211.4871608\ttotal: 504ms\tremaining: 4.63s\n",
      "98:\tlearn: 210.6997817\ttotal: 505ms\tremaining: 4.6s\n",
      "99:\tlearn: 209.5672995\ttotal: 507ms\tremaining: 4.56s\n",
      "100:\tlearn: 208.4983953\ttotal: 508ms\tremaining: 4.52s\n",
      "101:\tlearn: 207.5633445\ttotal: 510ms\tremaining: 4.49s\n",
      "102:\tlearn: 206.9418743\ttotal: 511ms\tremaining: 4.45s\n",
      "103:\tlearn: 206.0458561\ttotal: 513ms\tremaining: 4.42s\n",
      "104:\tlearn: 205.0635062\ttotal: 514ms\tremaining: 4.38s\n",
      "105:\tlearn: 203.9308607\ttotal: 516ms\tremaining: 4.35s\n",
      "106:\tlearn: 202.7258095\ttotal: 518ms\tremaining: 4.32s\n",
      "107:\tlearn: 201.8137716\ttotal: 519ms\tremaining: 4.29s\n",
      "108:\tlearn: 200.7978373\ttotal: 521ms\tremaining: 4.26s\n",
      "109:\tlearn: 199.8429607\ttotal: 522ms\tremaining: 4.22s\n",
      "110:\tlearn: 198.9300049\ttotal: 524ms\tremaining: 4.2s\n",
      "111:\tlearn: 198.0013974\ttotal: 526ms\tremaining: 4.17s\n",
      "112:\tlearn: 197.1208596\ttotal: 527ms\tremaining: 4.14s\n",
      "113:\tlearn: 196.1671467\ttotal: 529ms\tremaining: 4.11s\n",
      "114:\tlearn: 195.2336196\ttotal: 530ms\tremaining: 4.08s\n",
      "115:\tlearn: 194.4802711\ttotal: 532ms\tremaining: 4.05s\n",
      "116:\tlearn: 193.8566104\ttotal: 534ms\tremaining: 4.03s\n",
      "117:\tlearn: 193.0044297\ttotal: 535ms\tremaining: 4s\n",
      "118:\tlearn: 192.0673882\ttotal: 537ms\tremaining: 3.97s\n",
      "119:\tlearn: 191.3241746\ttotal: 538ms\tremaining: 3.95s\n",
      "120:\tlearn: 190.3776716\ttotal: 540ms\tremaining: 3.92s\n",
      "121:\tlearn: 189.1534796\ttotal: 542ms\tremaining: 3.9s\n",
      "122:\tlearn: 188.5757752\ttotal: 544ms\tremaining: 3.88s\n",
      "123:\tlearn: 187.6870422\ttotal: 545ms\tremaining: 3.85s\n",
      "124:\tlearn: 187.0210281\ttotal: 547ms\tremaining: 3.83s\n",
      "125:\tlearn: 186.2218411\ttotal: 548ms\tremaining: 3.8s\n",
      "126:\tlearn: 185.3543870\ttotal: 550ms\tremaining: 3.78s\n",
      "127:\tlearn: 184.6274644\ttotal: 551ms\tremaining: 3.75s\n",
      "128:\tlearn: 183.5455756\ttotal: 554ms\tremaining: 3.74s\n",
      "129:\tlearn: 182.7682652\ttotal: 556ms\tremaining: 3.72s\n",
      "130:\tlearn: 182.4424457\ttotal: 558ms\tremaining: 3.7s\n",
      "131:\tlearn: 181.6595266\ttotal: 559ms\tremaining: 3.68s\n",
      "132:\tlearn: 180.6984336\ttotal: 561ms\tremaining: 3.66s\n",
      "133:\tlearn: 180.1614675\ttotal: 562ms\tremaining: 3.63s\n",
      "134:\tlearn: 179.7328848\ttotal: 564ms\tremaining: 3.61s\n",
      "135:\tlearn: 179.3795651\ttotal: 566ms\tremaining: 3.59s\n",
      "136:\tlearn: 178.6192884\ttotal: 567ms\tremaining: 3.57s\n",
      "137:\tlearn: 177.8286601\ttotal: 569ms\tremaining: 3.55s\n",
      "138:\tlearn: 176.9418198\ttotal: 571ms\tremaining: 3.53s\n",
      "139:\tlearn: 176.3606628\ttotal: 572ms\tremaining: 3.51s\n",
      "140:\tlearn: 175.6459367\ttotal: 574ms\tremaining: 3.49s\n",
      "141:\tlearn: 174.9917729\ttotal: 575ms\tremaining: 3.47s\n",
      "142:\tlearn: 174.3052584\ttotal: 577ms\tremaining: 3.46s\n",
      "143:\tlearn: 173.6265678\ttotal: 578ms\tremaining: 3.44s\n",
      "144:\tlearn: 173.2031834\ttotal: 579ms\tremaining: 3.41s\n",
      "145:\tlearn: 172.3375076\ttotal: 580ms\tremaining: 3.39s\n",
      "146:\tlearn: 171.6368566\ttotal: 582ms\tremaining: 3.38s\n",
      "147:\tlearn: 170.9452466\ttotal: 584ms\tremaining: 3.36s\n",
      "148:\tlearn: 170.4662207\ttotal: 585ms\tremaining: 3.34s\n",
      "149:\tlearn: 169.7719573\ttotal: 587ms\tremaining: 3.32s\n",
      "150:\tlearn: 169.0460373\ttotal: 588ms\tremaining: 3.31s\n",
      "151:\tlearn: 168.5156059\ttotal: 590ms\tremaining: 3.29s\n",
      "152:\tlearn: 168.1890156\ttotal: 591ms\tremaining: 3.27s\n",
      "153:\tlearn: 167.4124576\ttotal: 593ms\tremaining: 3.26s\n",
      "154:\tlearn: 166.9371126\ttotal: 594ms\tremaining: 3.24s\n",
      "155:\tlearn: 166.3826597\ttotal: 596ms\tremaining: 3.22s\n",
      "156:\tlearn: 165.9474324\ttotal: 597ms\tremaining: 3.21s\n",
      "157:\tlearn: 165.1384931\ttotal: 599ms\tremaining: 3.19s\n",
      "158:\tlearn: 164.3383942\ttotal: 600ms\tremaining: 3.17s\n",
      "159:\tlearn: 163.9469034\ttotal: 602ms\tremaining: 3.16s\n",
      "160:\tlearn: 163.3109905\ttotal: 603ms\tremaining: 3.14s\n",
      "161:\tlearn: 162.9646658\ttotal: 605ms\tremaining: 3.13s\n",
      "162:\tlearn: 162.6257529\ttotal: 606ms\tremaining: 3.11s\n",
      "163:\tlearn: 162.1722192\ttotal: 608ms\tremaining: 3.1s\n",
      "164:\tlearn: 161.7547062\ttotal: 609ms\tremaining: 3.08s\n",
      "165:\tlearn: 161.4591309\ttotal: 611ms\tremaining: 3.07s\n",
      "166:\tlearn: 160.8474057\ttotal: 612ms\tremaining: 3.05s\n",
      "167:\tlearn: 160.6090217\ttotal: 614ms\tremaining: 3.04s\n",
      "168:\tlearn: 160.3984508\ttotal: 615ms\tremaining: 3.03s\n",
      "169:\tlearn: 160.1596712\ttotal: 617ms\tremaining: 3.01s\n",
      "170:\tlearn: 159.7689664\ttotal: 619ms\tremaining: 3s\n",
      "171:\tlearn: 159.3020143\ttotal: 620ms\tremaining: 2.98s\n",
      "172:\tlearn: 159.0168746\ttotal: 622ms\tremaining: 2.97s\n",
      "173:\tlearn: 158.5987846\ttotal: 623ms\tremaining: 2.96s\n",
      "174:\tlearn: 157.7469687\ttotal: 625ms\tremaining: 2.94s\n",
      "175:\tlearn: 157.5341788\ttotal: 626ms\tremaining: 2.93s\n",
      "176:\tlearn: 156.7898536\ttotal: 627ms\tremaining: 2.92s\n",
      "177:\tlearn: 156.1714096\ttotal: 629ms\tremaining: 2.9s\n",
      "178:\tlearn: 155.7604202\ttotal: 630ms\tremaining: 2.89s\n",
      "179:\tlearn: 155.1470855\ttotal: 632ms\tremaining: 2.88s\n",
      "180:\tlearn: 155.0091246\ttotal: 633ms\tremaining: 2.86s\n",
      "181:\tlearn: 154.3731857\ttotal: 635ms\tremaining: 2.85s\n",
      "182:\tlearn: 153.8005860\ttotal: 637ms\tremaining: 2.84s\n",
      "183:\tlearn: 153.1895030\ttotal: 638ms\tremaining: 2.83s\n",
      "184:\tlearn: 152.9429273\ttotal: 640ms\tremaining: 2.82s\n",
      "185:\tlearn: 152.6922001\ttotal: 642ms\tremaining: 2.81s\n",
      "186:\tlearn: 151.9249825\ttotal: 644ms\tremaining: 2.8s\n",
      "187:\tlearn: 151.6912033\ttotal: 645ms\tremaining: 2.79s\n",
      "188:\tlearn: 151.4171536\ttotal: 647ms\tremaining: 2.77s\n",
      "189:\tlearn: 150.9527863\ttotal: 649ms\tremaining: 2.76s\n",
      "190:\tlearn: 150.7845489\ttotal: 650ms\tremaining: 2.75s\n",
      "191:\tlearn: 150.2124021\ttotal: 652ms\tremaining: 2.74s\n",
      "192:\tlearn: 149.6323537\ttotal: 654ms\tremaining: 2.73s\n",
      "193:\tlearn: 149.2733850\ttotal: 655ms\tremaining: 2.72s\n",
      "194:\tlearn: 149.1072334\ttotal: 657ms\tremaining: 2.71s\n",
      "195:\tlearn: 148.8623817\ttotal: 659ms\tremaining: 2.7s\n",
      "196:\tlearn: 148.1339022\ttotal: 660ms\tremaining: 2.69s\n",
      "197:\tlearn: 147.4821424\ttotal: 662ms\tremaining: 2.68s\n",
      "198:\tlearn: 147.1812546\ttotal: 664ms\tremaining: 2.67s\n",
      "199:\tlearn: 146.9115023\ttotal: 665ms\tremaining: 2.66s\n",
      "200:\tlearn: 146.7578189\ttotal: 667ms\tremaining: 2.65s\n",
      "201:\tlearn: 146.1799387\ttotal: 668ms\tremaining: 2.64s\n",
      "202:\tlearn: 145.5140438\ttotal: 670ms\tremaining: 2.63s\n",
      "203:\tlearn: 145.2856521\ttotal: 672ms\tremaining: 2.62s\n",
      "204:\tlearn: 144.8246930\ttotal: 673ms\tremaining: 2.61s\n",
      "205:\tlearn: 144.4532392\ttotal: 675ms\tremaining: 2.6s\n",
      "206:\tlearn: 143.9613452\ttotal: 676ms\tremaining: 2.59s\n",
      "207:\tlearn: 143.6246236\ttotal: 678ms\tremaining: 2.58s\n",
      "208:\tlearn: 143.3962925\ttotal: 679ms\tremaining: 2.57s\n",
      "209:\tlearn: 143.0169011\ttotal: 681ms\tremaining: 2.56s\n",
      "210:\tlearn: 142.2182748\ttotal: 684ms\tremaining: 2.56s\n",
      "211:\tlearn: 141.9259329\ttotal: 686ms\tremaining: 2.55s\n",
      "212:\tlearn: 141.5970473\ttotal: 687ms\tremaining: 2.54s\n",
      "213:\tlearn: 141.3571870\ttotal: 689ms\tremaining: 2.53s\n",
      "214:\tlearn: 140.5897690\ttotal: 690ms\tremaining: 2.52s\n",
      "215:\tlearn: 140.3663934\ttotal: 692ms\tremaining: 2.51s\n",
      "216:\tlearn: 139.7098699\ttotal: 693ms\tremaining: 2.5s\n",
      "217:\tlearn: 139.5134236\ttotal: 695ms\tremaining: 2.49s\n",
      "218:\tlearn: 139.3350709\ttotal: 697ms\tremaining: 2.48s\n",
      "219:\tlearn: 139.0098432\ttotal: 698ms\tremaining: 2.48s\n",
      "220:\tlearn: 138.8751958\ttotal: 700ms\tremaining: 2.47s\n",
      "221:\tlearn: 138.3394064\ttotal: 701ms\tremaining: 2.46s\n",
      "222:\tlearn: 138.2214939\ttotal: 703ms\tremaining: 2.45s\n",
      "223:\tlearn: 137.9180677\ttotal: 704ms\tremaining: 2.44s\n",
      "224:\tlearn: 137.7450021\ttotal: 705ms\tremaining: 2.43s\n",
      "225:\tlearn: 137.6101178\ttotal: 707ms\tremaining: 2.42s\n",
      "226:\tlearn: 137.3501040\ttotal: 709ms\tremaining: 2.41s\n",
      "227:\tlearn: 136.7460106\ttotal: 710ms\tremaining: 2.4s\n",
      "228:\tlearn: 136.2245781\ttotal: 712ms\tremaining: 2.4s\n",
      "229:\tlearn: 135.9914047\ttotal: 713ms\tremaining: 2.39s\n",
      "230:\tlearn: 135.7262955\ttotal: 715ms\tremaining: 2.38s\n",
      "231:\tlearn: 135.5758996\ttotal: 716ms\tremaining: 2.37s\n",
      "232:\tlearn: 135.4055211\ttotal: 718ms\tremaining: 2.36s\n",
      "233:\tlearn: 134.9156316\ttotal: 719ms\tremaining: 2.35s\n",
      "234:\tlearn: 134.1838423\ttotal: 721ms\tremaining: 2.35s\n",
      "235:\tlearn: 134.0068374\ttotal: 722ms\tremaining: 2.34s\n",
      "236:\tlearn: 133.7952337\ttotal: 724ms\tremaining: 2.33s\n",
      "237:\tlearn: 133.5093558\ttotal: 725ms\tremaining: 2.32s\n",
      "238:\tlearn: 133.3599603\ttotal: 727ms\tremaining: 2.31s\n",
      "239:\tlearn: 133.1796711\ttotal: 728ms\tremaining: 2.31s\n",
      "240:\tlearn: 133.0225486\ttotal: 730ms\tremaining: 2.3s\n",
      "241:\tlearn: 132.3436100\ttotal: 731ms\tremaining: 2.29s\n",
      "242:\tlearn: 132.1433575\ttotal: 733ms\tremaining: 2.28s\n",
      "243:\tlearn: 131.9946682\ttotal: 734ms\tremaining: 2.27s\n",
      "244:\tlearn: 131.8048352\ttotal: 736ms\tremaining: 2.27s\n",
      "245:\tlearn: 131.6731056\ttotal: 737ms\tremaining: 2.26s\n",
      "246:\tlearn: 131.3495355\ttotal: 739ms\tremaining: 2.25s\n",
      "247:\tlearn: 131.2318768\ttotal: 740ms\tremaining: 2.24s\n",
      "248:\tlearn: 131.1020166\ttotal: 742ms\tremaining: 2.24s\n",
      "249:\tlearn: 130.9563475\ttotal: 743ms\tremaining: 2.23s\n",
      "250:\tlearn: 130.3125173\ttotal: 745ms\tremaining: 2.22s\n",
      "251:\tlearn: 130.1760290\ttotal: 747ms\tremaining: 2.22s\n",
      "252:\tlearn: 130.0457738\ttotal: 749ms\tremaining: 2.21s\n",
      "253:\tlearn: 129.1952717\ttotal: 751ms\tremaining: 2.21s\n",
      "254:\tlearn: 129.0741701\ttotal: 753ms\tremaining: 2.2s\n",
      "255:\tlearn: 128.9516570\ttotal: 755ms\tremaining: 2.19s\n",
      "256:\tlearn: 128.8243853\ttotal: 757ms\tremaining: 2.19s\n",
      "257:\tlearn: 128.6472807\ttotal: 759ms\tremaining: 2.18s\n",
      "258:\tlearn: 128.4112762\ttotal: 761ms\tremaining: 2.18s\n",
      "259:\tlearn: 128.1531258\ttotal: 763ms\tremaining: 2.17s\n",
      "260:\tlearn: 127.6956261\ttotal: 765ms\tremaining: 2.17s\n",
      "261:\tlearn: 127.5646104\ttotal: 767ms\tremaining: 2.16s\n",
      "262:\tlearn: 127.2993573\ttotal: 769ms\tremaining: 2.15s\n",
      "263:\tlearn: 127.1758127\ttotal: 770ms\tremaining: 2.15s\n",
      "264:\tlearn: 126.9224323\ttotal: 772ms\tremaining: 2.14s\n",
      "265:\tlearn: 126.4909591\ttotal: 774ms\tremaining: 2.14s\n",
      "266:\tlearn: 125.7227205\ttotal: 776ms\tremaining: 2.13s\n",
      "267:\tlearn: 125.5577982\ttotal: 777ms\tremaining: 2.12s\n",
      "268:\tlearn: 125.4044108\ttotal: 779ms\tremaining: 2.12s\n",
      "269:\tlearn: 124.7949475\ttotal: 781ms\tremaining: 2.11s\n",
      "270:\tlearn: 124.5189954\ttotal: 782ms\tremaining: 2.1s\n",
      "271:\tlearn: 124.3288243\ttotal: 784ms\tremaining: 2.1s\n",
      "272:\tlearn: 124.0293296\ttotal: 786ms\tremaining: 2.09s\n",
      "273:\tlearn: 123.8565883\ttotal: 787ms\tremaining: 2.08s\n",
      "274:\tlearn: 123.4601862\ttotal: 789ms\tremaining: 2.08s\n",
      "275:\tlearn: 123.3656883\ttotal: 791ms\tremaining: 2.07s\n",
      "276:\tlearn: 122.8218476\ttotal: 792ms\tremaining: 2.07s\n",
      "277:\tlearn: 122.1411129\ttotal: 794ms\tremaining: 2.06s\n",
      "278:\tlearn: 121.7325295\ttotal: 796ms\tremaining: 2.06s\n",
      "279:\tlearn: 121.5964870\ttotal: 798ms\tremaining: 2.05s\n",
      "280:\tlearn: 120.9872149\ttotal: 799ms\tremaining: 2.04s\n",
      "281:\tlearn: 120.3969924\ttotal: 801ms\tremaining: 2.04s\n",
      "282:\tlearn: 120.1750819\ttotal: 803ms\tremaining: 2.03s\n",
      "283:\tlearn: 119.4165259\ttotal: 804ms\tremaining: 2.03s\n",
      "284:\tlearn: 119.3272168\ttotal: 806ms\tremaining: 2.02s\n",
      "285:\tlearn: 118.5514477\ttotal: 808ms\tremaining: 2.02s\n",
      "286:\tlearn: 118.4492127\ttotal: 810ms\tremaining: 2.01s\n",
      "287:\tlearn: 118.2503654\ttotal: 811ms\tremaining: 2s\n",
      "288:\tlearn: 117.5110470\ttotal: 813ms\tremaining: 2s\n",
      "289:\tlearn: 117.3724115\ttotal: 815ms\tremaining: 1.99s\n",
      "290:\tlearn: 116.8177681\ttotal: 816ms\tremaining: 1.99s\n",
      "291:\tlearn: 116.1177016\ttotal: 818ms\tremaining: 1.98s\n",
      "292:\tlearn: 116.0053065\ttotal: 820ms\tremaining: 1.98s\n",
      "293:\tlearn: 115.8192072\ttotal: 821ms\tremaining: 1.97s\n",
      "294:\tlearn: 115.6882148\ttotal: 822ms\tremaining: 1.97s\n",
      "295:\tlearn: 115.0648226\ttotal: 824ms\tremaining: 1.96s\n",
      "296:\tlearn: 114.5372338\ttotal: 825ms\tremaining: 1.95s\n",
      "297:\tlearn: 114.1458417\ttotal: 827ms\tremaining: 1.95s\n",
      "298:\tlearn: 113.5059891\ttotal: 828ms\tremaining: 1.94s\n",
      "299:\tlearn: 113.2036005\ttotal: 830ms\tremaining: 1.94s\n",
      "300:\tlearn: 113.0805519\ttotal: 831ms\tremaining: 1.93s\n",
      "301:\tlearn: 112.4165837\ttotal: 833ms\tremaining: 1.92s\n",
      "302:\tlearn: 111.8288569\ttotal: 834ms\tremaining: 1.92s\n",
      "303:\tlearn: 111.6894278\ttotal: 836ms\tremaining: 1.91s\n",
      "304:\tlearn: 111.5947798\ttotal: 837ms\tremaining: 1.91s\n",
      "305:\tlearn: 110.9527366\ttotal: 839ms\tremaining: 1.9s\n",
      "306:\tlearn: 110.6129379\ttotal: 841ms\tremaining: 1.9s\n",
      "307:\tlearn: 110.4964680\ttotal: 842ms\tremaining: 1.89s\n",
      "308:\tlearn: 110.0948907\ttotal: 843ms\tremaining: 1.89s\n",
      "309:\tlearn: 109.5581940\ttotal: 845ms\tremaining: 1.88s\n",
      "310:\tlearn: 109.2845655\ttotal: 847ms\tremaining: 1.88s\n",
      "311:\tlearn: 109.1629795\ttotal: 848ms\tremaining: 1.87s\n",
      "312:\tlearn: 108.6279945\ttotal: 850ms\tremaining: 1.86s\n",
      "313:\tlearn: 107.9441030\ttotal: 852ms\tremaining: 1.86s\n",
      "314:\tlearn: 107.6117698\ttotal: 854ms\tremaining: 1.86s\n",
      "315:\tlearn: 107.3561163\ttotal: 856ms\tremaining: 1.85s\n",
      "316:\tlearn: 107.1050467\ttotal: 858ms\tremaining: 1.85s\n",
      "317:\tlearn: 106.9956026\ttotal: 859ms\tremaining: 1.84s\n",
      "318:\tlearn: 106.8762954\ttotal: 861ms\tremaining: 1.84s\n",
      "319:\tlearn: 106.5522684\ttotal: 862ms\tremaining: 1.83s\n",
      "320:\tlearn: 105.9161367\ttotal: 864ms\tremaining: 1.83s\n",
      "321:\tlearn: 105.8204564\ttotal: 866ms\tremaining: 1.82s\n",
      "322:\tlearn: 105.7441308\ttotal: 867ms\tremaining: 1.82s\n",
      "323:\tlearn: 105.2291450\ttotal: 869ms\tremaining: 1.81s\n",
      "324:\tlearn: 104.9314143\ttotal: 870ms\tremaining: 1.81s\n",
      "325:\tlearn: 104.4497225\ttotal: 872ms\tremaining: 1.8s\n",
      "326:\tlearn: 104.2754399\ttotal: 873ms\tremaining: 1.8s\n",
      "327:\tlearn: 104.2016041\ttotal: 875ms\tremaining: 1.79s\n",
      "328:\tlearn: 103.9343577\ttotal: 876ms\tremaining: 1.79s\n",
      "329:\tlearn: 103.8337927\ttotal: 878ms\tremaining: 1.78s\n",
      "330:\tlearn: 103.3944046\ttotal: 880ms\tremaining: 1.78s\n",
      "331:\tlearn: 103.2323329\ttotal: 882ms\tremaining: 1.77s\n",
      "332:\tlearn: 103.0436851\ttotal: 883ms\tremaining: 1.77s\n",
      "333:\tlearn: 102.8850481\ttotal: 885ms\tremaining: 1.76s\n",
      "334:\tlearn: 102.7145156\ttotal: 887ms\tremaining: 1.76s\n",
      "335:\tlearn: 102.6342729\ttotal: 888ms\tremaining: 1.75s\n",
      "336:\tlearn: 102.2452044\ttotal: 890ms\tremaining: 1.75s\n",
      "337:\tlearn: 101.8720798\ttotal: 891ms\tremaining: 1.75s\n",
      "338:\tlearn: 101.5736928\ttotal: 893ms\tremaining: 1.74s\n",
      "339:\tlearn: 101.1839279\ttotal: 894ms\tremaining: 1.74s\n",
      "340:\tlearn: 100.7619433\ttotal: 896ms\tremaining: 1.73s\n",
      "341:\tlearn: 100.4784131\ttotal: 897ms\tremaining: 1.73s\n",
      "342:\tlearn: 99.9909394\ttotal: 899ms\tremaining: 1.72s\n",
      "343:\tlearn: 99.8948851\ttotal: 900ms\tremaining: 1.72s\n",
      "344:\tlearn: 99.6016720\ttotal: 902ms\tremaining: 1.71s\n",
      "345:\tlearn: 99.1939237\ttotal: 903ms\tremaining: 1.71s\n",
      "346:\tlearn: 98.7364376\ttotal: 905ms\tremaining: 1.7s\n",
      "347:\tlearn: 98.5295394\ttotal: 906ms\tremaining: 1.7s\n",
      "348:\tlearn: 98.4700983\ttotal: 908ms\tremaining: 1.69s\n",
      "349:\tlearn: 98.0410287\ttotal: 909ms\tremaining: 1.69s\n",
      "350:\tlearn: 97.5892385\ttotal: 911ms\tremaining: 1.68s\n",
      "351:\tlearn: 97.1993894\ttotal: 913ms\tremaining: 1.68s\n",
      "352:\tlearn: 96.7071986\ttotal: 914ms\tremaining: 1.68s\n",
      "353:\tlearn: 96.1718958\ttotal: 916ms\tremaining: 1.67s\n",
      "354:\tlearn: 96.1048616\ttotal: 917ms\tremaining: 1.67s\n",
      "355:\tlearn: 95.5858926\ttotal: 919ms\tremaining: 1.66s\n",
      "356:\tlearn: 95.1660059\ttotal: 920ms\tremaining: 1.66s\n",
      "357:\tlearn: 94.8017600\ttotal: 922ms\tremaining: 1.65s\n",
      "358:\tlearn: 94.5742289\ttotal: 923ms\tremaining: 1.65s\n",
      "359:\tlearn: 94.5088784\ttotal: 924ms\tremaining: 1.64s\n",
      "360:\tlearn: 94.3056511\ttotal: 925ms\tremaining: 1.64s\n",
      "361:\tlearn: 94.1872626\ttotal: 927ms\tremaining: 1.63s\n",
      "362:\tlearn: 93.8037484\ttotal: 928ms\tremaining: 1.63s\n",
      "363:\tlearn: 93.4092069\ttotal: 930ms\tremaining: 1.62s\n",
      "364:\tlearn: 92.9729459\ttotal: 932ms\tremaining: 1.62s\n",
      "365:\tlearn: 92.6651016\ttotal: 933ms\tremaining: 1.62s\n",
      "366:\tlearn: 92.5654534\ttotal: 935ms\tremaining: 1.61s\n",
      "367:\tlearn: 92.2616847\ttotal: 936ms\tremaining: 1.61s\n",
      "368:\tlearn: 92.0398873\ttotal: 938ms\tremaining: 1.6s\n",
      "369:\tlearn: 91.9560798\ttotal: 940ms\tremaining: 1.6s\n",
      "370:\tlearn: 91.7989094\ttotal: 942ms\tremaining: 1.6s\n",
      "371:\tlearn: 91.5975591\ttotal: 943ms\tremaining: 1.59s\n",
      "372:\tlearn: 91.2553364\ttotal: 945ms\tremaining: 1.59s\n",
      "373:\tlearn: 91.1165360\ttotal: 947ms\tremaining: 1.58s\n",
      "374:\tlearn: 90.6666422\ttotal: 948ms\tremaining: 1.58s\n",
      "375:\tlearn: 90.2476230\ttotal: 950ms\tremaining: 1.58s\n",
      "376:\tlearn: 89.8584891\ttotal: 952ms\tremaining: 1.57s\n",
      "377:\tlearn: 89.7939690\ttotal: 953ms\tremaining: 1.57s\n",
      "378:\tlearn: 89.4894227\ttotal: 955ms\tremaining: 1.56s\n",
      "379:\tlearn: 89.3883988\ttotal: 957ms\tremaining: 1.56s\n",
      "380:\tlearn: 88.9383865\ttotal: 958ms\tremaining: 1.56s\n",
      "381:\tlearn: 88.4468744\ttotal: 960ms\tremaining: 1.55s\n",
      "382:\tlearn: 88.3642408\ttotal: 962ms\tremaining: 1.55s\n",
      "383:\tlearn: 88.0222077\ttotal: 964ms\tremaining: 1.54s\n",
      "384:\tlearn: 87.5923946\ttotal: 965ms\tremaining: 1.54s\n",
      "385:\tlearn: 87.3894253\ttotal: 967ms\tremaining: 1.54s\n",
      "386:\tlearn: 87.3211202\ttotal: 969ms\tremaining: 1.53s\n",
      "387:\tlearn: 86.9921070\ttotal: 970ms\tremaining: 1.53s\n",
      "388:\tlearn: 86.6848361\ttotal: 972ms\tremaining: 1.53s\n",
      "389:\tlearn: 86.5980401\ttotal: 973ms\tremaining: 1.52s\n",
      "390:\tlearn: 86.5330282\ttotal: 975ms\tremaining: 1.52s\n",
      "391:\tlearn: 86.2534985\ttotal: 977ms\tremaining: 1.51s\n",
      "392:\tlearn: 86.0796215\ttotal: 978ms\tremaining: 1.51s\n",
      "393:\tlearn: 85.8945720\ttotal: 980ms\tremaining: 1.51s\n",
      "394:\tlearn: 85.7364316\ttotal: 981ms\tremaining: 1.5s\n",
      "395:\tlearn: 85.4563953\ttotal: 983ms\tremaining: 1.5s\n",
      "396:\tlearn: 85.0503570\ttotal: 984ms\tremaining: 1.5s\n",
      "397:\tlearn: 84.7788829\ttotal: 986ms\tremaining: 1.49s\n",
      "398:\tlearn: 84.6763600\ttotal: 987ms\tremaining: 1.49s\n",
      "399:\tlearn: 84.5789162\ttotal: 989ms\tremaining: 1.48s\n",
      "400:\tlearn: 84.2870972\ttotal: 991ms\tremaining: 1.48s\n",
      "401:\tlearn: 84.2305133\ttotal: 992ms\tremaining: 1.48s\n",
      "402:\tlearn: 83.9354787\ttotal: 994ms\tremaining: 1.47s\n",
      "403:\tlearn: 83.7080428\ttotal: 996ms\tremaining: 1.47s\n",
      "404:\tlearn: 83.6505891\ttotal: 997ms\tremaining: 1.47s\n",
      "405:\tlearn: 83.5355436\ttotal: 999ms\tremaining: 1.46s\n",
      "406:\tlearn: 83.2281752\ttotal: 1s\tremaining: 1.46s\n",
      "407:\tlearn: 83.0483191\ttotal: 1s\tremaining: 1.45s\n",
      "408:\tlearn: 82.6090914\ttotal: 1s\tremaining: 1.45s\n",
      "409:\tlearn: 82.2983067\ttotal: 1s\tremaining: 1.45s\n",
      "410:\tlearn: 81.8810006\ttotal: 1.01s\tremaining: 1.44s\n",
      "411:\tlearn: 81.6244468\ttotal: 1.01s\tremaining: 1.44s\n",
      "412:\tlearn: 81.4422591\ttotal: 1.01s\tremaining: 1.44s\n",
      "413:\tlearn: 81.2347611\ttotal: 1.01s\tremaining: 1.43s\n",
      "414:\tlearn: 81.1421614\ttotal: 1.01s\tremaining: 1.43s\n",
      "415:\tlearn: 80.7130214\ttotal: 1.02s\tremaining: 1.43s\n",
      "416:\tlearn: 80.5388341\ttotal: 1.02s\tremaining: 1.42s\n",
      "417:\tlearn: 80.4449505\ttotal: 1.02s\tremaining: 1.42s\n",
      "418:\tlearn: 80.1689274\ttotal: 1.02s\tremaining: 1.42s\n",
      "419:\tlearn: 79.7547303\ttotal: 1.02s\tremaining: 1.41s\n",
      "420:\tlearn: 79.6639875\ttotal: 1.02s\tremaining: 1.41s\n",
      "421:\tlearn: 79.3092636\ttotal: 1.03s\tremaining: 1.41s\n",
      "422:\tlearn: 79.1524736\ttotal: 1.03s\tremaining: 1.4s\n",
      "423:\tlearn: 79.1152875\ttotal: 1.03s\tremaining: 1.4s\n",
      "424:\tlearn: 79.0812035\ttotal: 1.03s\tremaining: 1.39s\n",
      "425:\tlearn: 78.7119126\ttotal: 1.03s\tremaining: 1.39s\n",
      "426:\tlearn: 78.5132649\ttotal: 1.03s\tremaining: 1.39s\n",
      "427:\tlearn: 78.0901256\ttotal: 1.03s\tremaining: 1.38s\n",
      "428:\tlearn: 77.7685056\ttotal: 1.04s\tremaining: 1.38s\n",
      "429:\tlearn: 77.6297738\ttotal: 1.04s\tremaining: 1.38s\n",
      "430:\tlearn: 77.2579792\ttotal: 1.04s\tremaining: 1.37s\n",
      "431:\tlearn: 77.0257921\ttotal: 1.04s\tremaining: 1.37s\n",
      "432:\tlearn: 76.6740420\ttotal: 1.04s\tremaining: 1.37s\n",
      "433:\tlearn: 76.3539858\ttotal: 1.04s\tremaining: 1.36s\n",
      "434:\tlearn: 76.2296226\ttotal: 1.05s\tremaining: 1.36s\n",
      "435:\tlearn: 75.8495961\ttotal: 1.05s\tremaining: 1.35s\n",
      "436:\tlearn: 75.7239458\ttotal: 1.05s\tremaining: 1.35s\n",
      "437:\tlearn: 75.3963684\ttotal: 1.05s\tremaining: 1.35s\n",
      "438:\tlearn: 75.3177406\ttotal: 1.05s\tremaining: 1.34s\n",
      "439:\tlearn: 75.0996549\ttotal: 1.05s\tremaining: 1.34s\n",
      "440:\tlearn: 74.7688646\ttotal: 1.05s\tremaining: 1.34s\n",
      "441:\tlearn: 74.3861381\ttotal: 1.06s\tremaining: 1.33s\n",
      "442:\tlearn: 74.0916973\ttotal: 1.06s\tremaining: 1.33s\n",
      "443:\tlearn: 74.0150742\ttotal: 1.06s\tremaining: 1.33s\n",
      "444:\tlearn: 73.8557599\ttotal: 1.06s\tremaining: 1.32s\n",
      "445:\tlearn: 73.8036532\ttotal: 1.06s\tremaining: 1.32s\n",
      "446:\tlearn: 73.4490340\ttotal: 1.06s\tremaining: 1.32s\n",
      "447:\tlearn: 73.1964380\ttotal: 1.07s\tremaining: 1.31s\n",
      "448:\tlearn: 72.9068005\ttotal: 1.07s\tremaining: 1.31s\n",
      "449:\tlearn: 72.6458773\ttotal: 1.07s\tremaining: 1.31s\n",
      "450:\tlearn: 72.6166476\ttotal: 1.07s\tremaining: 1.3s\n",
      "451:\tlearn: 72.3209132\ttotal: 1.07s\tremaining: 1.3s\n",
      "452:\tlearn: 72.0203859\ttotal: 1.07s\tremaining: 1.3s\n",
      "453:\tlearn: 71.8248554\ttotal: 1.07s\tremaining: 1.29s\n",
      "454:\tlearn: 71.6668281\ttotal: 1.08s\tremaining: 1.29s\n",
      "455:\tlearn: 71.3677456\ttotal: 1.08s\tremaining: 1.29s\n",
      "456:\tlearn: 71.1081280\ttotal: 1.08s\tremaining: 1.28s\n",
      "457:\tlearn: 70.8914365\ttotal: 1.08s\tremaining: 1.28s\n",
      "458:\tlearn: 70.6901776\ttotal: 1.08s\tremaining: 1.28s\n",
      "459:\tlearn: 70.3741164\ttotal: 1.08s\tremaining: 1.27s\n",
      "460:\tlearn: 70.1209176\ttotal: 1.08s\tremaining: 1.27s\n",
      "461:\tlearn: 69.8997864\ttotal: 1.09s\tremaining: 1.27s\n",
      "462:\tlearn: 69.6361849\ttotal: 1.09s\tremaining: 1.26s\n",
      "463:\tlearn: 69.4161998\ttotal: 1.09s\tremaining: 1.26s\n",
      "464:\tlearn: 69.3501700\ttotal: 1.09s\tremaining: 1.26s\n",
      "465:\tlearn: 69.3228675\ttotal: 1.09s\tremaining: 1.25s\n",
      "466:\tlearn: 69.1073131\ttotal: 1.09s\tremaining: 1.25s\n",
      "467:\tlearn: 68.9248363\ttotal: 1.1s\tremaining: 1.25s\n",
      "468:\tlearn: 68.6271067\ttotal: 1.1s\tremaining: 1.24s\n",
      "469:\tlearn: 68.3387343\ttotal: 1.1s\tremaining: 1.24s\n",
      "470:\tlearn: 68.1163177\ttotal: 1.1s\tremaining: 1.24s\n",
      "471:\tlearn: 67.8119255\ttotal: 1.1s\tremaining: 1.23s\n",
      "472:\tlearn: 67.6104548\ttotal: 1.1s\tremaining: 1.23s\n",
      "473:\tlearn: 67.5429871\ttotal: 1.11s\tremaining: 1.23s\n",
      "474:\tlearn: 67.3479160\ttotal: 1.11s\tremaining: 1.22s\n",
      "475:\tlearn: 67.1323317\ttotal: 1.11s\tremaining: 1.22s\n",
      "476:\tlearn: 66.8644912\ttotal: 1.11s\tremaining: 1.22s\n",
      "477:\tlearn: 66.6195398\ttotal: 1.11s\tremaining: 1.22s\n",
      "478:\tlearn: 66.5091835\ttotal: 1.11s\tremaining: 1.21s\n",
      "479:\tlearn: 66.3510240\ttotal: 1.12s\tremaining: 1.21s\n",
      "480:\tlearn: 66.1587116\ttotal: 1.12s\tremaining: 1.21s\n",
      "481:\tlearn: 65.9994371\ttotal: 1.12s\tremaining: 1.2s\n",
      "482:\tlearn: 65.7276876\ttotal: 1.12s\tremaining: 1.2s\n",
      "483:\tlearn: 65.5801450\ttotal: 1.12s\tremaining: 1.2s\n",
      "484:\tlearn: 65.3940860\ttotal: 1.12s\tremaining: 1.19s\n",
      "485:\tlearn: 65.2162026\ttotal: 1.13s\tremaining: 1.19s\n",
      "486:\tlearn: 65.0034745\ttotal: 1.13s\tremaining: 1.19s\n",
      "487:\tlearn: 64.8332583\ttotal: 1.13s\tremaining: 1.19s\n",
      "488:\tlearn: 64.7590817\ttotal: 1.13s\tremaining: 1.18s\n",
      "489:\tlearn: 64.4530379\ttotal: 1.13s\tremaining: 1.18s\n",
      "490:\tlearn: 64.2932478\ttotal: 1.14s\tremaining: 1.18s\n",
      "491:\tlearn: 64.0927752\ttotal: 1.14s\tremaining: 1.17s\n",
      "492:\tlearn: 63.8464997\ttotal: 1.14s\tremaining: 1.17s\n",
      "493:\tlearn: 63.8126205\ttotal: 1.14s\tremaining: 1.17s\n",
      "494:\tlearn: 63.7775575\ttotal: 1.14s\tremaining: 1.16s\n",
      "495:\tlearn: 63.5052696\ttotal: 1.14s\tremaining: 1.16s\n",
      "496:\tlearn: 63.4550342\ttotal: 1.15s\tremaining: 1.16s\n",
      "497:\tlearn: 63.4287882\ttotal: 1.15s\tremaining: 1.16s\n",
      "498:\tlearn: 63.2234937\ttotal: 1.15s\tremaining: 1.15s\n",
      "499:\tlearn: 62.9935183\ttotal: 1.15s\tremaining: 1.15s\n",
      "500:\tlearn: 62.9062417\ttotal: 1.15s\tremaining: 1.15s\n",
      "501:\tlearn: 62.8225836\ttotal: 1.15s\tremaining: 1.14s\n",
      "502:\tlearn: 62.7686909\ttotal: 1.16s\tremaining: 1.14s\n",
      "503:\tlearn: 62.5514905\ttotal: 1.16s\tremaining: 1.14s\n",
      "504:\tlearn: 62.5178340\ttotal: 1.16s\tremaining: 1.14s\n",
      "505:\tlearn: 62.3228605\ttotal: 1.16s\tremaining: 1.13s\n",
      "506:\tlearn: 62.2140494\ttotal: 1.16s\tremaining: 1.13s\n",
      "507:\tlearn: 61.9938126\ttotal: 1.16s\tremaining: 1.13s\n",
      "508:\tlearn: 61.6934772\ttotal: 1.17s\tremaining: 1.12s\n",
      "509:\tlearn: 61.3678099\ttotal: 1.17s\tremaining: 1.12s\n",
      "510:\tlearn: 60.9917264\ttotal: 1.17s\tremaining: 1.12s\n",
      "511:\tlearn: 60.8531976\ttotal: 1.17s\tremaining: 1.11s\n",
      "512:\tlearn: 60.7516851\ttotal: 1.17s\tremaining: 1.11s\n",
      "513:\tlearn: 60.7069197\ttotal: 1.17s\tremaining: 1.11s\n",
      "514:\tlearn: 60.6581275\ttotal: 1.17s\tremaining: 1.11s\n",
      "515:\tlearn: 60.4606955\ttotal: 1.18s\tremaining: 1.1s\n",
      "516:\tlearn: 60.1880369\ttotal: 1.18s\tremaining: 1.1s\n",
      "517:\tlearn: 60.1469226\ttotal: 1.18s\tremaining: 1.1s\n",
      "518:\tlearn: 59.9421390\ttotal: 1.18s\tremaining: 1.1s\n",
      "519:\tlearn: 59.8801440\ttotal: 1.18s\tremaining: 1.09s\n",
      "520:\tlearn: 59.7955649\ttotal: 1.19s\tremaining: 1.09s\n",
      "521:\tlearn: 59.4116453\ttotal: 1.19s\tremaining: 1.09s\n",
      "522:\tlearn: 59.3425227\ttotal: 1.19s\tremaining: 1.08s\n",
      "523:\tlearn: 59.1760636\ttotal: 1.19s\tremaining: 1.08s\n",
      "524:\tlearn: 58.9979641\ttotal: 1.19s\tremaining: 1.08s\n",
      "525:\tlearn: 58.8605059\ttotal: 1.19s\tremaining: 1.08s\n",
      "526:\tlearn: 58.8280110\ttotal: 1.2s\tremaining: 1.07s\n",
      "527:\tlearn: 58.5038019\ttotal: 1.2s\tremaining: 1.07s\n",
      "528:\tlearn: 58.4644117\ttotal: 1.2s\tremaining: 1.07s\n",
      "529:\tlearn: 58.2333905\ttotal: 1.2s\tremaining: 1.06s\n",
      "530:\tlearn: 58.1362916\ttotal: 1.2s\tremaining: 1.06s\n",
      "531:\tlearn: 58.1063914\ttotal: 1.2s\tremaining: 1.06s\n",
      "532:\tlearn: 57.9405254\ttotal: 1.2s\tremaining: 1.05s\n",
      "533:\tlearn: 57.7342349\ttotal: 1.21s\tremaining: 1.05s\n",
      "534:\tlearn: 57.6975764\ttotal: 1.21s\tremaining: 1.05s\n",
      "535:\tlearn: 57.5566597\ttotal: 1.21s\tremaining: 1.05s\n",
      "536:\tlearn: 57.3332107\ttotal: 1.21s\tremaining: 1.04s\n",
      "537:\tlearn: 57.0747875\ttotal: 1.21s\tremaining: 1.04s\n",
      "538:\tlearn: 56.7952430\ttotal: 1.21s\tremaining: 1.04s\n",
      "539:\tlearn: 56.7613769\ttotal: 1.22s\tremaining: 1.03s\n",
      "540:\tlearn: 56.4686940\ttotal: 1.22s\tremaining: 1.03s\n",
      "541:\tlearn: 56.2563072\ttotal: 1.22s\tremaining: 1.03s\n",
      "542:\tlearn: 56.0513646\ttotal: 1.22s\tremaining: 1.03s\n",
      "543:\tlearn: 55.9043560\ttotal: 1.22s\tremaining: 1.02s\n",
      "544:\tlearn: 55.5543108\ttotal: 1.22s\tremaining: 1.02s\n",
      "545:\tlearn: 55.4911402\ttotal: 1.22s\tremaining: 1.02s\n",
      "546:\tlearn: 55.3223043\ttotal: 1.23s\tremaining: 1.01s\n",
      "547:\tlearn: 55.2933382\ttotal: 1.23s\tremaining: 1.01s\n",
      "548:\tlearn: 55.2740844\ttotal: 1.23s\tremaining: 1.01s\n",
      "549:\tlearn: 55.1681922\ttotal: 1.23s\tremaining: 1.01s\n",
      "550:\tlearn: 55.1379298\ttotal: 1.23s\tremaining: 1s\n",
      "551:\tlearn: 55.1081992\ttotal: 1.23s\tremaining: 1s\n",
      "552:\tlearn: 54.8849899\ttotal: 1.24s\tremaining: 999ms\n",
      "553:\tlearn: 54.8565223\ttotal: 1.24s\tremaining: 996ms\n",
      "554:\tlearn: 54.6362094\ttotal: 1.24s\tremaining: 993ms\n",
      "555:\tlearn: 54.5362870\ttotal: 1.24s\tremaining: 990ms\n",
      "556:\tlearn: 54.2259374\ttotal: 1.24s\tremaining: 987ms\n",
      "557:\tlearn: 54.0476956\ttotal: 1.24s\tremaining: 985ms\n",
      "558:\tlearn: 54.0212303\ttotal: 1.24s\tremaining: 982ms\n",
      "559:\tlearn: 53.7215026\ttotal: 1.25s\tremaining: 979ms\n",
      "560:\tlearn: 53.5934533\ttotal: 1.25s\tremaining: 976ms\n",
      "561:\tlearn: 53.3433360\ttotal: 1.25s\tremaining: 974ms\n",
      "562:\tlearn: 53.2513670\ttotal: 1.25s\tremaining: 971ms\n",
      "563:\tlearn: 53.2185806\ttotal: 1.25s\tremaining: 968ms\n",
      "564:\tlearn: 53.0501117\ttotal: 1.25s\tremaining: 965ms\n",
      "565:\tlearn: 52.9454090\ttotal: 1.25s\tremaining: 963ms\n",
      "566:\tlearn: 52.8890126\ttotal: 1.26s\tremaining: 960ms\n",
      "567:\tlearn: 52.7000346\ttotal: 1.26s\tremaining: 957ms\n",
      "568:\tlearn: 52.6016792\ttotal: 1.26s\tremaining: 955ms\n",
      "569:\tlearn: 52.4024392\ttotal: 1.26s\tremaining: 952ms\n",
      "570:\tlearn: 52.2726891\ttotal: 1.26s\tremaining: 949ms\n",
      "571:\tlearn: 52.0602516\ttotal: 1.26s\tremaining: 947ms\n",
      "572:\tlearn: 51.7553292\ttotal: 1.27s\tremaining: 944ms\n",
      "573:\tlearn: 51.4614924\ttotal: 1.27s\tremaining: 942ms\n",
      "574:\tlearn: 51.3160702\ttotal: 1.27s\tremaining: 939ms\n",
      "575:\tlearn: 51.2910283\ttotal: 1.27s\tremaining: 937ms\n",
      "576:\tlearn: 51.1908245\ttotal: 1.27s\tremaining: 934ms\n",
      "577:\tlearn: 51.0969295\ttotal: 1.27s\tremaining: 931ms\n",
      "578:\tlearn: 50.9617799\ttotal: 1.28s\tremaining: 929ms\n",
      "579:\tlearn: 50.7982279\ttotal: 1.28s\tremaining: 926ms\n",
      "580:\tlearn: 50.6748803\ttotal: 1.28s\tremaining: 924ms\n",
      "581:\tlearn: 50.5292934\ttotal: 1.28s\tremaining: 921ms\n",
      "582:\tlearn: 50.3683276\ttotal: 1.28s\tremaining: 919ms\n",
      "583:\tlearn: 50.1643921\ttotal: 1.29s\tremaining: 916ms\n",
      "584:\tlearn: 50.0966036\ttotal: 1.29s\tremaining: 914ms\n",
      "585:\tlearn: 49.9599785\ttotal: 1.29s\tremaining: 911ms\n",
      "586:\tlearn: 49.8117738\ttotal: 1.29s\tremaining: 908ms\n",
      "587:\tlearn: 49.7809859\ttotal: 1.29s\tremaining: 906ms\n",
      "588:\tlearn: 49.5114528\ttotal: 1.29s\tremaining: 903ms\n",
      "589:\tlearn: 49.1783442\ttotal: 1.3s\tremaining: 901ms\n",
      "590:\tlearn: 49.0200123\ttotal: 1.3s\tremaining: 898ms\n",
      "591:\tlearn: 48.8639280\ttotal: 1.3s\tremaining: 895ms\n",
      "592:\tlearn: 48.6822276\ttotal: 1.3s\tremaining: 893ms\n",
      "593:\tlearn: 48.5901329\ttotal: 1.3s\tremaining: 890ms\n",
      "594:\tlearn: 48.5630966\ttotal: 1.3s\tremaining: 888ms\n",
      "595:\tlearn: 48.3260981\ttotal: 1.3s\tremaining: 885ms\n",
      "596:\tlearn: 48.2420288\ttotal: 1.31s\tremaining: 882ms\n",
      "597:\tlearn: 48.1114759\ttotal: 1.31s\tremaining: 880ms\n",
      "598:\tlearn: 48.0809402\ttotal: 1.31s\tremaining: 877ms\n",
      "599:\tlearn: 48.0074504\ttotal: 1.31s\tremaining: 875ms\n",
      "600:\tlearn: 47.9010464\ttotal: 1.31s\tremaining: 872ms\n",
      "601:\tlearn: 47.8829281\ttotal: 1.31s\tremaining: 870ms\n",
      "602:\tlearn: 47.7509660\ttotal: 1.32s\tremaining: 867ms\n",
      "603:\tlearn: 47.6204342\ttotal: 1.32s\tremaining: 865ms\n",
      "604:\tlearn: 47.4074585\ttotal: 1.32s\tremaining: 862ms\n",
      "605:\tlearn: 47.2729420\ttotal: 1.32s\tremaining: 860ms\n",
      "606:\tlearn: 47.0893893\ttotal: 1.32s\tremaining: 857ms\n",
      "607:\tlearn: 46.9209628\ttotal: 1.32s\tremaining: 855ms\n",
      "608:\tlearn: 46.8951075\ttotal: 1.33s\tremaining: 852ms\n",
      "609:\tlearn: 46.7730599\ttotal: 1.33s\tremaining: 850ms\n",
      "610:\tlearn: 46.6873545\ttotal: 1.33s\tremaining: 847ms\n",
      "611:\tlearn: 46.5696861\ttotal: 1.33s\tremaining: 844ms\n",
      "612:\tlearn: 46.5271435\ttotal: 1.33s\tremaining: 842ms\n",
      "613:\tlearn: 46.5117899\ttotal: 1.33s\tremaining: 840ms\n",
      "614:\tlearn: 46.3916177\ttotal: 1.34s\tremaining: 837ms\n",
      "615:\tlearn: 46.2300084\ttotal: 1.34s\tremaining: 835ms\n",
      "616:\tlearn: 46.2015160\ttotal: 1.34s\tremaining: 832ms\n",
      "617:\tlearn: 46.1858563\ttotal: 1.34s\tremaining: 829ms\n",
      "618:\tlearn: 46.0913077\ttotal: 1.34s\tremaining: 827ms\n",
      "619:\tlearn: 46.0750162\ttotal: 1.34s\tremaining: 825ms\n",
      "620:\tlearn: 46.0233767\ttotal: 1.35s\tremaining: 822ms\n",
      "621:\tlearn: 46.0084724\ttotal: 1.35s\tremaining: 820ms\n",
      "622:\tlearn: 45.9884977\ttotal: 1.35s\tremaining: 817ms\n",
      "623:\tlearn: 45.8746332\ttotal: 1.35s\tremaining: 814ms\n",
      "624:\tlearn: 45.7223902\ttotal: 1.35s\tremaining: 812ms\n",
      "625:\tlearn: 45.4986319\ttotal: 1.35s\tremaining: 809ms\n",
      "626:\tlearn: 45.3864815\ttotal: 1.36s\tremaining: 807ms\n",
      "627:\tlearn: 45.1478307\ttotal: 1.36s\tremaining: 804ms\n",
      "628:\tlearn: 45.0132723\ttotal: 1.36s\tremaining: 802ms\n",
      "629:\tlearn: 44.9394521\ttotal: 1.36s\tremaining: 800ms\n",
      "630:\tlearn: 44.8027839\ttotal: 1.36s\tremaining: 797ms\n",
      "631:\tlearn: 44.7811965\ttotal: 1.36s\tremaining: 795ms\n",
      "632:\tlearn: 44.7140165\ttotal: 1.37s\tremaining: 792ms\n",
      "633:\tlearn: 44.5493470\ttotal: 1.37s\tremaining: 790ms\n",
      "634:\tlearn: 44.3890647\ttotal: 1.37s\tremaining: 788ms\n",
      "635:\tlearn: 44.3686163\ttotal: 1.37s\tremaining: 786ms\n",
      "636:\tlearn: 44.1572143\ttotal: 1.37s\tremaining: 783ms\n",
      "637:\tlearn: 44.0238445\ttotal: 1.38s\tremaining: 780ms\n",
      "638:\tlearn: 43.8719756\ttotal: 1.38s\tremaining: 778ms\n",
      "639:\tlearn: 43.8494732\ttotal: 1.38s\tremaining: 776ms\n",
      "640:\tlearn: 43.7119241\ttotal: 1.38s\tremaining: 773ms\n",
      "641:\tlearn: 43.6962176\ttotal: 1.38s\tremaining: 771ms\n",
      "642:\tlearn: 43.6485134\ttotal: 1.38s\tremaining: 768ms\n",
      "643:\tlearn: 43.4744401\ttotal: 1.39s\tremaining: 766ms\n",
      "644:\tlearn: 43.4589473\ttotal: 1.39s\tremaining: 763ms\n",
      "645:\tlearn: 43.2476658\ttotal: 1.39s\tremaining: 761ms\n",
      "646:\tlearn: 43.0447501\ttotal: 1.39s\tremaining: 758ms\n",
      "647:\tlearn: 42.9664921\ttotal: 1.39s\tremaining: 756ms\n",
      "648:\tlearn: 42.9422112\ttotal: 1.39s\tremaining: 753ms\n",
      "649:\tlearn: 42.8704361\ttotal: 1.39s\tremaining: 751ms\n",
      "650:\tlearn: 42.6753748\ttotal: 1.4s\tremaining: 749ms\n",
      "651:\tlearn: 42.5250863\ttotal: 1.4s\tremaining: 746ms\n",
      "652:\tlearn: 42.2877256\ttotal: 1.4s\tremaining: 744ms\n",
      "653:\tlearn: 42.2756493\ttotal: 1.4s\tremaining: 741ms\n",
      "654:\tlearn: 42.2646418\ttotal: 1.4s\tremaining: 739ms\n",
      "655:\tlearn: 42.1744969\ttotal: 1.4s\tremaining: 736ms\n",
      "656:\tlearn: 41.9973058\ttotal: 1.41s\tremaining: 734ms\n",
      "657:\tlearn: 41.7819448\ttotal: 1.41s\tremaining: 732ms\n",
      "658:\tlearn: 41.6747346\ttotal: 1.41s\tremaining: 729ms\n",
      "659:\tlearn: 41.5731441\ttotal: 1.41s\tremaining: 727ms\n",
      "660:\tlearn: 41.3306636\ttotal: 1.41s\tremaining: 724ms\n",
      "661:\tlearn: 41.3190178\ttotal: 1.41s\tremaining: 722ms\n",
      "662:\tlearn: 41.2007835\ttotal: 1.42s\tremaining: 719ms\n",
      "663:\tlearn: 41.1616009\ttotal: 1.42s\tremaining: 717ms\n",
      "664:\tlearn: 41.0597519\ttotal: 1.42s\tremaining: 715ms\n",
      "665:\tlearn: 40.9287021\ttotal: 1.42s\tremaining: 713ms\n",
      "666:\tlearn: 40.8871270\ttotal: 1.42s\tremaining: 710ms\n",
      "667:\tlearn: 40.6963687\ttotal: 1.42s\tremaining: 708ms\n",
      "668:\tlearn: 40.5866470\ttotal: 1.43s\tremaining: 706ms\n",
      "669:\tlearn: 40.4931228\ttotal: 1.43s\tremaining: 703ms\n",
      "670:\tlearn: 40.3982539\ttotal: 1.43s\tremaining: 701ms\n",
      "671:\tlearn: 40.3271295\ttotal: 1.43s\tremaining: 699ms\n",
      "672:\tlearn: 40.1494909\ttotal: 1.43s\tremaining: 696ms\n",
      "673:\tlearn: 40.0225335\ttotal: 1.43s\tremaining: 694ms\n",
      "674:\tlearn: 39.8684683\ttotal: 1.44s\tremaining: 692ms\n",
      "675:\tlearn: 39.7792475\ttotal: 1.44s\tremaining: 689ms\n",
      "676:\tlearn: 39.6674371\ttotal: 1.44s\tremaining: 687ms\n",
      "677:\tlearn: 39.5590311\ttotal: 1.44s\tremaining: 685ms\n",
      "678:\tlearn: 39.3827574\ttotal: 1.44s\tremaining: 682ms\n",
      "679:\tlearn: 39.2059654\ttotal: 1.45s\tremaining: 680ms\n",
      "680:\tlearn: 39.0734978\ttotal: 1.45s\tremaining: 678ms\n",
      "681:\tlearn: 38.9773516\ttotal: 1.45s\tremaining: 675ms\n",
      "682:\tlearn: 38.9671518\ttotal: 1.45s\tremaining: 673ms\n",
      "683:\tlearn: 38.9046034\ttotal: 1.45s\tremaining: 671ms\n",
      "684:\tlearn: 38.8037881\ttotal: 1.45s\tremaining: 668ms\n",
      "685:\tlearn: 38.7538275\ttotal: 1.46s\tremaining: 666ms\n",
      "686:\tlearn: 38.7310624\ttotal: 1.46s\tremaining: 664ms\n",
      "687:\tlearn: 38.6076584\ttotal: 1.46s\tremaining: 661ms\n",
      "688:\tlearn: 38.4366157\ttotal: 1.46s\tremaining: 659ms\n",
      "689:\tlearn: 38.1884371\ttotal: 1.46s\tremaining: 657ms\n",
      "690:\tlearn: 37.9439766\ttotal: 1.46s\tremaining: 654ms\n",
      "691:\tlearn: 37.9135564\ttotal: 1.47s\tremaining: 652ms\n",
      "692:\tlearn: 37.8047154\ttotal: 1.47s\tremaining: 650ms\n",
      "693:\tlearn: 37.7944175\ttotal: 1.47s\tremaining: 648ms\n",
      "694:\tlearn: 37.7784920\ttotal: 1.47s\tremaining: 645ms\n",
      "695:\tlearn: 37.6420424\ttotal: 1.47s\tremaining: 643ms\n",
      "696:\tlearn: 37.4563091\ttotal: 1.47s\tremaining: 641ms\n",
      "697:\tlearn: 37.3137190\ttotal: 1.48s\tremaining: 638ms\n",
      "698:\tlearn: 37.2237146\ttotal: 1.48s\tremaining: 636ms\n",
      "699:\tlearn: 37.0775316\ttotal: 1.48s\tremaining: 633ms\n",
      "700:\tlearn: 36.9575266\ttotal: 1.48s\tremaining: 631ms\n",
      "701:\tlearn: 36.8744639\ttotal: 1.48s\tremaining: 629ms\n",
      "702:\tlearn: 36.7387162\ttotal: 1.48s\tremaining: 626ms\n",
      "703:\tlearn: 36.6664698\ttotal: 1.48s\tremaining: 624ms\n",
      "704:\tlearn: 36.5206927\ttotal: 1.49s\tremaining: 622ms\n",
      "705:\tlearn: 36.3590190\ttotal: 1.49s\tremaining: 620ms\n",
      "706:\tlearn: 36.3294227\ttotal: 1.49s\tremaining: 617ms\n",
      "707:\tlearn: 36.1573548\ttotal: 1.49s\tremaining: 615ms\n",
      "708:\tlearn: 36.0787153\ttotal: 1.49s\tremaining: 613ms\n",
      "709:\tlearn: 36.0354964\ttotal: 1.49s\tremaining: 611ms\n",
      "710:\tlearn: 35.9109457\ttotal: 1.5s\tremaining: 608ms\n",
      "711:\tlearn: 35.8212722\ttotal: 1.5s\tremaining: 606ms\n",
      "712:\tlearn: 35.7496720\ttotal: 1.5s\tremaining: 604ms\n",
      "713:\tlearn: 35.6649088\ttotal: 1.5s\tremaining: 601ms\n",
      "714:\tlearn: 35.5947245\ttotal: 1.5s\tremaining: 599ms\n",
      "715:\tlearn: 35.4785528\ttotal: 1.5s\tremaining: 597ms\n",
      "716:\tlearn: 35.3532288\ttotal: 1.51s\tremaining: 595ms\n",
      "717:\tlearn: 35.2009519\ttotal: 1.51s\tremaining: 593ms\n",
      "718:\tlearn: 35.0870297\ttotal: 1.51s\tremaining: 591ms\n",
      "719:\tlearn: 34.9779994\ttotal: 1.51s\tremaining: 589ms\n",
      "720:\tlearn: 34.8558836\ttotal: 1.51s\tremaining: 586ms\n",
      "721:\tlearn: 34.7807671\ttotal: 1.52s\tremaining: 584ms\n",
      "722:\tlearn: 34.6256100\ttotal: 1.52s\tremaining: 582ms\n",
      "723:\tlearn: 34.4568784\ttotal: 1.52s\tremaining: 579ms\n",
      "724:\tlearn: 34.4410020\ttotal: 1.52s\tremaining: 577ms\n",
      "725:\tlearn: 34.3478306\ttotal: 1.52s\tremaining: 575ms\n",
      "726:\tlearn: 34.2558000\ttotal: 1.52s\tremaining: 572ms\n",
      "727:\tlearn: 34.1966175\ttotal: 1.52s\tremaining: 570ms\n",
      "728:\tlearn: 34.0800437\ttotal: 1.53s\tremaining: 568ms\n",
      "729:\tlearn: 33.9816930\ttotal: 1.53s\tremaining: 566ms\n",
      "730:\tlearn: 33.8729612\ttotal: 1.53s\tremaining: 563ms\n",
      "731:\tlearn: 33.7129807\ttotal: 1.53s\tremaining: 561ms\n",
      "732:\tlearn: 33.6095421\ttotal: 1.53s\tremaining: 559ms\n",
      "733:\tlearn: 33.5988395\ttotal: 1.53s\tremaining: 556ms\n",
      "734:\tlearn: 33.4928533\ttotal: 1.54s\tremaining: 554ms\n",
      "735:\tlearn: 33.3053050\ttotal: 1.54s\tremaining: 552ms\n",
      "736:\tlearn: 33.1518712\ttotal: 1.54s\tremaining: 550ms\n",
      "737:\tlearn: 33.1293018\ttotal: 1.54s\tremaining: 547ms\n",
      "738:\tlearn: 33.1098688\ttotal: 1.54s\tremaining: 545ms\n",
      "739:\tlearn: 33.0312587\ttotal: 1.54s\tremaining: 543ms\n",
      "740:\tlearn: 32.9795019\ttotal: 1.55s\tremaining: 540ms\n",
      "741:\tlearn: 32.9383082\ttotal: 1.55s\tremaining: 538ms\n",
      "742:\tlearn: 32.7366825\ttotal: 1.55s\tremaining: 536ms\n",
      "743:\tlearn: 32.7265926\ttotal: 1.55s\tremaining: 534ms\n",
      "744:\tlearn: 32.6250393\ttotal: 1.55s\tremaining: 531ms\n",
      "745:\tlearn: 32.6115391\ttotal: 1.55s\tremaining: 529ms\n",
      "746:\tlearn: 32.5684099\ttotal: 1.55s\tremaining: 527ms\n",
      "747:\tlearn: 32.4811313\ttotal: 1.56s\tremaining: 525ms\n",
      "748:\tlearn: 32.3639862\ttotal: 1.56s\tremaining: 522ms\n",
      "749:\tlearn: 32.3489609\ttotal: 1.56s\tremaining: 520ms\n",
      "750:\tlearn: 32.3368531\ttotal: 1.56s\tremaining: 518ms\n",
      "751:\tlearn: 32.2497734\ttotal: 1.56s\tremaining: 515ms\n",
      "752:\tlearn: 32.1388841\ttotal: 1.56s\tremaining: 513ms\n",
      "753:\tlearn: 32.0438499\ttotal: 1.56s\tremaining: 511ms\n",
      "754:\tlearn: 31.9582415\ttotal: 1.57s\tremaining: 509ms\n",
      "755:\tlearn: 31.8374776\ttotal: 1.57s\tremaining: 506ms\n",
      "756:\tlearn: 31.7806673\ttotal: 1.57s\tremaining: 504ms\n",
      "757:\tlearn: 31.6913314\ttotal: 1.57s\tremaining: 502ms\n",
      "758:\tlearn: 31.6796269\ttotal: 1.57s\tremaining: 500ms\n",
      "759:\tlearn: 31.6642487\ttotal: 1.57s\tremaining: 497ms\n",
      "760:\tlearn: 31.5295848\ttotal: 1.58s\tremaining: 495ms\n",
      "761:\tlearn: 31.4582803\ttotal: 1.58s\tremaining: 493ms\n",
      "762:\tlearn: 31.3355570\ttotal: 1.58s\tremaining: 491ms\n",
      "763:\tlearn: 31.1735475\ttotal: 1.58s\tremaining: 489ms\n",
      "764:\tlearn: 31.0330783\ttotal: 1.58s\tremaining: 487ms\n",
      "765:\tlearn: 30.9183023\ttotal: 1.58s\tremaining: 484ms\n",
      "766:\tlearn: 30.8639902\ttotal: 1.59s\tremaining: 482ms\n",
      "767:\tlearn: 30.7645069\ttotal: 1.59s\tremaining: 480ms\n",
      "768:\tlearn: 30.7066892\ttotal: 1.59s\tremaining: 478ms\n",
      "769:\tlearn: 30.5882950\ttotal: 1.59s\tremaining: 476ms\n",
      "770:\tlearn: 30.4668246\ttotal: 1.59s\tremaining: 474ms\n",
      "771:\tlearn: 30.3389610\ttotal: 1.6s\tremaining: 471ms\n",
      "772:\tlearn: 30.2647741\ttotal: 1.6s\tremaining: 469ms\n",
      "773:\tlearn: 30.2570442\ttotal: 1.6s\tremaining: 467ms\n",
      "774:\tlearn: 30.1542780\ttotal: 1.6s\tremaining: 465ms\n",
      "775:\tlearn: 30.0857401\ttotal: 1.6s\tremaining: 463ms\n",
      "776:\tlearn: 30.0752285\ttotal: 1.6s\tremaining: 461ms\n",
      "777:\tlearn: 29.9620035\ttotal: 1.61s\tremaining: 458ms\n",
      "778:\tlearn: 29.8923860\ttotal: 1.61s\tremaining: 456ms\n",
      "779:\tlearn: 29.8234949\ttotal: 1.61s\tremaining: 454ms\n",
      "780:\tlearn: 29.6747185\ttotal: 1.61s\tremaining: 452ms\n",
      "781:\tlearn: 29.6402997\ttotal: 1.61s\tremaining: 450ms\n",
      "782:\tlearn: 29.6053874\ttotal: 1.61s\tremaining: 447ms\n",
      "783:\tlearn: 29.5882949\ttotal: 1.62s\tremaining: 445ms\n",
      "784:\tlearn: 29.5772302\ttotal: 1.62s\tremaining: 443ms\n",
      "785:\tlearn: 29.4825983\ttotal: 1.62s\tremaining: 441ms\n",
      "786:\tlearn: 29.4269270\ttotal: 1.62s\tremaining: 439ms\n",
      "787:\tlearn: 29.3079818\ttotal: 1.62s\tremaining: 436ms\n",
      "788:\tlearn: 29.2047494\ttotal: 1.62s\tremaining: 434ms\n",
      "789:\tlearn: 29.1549181\ttotal: 1.63s\tremaining: 432ms\n",
      "790:\tlearn: 29.0812662\ttotal: 1.63s\tremaining: 430ms\n",
      "791:\tlearn: 28.9935837\ttotal: 1.63s\tremaining: 428ms\n",
      "792:\tlearn: 28.9002410\ttotal: 1.63s\tremaining: 426ms\n",
      "793:\tlearn: 28.8881838\ttotal: 1.63s\tremaining: 424ms\n",
      "794:\tlearn: 28.7968846\ttotal: 1.63s\tremaining: 422ms\n",
      "795:\tlearn: 28.7039130\ttotal: 1.64s\tremaining: 420ms\n",
      "796:\tlearn: 28.6934015\ttotal: 1.64s\tremaining: 417ms\n",
      "797:\tlearn: 28.5879565\ttotal: 1.64s\tremaining: 415ms\n",
      "798:\tlearn: 28.4951865\ttotal: 1.64s\tremaining: 413ms\n",
      "799:\tlearn: 28.3744300\ttotal: 1.65s\tremaining: 411ms\n",
      "800:\tlearn: 28.2841650\ttotal: 1.65s\tremaining: 409ms\n",
      "801:\tlearn: 28.2252838\ttotal: 1.65s\tremaining: 407ms\n",
      "802:\tlearn: 28.2151559\ttotal: 1.65s\tremaining: 405ms\n",
      "803:\tlearn: 28.1384036\ttotal: 1.65s\tremaining: 403ms\n",
      "804:\tlearn: 28.0383475\ttotal: 1.66s\tremaining: 401ms\n",
      "805:\tlearn: 27.9122376\ttotal: 1.66s\tremaining: 399ms\n",
      "806:\tlearn: 27.7751982\ttotal: 1.66s\tremaining: 397ms\n",
      "807:\tlearn: 27.6963698\ttotal: 1.66s\tremaining: 395ms\n",
      "808:\tlearn: 27.6280973\ttotal: 1.66s\tremaining: 393ms\n",
      "809:\tlearn: 27.5714899\ttotal: 1.67s\tremaining: 391ms\n",
      "810:\tlearn: 27.5116406\ttotal: 1.67s\tremaining: 389ms\n",
      "811:\tlearn: 27.4969238\ttotal: 1.67s\tremaining: 386ms\n",
      "812:\tlearn: 27.4440190\ttotal: 1.67s\tremaining: 384ms\n",
      "813:\tlearn: 27.3108750\ttotal: 1.67s\tremaining: 382ms\n",
      "814:\tlearn: 27.2140306\ttotal: 1.67s\tremaining: 380ms\n",
      "815:\tlearn: 27.1343699\ttotal: 1.68s\tremaining: 378ms\n",
      "816:\tlearn: 27.0634231\ttotal: 1.68s\tremaining: 376ms\n",
      "817:\tlearn: 27.0120786\ttotal: 1.68s\tremaining: 374ms\n",
      "818:\tlearn: 26.9243366\ttotal: 1.68s\tremaining: 372ms\n",
      "819:\tlearn: 26.9143121\ttotal: 1.68s\tremaining: 369ms\n",
      "820:\tlearn: 26.9003226\ttotal: 1.68s\tremaining: 367ms\n",
      "821:\tlearn: 26.7911190\ttotal: 1.69s\tremaining: 365ms\n",
      "822:\tlearn: 26.6770786\ttotal: 1.69s\tremaining: 363ms\n",
      "823:\tlearn: 26.5838578\ttotal: 1.69s\tremaining: 361ms\n",
      "824:\tlearn: 26.5165921\ttotal: 1.69s\tremaining: 359ms\n",
      "825:\tlearn: 26.4543137\ttotal: 1.69s\tremaining: 357ms\n",
      "826:\tlearn: 26.3716215\ttotal: 1.7s\tremaining: 355ms\n",
      "827:\tlearn: 26.3074656\ttotal: 1.7s\tremaining: 352ms\n",
      "828:\tlearn: 26.2712208\ttotal: 1.7s\tremaining: 350ms\n",
      "829:\tlearn: 26.1713329\ttotal: 1.7s\tremaining: 348ms\n",
      "830:\tlearn: 26.1571002\ttotal: 1.7s\tremaining: 346ms\n",
      "831:\tlearn: 26.0572999\ttotal: 1.7s\tremaining: 344ms\n",
      "832:\tlearn: 25.9906829\ttotal: 1.71s\tremaining: 342ms\n",
      "833:\tlearn: 25.8988396\ttotal: 1.71s\tremaining: 340ms\n",
      "834:\tlearn: 25.8896369\ttotal: 1.71s\tremaining: 338ms\n",
      "835:\tlearn: 25.8058243\ttotal: 1.71s\tremaining: 336ms\n",
      "836:\tlearn: 25.7993208\ttotal: 1.71s\tremaining: 334ms\n",
      "837:\tlearn: 25.7353722\ttotal: 1.71s\tremaining: 331ms\n",
      "838:\tlearn: 25.7261573\ttotal: 1.72s\tremaining: 329ms\n",
      "839:\tlearn: 25.6260729\ttotal: 1.72s\tremaining: 328ms\n",
      "840:\tlearn: 25.4824933\ttotal: 1.72s\tremaining: 326ms\n",
      "841:\tlearn: 25.3765205\ttotal: 1.72s\tremaining: 323ms\n",
      "842:\tlearn: 25.2934527\ttotal: 1.73s\tremaining: 321ms\n",
      "843:\tlearn: 25.2287342\ttotal: 1.73s\tremaining: 319ms\n",
      "844:\tlearn: 25.1577010\ttotal: 1.73s\tremaining: 317ms\n",
      "845:\tlearn: 25.0508363\ttotal: 1.73s\tremaining: 315ms\n",
      "846:\tlearn: 24.9640379\ttotal: 1.73s\tremaining: 313ms\n",
      "847:\tlearn: 24.8962252\ttotal: 1.73s\tremaining: 311ms\n",
      "848:\tlearn: 24.7805520\ttotal: 1.74s\tremaining: 309ms\n",
      "849:\tlearn: 24.7165739\ttotal: 1.74s\tremaining: 307ms\n",
      "850:\tlearn: 24.6036026\ttotal: 1.74s\tremaining: 305ms\n",
      "851:\tlearn: 24.5072401\ttotal: 1.74s\tremaining: 303ms\n",
      "852:\tlearn: 24.4377646\ttotal: 1.74s\tremaining: 301ms\n",
      "853:\tlearn: 24.3496516\ttotal: 1.75s\tremaining: 299ms\n",
      "854:\tlearn: 24.2469440\ttotal: 1.75s\tremaining: 297ms\n",
      "855:\tlearn: 24.1966138\ttotal: 1.75s\tremaining: 294ms\n",
      "856:\tlearn: 24.1538044\ttotal: 1.75s\tremaining: 292ms\n",
      "857:\tlearn: 24.0946195\ttotal: 1.75s\tremaining: 290ms\n",
      "858:\tlearn: 24.0329702\ttotal: 1.75s\tremaining: 288ms\n",
      "859:\tlearn: 24.0174990\ttotal: 1.76s\tremaining: 286ms\n",
      "860:\tlearn: 23.9402502\ttotal: 1.76s\tremaining: 284ms\n",
      "861:\tlearn: 23.8840321\ttotal: 1.76s\tremaining: 282ms\n",
      "862:\tlearn: 23.8446783\ttotal: 1.76s\tremaining: 280ms\n",
      "863:\tlearn: 23.8118180\ttotal: 1.76s\tremaining: 278ms\n",
      "864:\tlearn: 23.8047264\ttotal: 1.77s\tremaining: 276ms\n",
      "865:\tlearn: 23.7323403\ttotal: 1.77s\tremaining: 274ms\n",
      "866:\tlearn: 23.6402057\ttotal: 1.77s\tremaining: 272ms\n",
      "867:\tlearn: 23.5629536\ttotal: 1.77s\tremaining: 269ms\n",
      "868:\tlearn: 23.4577896\ttotal: 1.77s\tremaining: 267ms\n",
      "869:\tlearn: 23.4014413\ttotal: 1.77s\tremaining: 265ms\n",
      "870:\tlearn: 23.3909432\ttotal: 1.78s\tremaining: 263ms\n",
      "871:\tlearn: 23.3851909\ttotal: 1.78s\tremaining: 261ms\n",
      "872:\tlearn: 23.3771603\ttotal: 1.78s\tremaining: 259ms\n",
      "873:\tlearn: 23.3088102\ttotal: 1.78s\tremaining: 257ms\n",
      "874:\tlearn: 23.2569877\ttotal: 1.78s\tremaining: 255ms\n",
      "875:\tlearn: 23.2000879\ttotal: 1.78s\tremaining: 253ms\n",
      "876:\tlearn: 23.1256717\ttotal: 1.79s\tremaining: 251ms\n",
      "877:\tlearn: 23.0341676\ttotal: 1.79s\tremaining: 249ms\n",
      "878:\tlearn: 22.9342326\ttotal: 1.79s\tremaining: 247ms\n",
      "879:\tlearn: 22.8322531\ttotal: 1.79s\tremaining: 244ms\n",
      "880:\tlearn: 22.7729809\ttotal: 1.79s\tremaining: 242ms\n",
      "881:\tlearn: 22.7637911\ttotal: 1.79s\tremaining: 240ms\n",
      "882:\tlearn: 22.7046149\ttotal: 1.8s\tremaining: 238ms\n",
      "883:\tlearn: 22.5928115\ttotal: 1.8s\tremaining: 236ms\n",
      "884:\tlearn: 22.5100119\ttotal: 1.8s\tremaining: 234ms\n",
      "885:\tlearn: 22.4408533\ttotal: 1.8s\tremaining: 232ms\n",
      "886:\tlearn: 22.3448548\ttotal: 1.8s\tremaining: 230ms\n",
      "887:\tlearn: 22.2756650\ttotal: 1.81s\tremaining: 228ms\n",
      "888:\tlearn: 22.2480353\ttotal: 1.81s\tremaining: 226ms\n",
      "889:\tlearn: 22.1732081\ttotal: 1.81s\tremaining: 224ms\n",
      "890:\tlearn: 22.1020126\ttotal: 1.81s\tremaining: 222ms\n",
      "891:\tlearn: 22.0567791\ttotal: 1.81s\tremaining: 220ms\n",
      "892:\tlearn: 21.9820917\ttotal: 1.81s\tremaining: 218ms\n",
      "893:\tlearn: 21.9591813\ttotal: 1.82s\tremaining: 215ms\n",
      "894:\tlearn: 21.9352816\ttotal: 1.82s\tremaining: 213ms\n",
      "895:\tlearn: 21.8748561\ttotal: 1.82s\tremaining: 211ms\n",
      "896:\tlearn: 21.7883753\ttotal: 1.82s\tremaining: 209ms\n",
      "897:\tlearn: 21.7130624\ttotal: 1.82s\tremaining: 207ms\n",
      "898:\tlearn: 21.6344727\ttotal: 1.82s\tremaining: 205ms\n",
      "899:\tlearn: 21.6278292\ttotal: 1.83s\tremaining: 203ms\n",
      "900:\tlearn: 21.5987855\ttotal: 1.83s\tremaining: 201ms\n",
      "901:\tlearn: 21.5810069\ttotal: 1.83s\tremaining: 199ms\n",
      "902:\tlearn: 21.4806389\ttotal: 1.83s\tremaining: 197ms\n",
      "903:\tlearn: 21.4530857\ttotal: 1.83s\tremaining: 195ms\n",
      "904:\tlearn: 21.3904041\ttotal: 1.84s\tremaining: 193ms\n",
      "905:\tlearn: 21.2975893\ttotal: 1.84s\tremaining: 191ms\n",
      "906:\tlearn: 21.2145968\ttotal: 1.84s\tremaining: 189ms\n",
      "907:\tlearn: 21.1587833\ttotal: 1.84s\tremaining: 187ms\n",
      "908:\tlearn: 21.0772042\ttotal: 1.84s\tremaining: 185ms\n",
      "909:\tlearn: 21.0389130\ttotal: 1.84s\tremaining: 182ms\n",
      "910:\tlearn: 20.9610184\ttotal: 1.85s\tremaining: 180ms\n",
      "911:\tlearn: 20.9381137\ttotal: 1.85s\tremaining: 178ms\n",
      "912:\tlearn: 20.8637506\ttotal: 1.85s\tremaining: 176ms\n",
      "913:\tlearn: 20.8124838\ttotal: 1.85s\tremaining: 174ms\n",
      "914:\tlearn: 20.7635903\ttotal: 1.85s\tremaining: 172ms\n",
      "915:\tlearn: 20.7174417\ttotal: 1.85s\tremaining: 170ms\n",
      "916:\tlearn: 20.6602099\ttotal: 1.86s\tremaining: 168ms\n",
      "917:\tlearn: 20.6116946\ttotal: 1.86s\tremaining: 166ms\n",
      "918:\tlearn: 20.5586510\ttotal: 1.86s\tremaining: 164ms\n",
      "919:\tlearn: 20.5183001\ttotal: 1.86s\tremaining: 162ms\n",
      "920:\tlearn: 20.4025518\ttotal: 1.86s\tremaining: 160ms\n",
      "921:\tlearn: 20.3333571\ttotal: 1.87s\tremaining: 158ms\n",
      "922:\tlearn: 20.2696755\ttotal: 1.87s\tremaining: 156ms\n",
      "923:\tlearn: 20.2238639\ttotal: 1.87s\tremaining: 154ms\n",
      "924:\tlearn: 20.1888151\ttotal: 1.87s\tremaining: 152ms\n",
      "925:\tlearn: 20.1443962\ttotal: 1.87s\tremaining: 150ms\n",
      "926:\tlearn: 20.0624152\ttotal: 1.88s\tremaining: 148ms\n",
      "927:\tlearn: 20.0549863\ttotal: 1.88s\tremaining: 146ms\n",
      "928:\tlearn: 20.0087975\ttotal: 1.88s\tremaining: 144ms\n",
      "929:\tlearn: 19.9800518\ttotal: 1.88s\tremaining: 142ms\n",
      "930:\tlearn: 19.9527169\ttotal: 1.88s\tremaining: 140ms\n",
      "931:\tlearn: 19.8624633\ttotal: 1.88s\tremaining: 137ms\n",
      "932:\tlearn: 19.8035732\ttotal: 1.89s\tremaining: 135ms\n",
      "933:\tlearn: 19.7964147\ttotal: 1.89s\tremaining: 133ms\n",
      "934:\tlearn: 19.7915529\ttotal: 1.89s\tremaining: 131ms\n",
      "935:\tlearn: 19.6839212\ttotal: 1.89s\tremaining: 129ms\n",
      "936:\tlearn: 19.6101624\ttotal: 1.89s\tremaining: 127ms\n",
      "937:\tlearn: 19.5663321\ttotal: 1.89s\tremaining: 125ms\n",
      "938:\tlearn: 19.5285472\ttotal: 1.9s\tremaining: 123ms\n",
      "939:\tlearn: 19.4863317\ttotal: 1.9s\tremaining: 121ms\n",
      "940:\tlearn: 19.4327846\ttotal: 1.9s\tremaining: 119ms\n",
      "941:\tlearn: 19.3859194\ttotal: 1.9s\tremaining: 117ms\n",
      "942:\tlearn: 19.2794870\ttotal: 1.9s\tremaining: 115ms\n",
      "943:\tlearn: 19.2736051\ttotal: 1.91s\tremaining: 113ms\n",
      "944:\tlearn: 19.2100793\ttotal: 1.91s\tremaining: 111ms\n",
      "945:\tlearn: 19.1718278\ttotal: 1.91s\tremaining: 109ms\n",
      "946:\tlearn: 19.1056639\ttotal: 1.91s\tremaining: 107ms\n",
      "947:\tlearn: 19.0579771\ttotal: 1.91s\tremaining: 105ms\n",
      "948:\tlearn: 18.9854565\ttotal: 1.91s\tremaining: 103ms\n",
      "949:\tlearn: 18.9451363\ttotal: 1.92s\tremaining: 101ms\n",
      "950:\tlearn: 18.9096636\ttotal: 1.92s\tremaining: 98.8ms\n",
      "951:\tlearn: 18.7965154\ttotal: 1.92s\tremaining: 96.8ms\n",
      "952:\tlearn: 18.7900195\ttotal: 1.92s\tremaining: 94.8ms\n",
      "953:\tlearn: 18.7008434\ttotal: 1.92s\tremaining: 92.7ms\n",
      "954:\tlearn: 18.6611313\ttotal: 1.93s\tremaining: 90.7ms\n",
      "955:\tlearn: 18.6088355\ttotal: 1.93s\tremaining: 88.7ms\n",
      "956:\tlearn: 18.5735209\ttotal: 1.93s\tremaining: 86.7ms\n",
      "957:\tlearn: 18.5052518\ttotal: 1.93s\tremaining: 84.6ms\n",
      "958:\tlearn: 18.4689417\ttotal: 1.93s\tremaining: 82.6ms\n",
      "959:\tlearn: 18.4053473\ttotal: 1.93s\tremaining: 80.6ms\n",
      "960:\tlearn: 18.3627825\ttotal: 1.94s\tremaining: 78.5ms\n",
      "961:\tlearn: 18.3053806\ttotal: 1.94s\tremaining: 76.5ms\n",
      "962:\tlearn: 18.2331868\ttotal: 1.94s\tremaining: 74.5ms\n",
      "963:\tlearn: 18.2243249\ttotal: 1.94s\tremaining: 72.4ms\n",
      "964:\tlearn: 18.1896679\ttotal: 1.94s\tremaining: 70.4ms\n",
      "965:\tlearn: 18.1410677\ttotal: 1.94s\tremaining: 68.4ms\n",
      "966:\tlearn: 18.1288173\ttotal: 1.95s\tremaining: 66.4ms\n",
      "967:\tlearn: 18.0369781\ttotal: 1.95s\tremaining: 64.4ms\n",
      "968:\tlearn: 17.9653938\ttotal: 1.95s\tremaining: 62.3ms\n",
      "969:\tlearn: 17.8950563\ttotal: 1.95s\tremaining: 60.3ms\n",
      "970:\tlearn: 17.8787459\ttotal: 1.95s\tremaining: 58.3ms\n",
      "971:\tlearn: 17.8125948\ttotal: 1.95s\tremaining: 56.3ms\n",
      "972:\tlearn: 17.7583898\ttotal: 1.95s\tremaining: 54.2ms\n",
      "973:\tlearn: 17.6843936\ttotal: 1.96s\tremaining: 52.2ms\n",
      "974:\tlearn: 17.6295575\ttotal: 1.96s\tremaining: 50.2ms\n",
      "975:\tlearn: 17.5811266\ttotal: 1.96s\tremaining: 48.2ms\n",
      "976:\tlearn: 17.5238701\ttotal: 1.96s\tremaining: 46.2ms\n",
      "977:\tlearn: 17.4744794\ttotal: 1.96s\tremaining: 44.2ms\n",
      "978:\tlearn: 17.4528556\ttotal: 1.96s\tremaining: 42.1ms\n",
      "979:\tlearn: 17.4158144\ttotal: 1.97s\tremaining: 40.1ms\n",
      "980:\tlearn: 17.3661608\ttotal: 1.97s\tremaining: 38.1ms\n",
      "981:\tlearn: 17.2950432\ttotal: 1.97s\tremaining: 36.1ms\n",
      "982:\tlearn: 17.2622661\ttotal: 1.97s\tremaining: 34.1ms\n",
      "983:\tlearn: 17.2300247\ttotal: 1.97s\tremaining: 32.1ms\n",
      "984:\tlearn: 17.1816076\ttotal: 1.98s\tremaining: 30.1ms\n",
      "985:\tlearn: 17.1521909\ttotal: 1.98s\tremaining: 28.1ms\n",
      "986:\tlearn: 17.1085063\ttotal: 1.98s\tremaining: 26.1ms\n",
      "987:\tlearn: 17.0464697\ttotal: 1.98s\tremaining: 24.1ms\n",
      "988:\tlearn: 16.9725967\ttotal: 1.98s\tremaining: 22.1ms\n",
      "989:\tlearn: 16.9172341\ttotal: 1.98s\tremaining: 20ms\n",
      "990:\tlearn: 16.8802214\ttotal: 1.99s\tremaining: 18ms\n",
      "991:\tlearn: 16.8383822\ttotal: 1.99s\tremaining: 16ms\n",
      "992:\tlearn: 16.8159493\ttotal: 1.99s\tremaining: 14ms\n",
      "993:\tlearn: 16.8041797\ttotal: 1.99s\tremaining: 12ms\n",
      "994:\tlearn: 16.7960658\ttotal: 1.99s\tremaining: 10ms\n",
      "995:\tlearn: 16.7552061\ttotal: 1.99s\tremaining: 8.01ms\n",
      "996:\tlearn: 16.7111984\ttotal: 2s\tremaining: 6.01ms\n",
      "997:\tlearn: 16.6706212\ttotal: 2s\tremaining: 4ms\n",
      "998:\tlearn: 16.6357406\ttotal: 2s\tremaining: 2ms\n",
      "999:\tlearn: 16.5627688\ttotal: 2s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "258.90197830660554"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catb = CatBoostRegressor()\n",
    "catb_model = catb.fit(X_train, y_train)\n",
    "y_pred = catb_model.predict(X_test)\n",
    "df_catb_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "df_catb_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On üç modelin Kök Ortalama Kare Hataları (RMSE)\n",
    "# CatBoostRegressor'u dahil ettiğimde yaklaşık 2 saat sürdüğü için CatBoostRegressor'u dahil etmeyeceğim.\n",
    "# Zaman kazanmak için ayrı ayrı rapor edeceğim\n",
    "\n",
    "def compML(df, y, alg):\n",
    "    model = alg().fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    RMSE = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    model_name = alg.__name__\n",
    "    print(model_name, \"Model RMSE:\", RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LinearRegression, Ridge, Lasso, ElasticNet, KNeighborsRegressor, SVR, MLPRegressor, DecisionTreeRegressor, \n",
    "          RandomForestRegressor, GradientBoostingRegressor, XGBRegressor, LGBMRegressor] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression Model RMSE: 327.3400731081062\n",
      "Ridge Model RMSE: 315.28976474517196\n",
      "Lasso Model RMSE: 318.6803120461422\n",
      "ElasticNet Model RMSE: 302.78610988935907\n",
      "KNeighborsRegressor Model RMSE: 315.07079037040967\n",
      "SVR Model RMSE: 330.74557635195526\n",
      "MLPRegressor Model RMSE: 504.8537701381368\n",
      "DecisionTreeRegressor Model RMSE: 396.43616802451504\n",
      "RandomForestRegressor Model RMSE: 283.7196858255249\n",
      "GradientBoostingRegressor Model RMSE: 267.87313763660774\n",
      "XGBRegressor Model RMSE: 324.3984254581684\n",
      "LGBMRegressor Model RMSE: 276.06253566372055\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    compML(df, 'Salary', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temel makine öğrenimi modelleri arasında CatBoost (Kategori Yükseltme) modeli, 258.90197830660554 RMSE değeri ile maaş tahmininde en iyi modeldir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326.89105254127037"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Çapraz doğrulama işleviyle hiper parametre optimizasyonu.\n",
    "# Yeni alfa değerleri atayarak modeli ayarlamaya çalışacağız.\n",
    "# Ridge regresyonunda varsayılan alfa değeri 1.0'dır. Farklı değerler deneyeceğiz.\n",
    "# En uygun alfa değeri veya parametresi nihai modelde kullanılacaktır.\n",
    "\n",
    "alpha = [0.1,0.01,0.001,0.2,0.3,0.5,0.8,0.9,1]\n",
    "ridreg_cv = RidgeCV(alphas = alpha, scoring = \"neg_mean_squared_error\", cv = 10, normalize = True)\n",
    "ridreg_cv.fit(X_train, y_train)\n",
    "ridreg_cv.alpha_\n",
    "\n",
    "#Final Model \n",
    "\n",
    "ridreg_tuned = Ridge(alpha = ridreg_cv.alpha_).fit(X_train,y_train)\n",
    "y_pred = ridreg_tuned.predict(X_test)\n",
    "df_ridge_tuned_rmse = np.sqrt(mean_squared_error(y_test,y_pred))\n",
    "df_ridge_tuned_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "323.4231245802036"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Çapraz doğrulama işleviyle hiper parametre optimizasyonu.\n",
    "# Yeni alfa değerleri atayarak modeli ayarlamaya çalışacağız.\n",
    "# Kement regresyonunda varsayılan alfa değeri 1.0'dır. Farklı değerler deneyeceğiz.\n",
    "# En uygun alfa değeri veya parametresi nihai modelde kullanılacaktır.\n",
    "\n",
    "alpha = [0.1,0.01,0.001,0.2,0.3,0.5,0.8,0.9,1]\n",
    "lasso_cv = LassoCV(alphas = alpha, cv = 10, normalize = True)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "lasso_cv.alpha_\n",
    "\n",
    "# Final Model \n",
    "\n",
    "lasso_tuned = Lasso(alpha = lasso_cv.alpha_).fit(X_train,y_train)\n",
    "y_pred = lasso_tuned.predict(X_test)\n",
    "df_lasso_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "df_lasso_tuned_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mLasso\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfit_intercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mprecompute\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcopy_X\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mselection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cyclic'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Linear Model trained with L1 prior as regularizer (aka the Lasso)\n",
       "\n",
       "The optimization objective for Lasso is::\n",
       "\n",
       "    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
       "\n",
       "Technically the Lasso model is optimizing the same objective function as\n",
       "the Elastic Net with ``l1_ratio=1.0`` (no L2 penalty).\n",
       "\n",
       "Read more in the :ref:`User Guide <lasso>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "alpha : float, default=1.0\n",
       "    Constant that multiplies the L1 term. Defaults to 1.0.\n",
       "    ``alpha = 0`` is equivalent to an ordinary least square, solved\n",
       "    by the :class:`LinearRegression` object. For numerical\n",
       "    reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n",
       "    Given this, you should use the :class:`LinearRegression` object.\n",
       "\n",
       "fit_intercept : bool, default=True\n",
       "    Whether to calculate the intercept for this model. If set\n",
       "    to False, no intercept will be used in calculations\n",
       "    (i.e. data is expected to be centered).\n",
       "\n",
       "normalize : bool, default=False\n",
       "    This parameter is ignored when ``fit_intercept`` is set to False.\n",
       "    If True, the regressors X will be normalized before regression by\n",
       "    subtracting the mean and dividing by the l2-norm.\n",
       "    If you wish to standardize, please use\n",
       "    :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
       "    on an estimator with ``normalize=False``.\n",
       "\n",
       "precompute : 'auto', bool or array-like of shape (n_features, n_features),                 default=False\n",
       "    Whether to use a precomputed Gram matrix to speed up\n",
       "    calculations. If set to ``'auto'`` let us decide. The Gram\n",
       "    matrix can also be passed as argument. For sparse input\n",
       "    this option is always ``True`` to preserve sparsity.\n",
       "\n",
       "copy_X : bool, default=True\n",
       "    If ``True``, X will be copied; else, it may be overwritten.\n",
       "\n",
       "max_iter : int, default=1000\n",
       "    The maximum number of iterations\n",
       "\n",
       "tol : float, default=1e-4\n",
       "    The tolerance for the optimization: if the updates are\n",
       "    smaller than ``tol``, the optimization code checks the\n",
       "    dual gap for optimality and continues until it is smaller\n",
       "    than ``tol``.\n",
       "\n",
       "warm_start : bool, default=False\n",
       "    When set to True, reuse the solution of the previous call to fit as\n",
       "    initialization, otherwise, just erase the previous solution.\n",
       "    See :term:`the Glossary <warm_start>`.\n",
       "\n",
       "positive : bool, default=False\n",
       "    When set to ``True``, forces the coefficients to be positive.\n",
       "\n",
       "random_state : int, RandomState instance, default=None\n",
       "    The seed of the pseudo random number generator that selects a random\n",
       "    feature to update. Used when ``selection`` == 'random'.\n",
       "    Pass an int for reproducible output across multiple function calls.\n",
       "    See :term:`Glossary <random_state>`.\n",
       "\n",
       "selection : {'cyclic', 'random'}, default='cyclic'\n",
       "    If set to 'random', a random coefficient is updated every iteration\n",
       "    rather than looping over features sequentially by default. This\n",
       "    (setting to 'random') often leads to significantly faster convergence\n",
       "    especially when tol is higher than 1e-4.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
       "    parameter vector (w in the cost function formula)\n",
       "\n",
       "sparse_coef_ : sparse matrix of shape (n_features, 1) or             (n_targets, n_features)\n",
       "    ``sparse_coef_`` is a readonly property derived from ``coef_``\n",
       "\n",
       "intercept_ : float or ndarray of shape (n_targets,)\n",
       "    independent term in decision function.\n",
       "\n",
       "n_iter_ : int or list of int\n",
       "    number of iterations run by the coordinate descent solver to reach\n",
       "    the specified tolerance.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn import linear_model\n",
       ">>> clf = linear_model.Lasso(alpha=0.1)\n",
       ">>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
       "Lasso(alpha=0.1)\n",
       ">>> print(clf.coef_)\n",
       "[0.85 0.  ]\n",
       ">>> print(clf.intercept_)\n",
       "0.15...\n",
       "\n",
       "See also\n",
       "--------\n",
       "lars_path\n",
       "lasso_path\n",
       "LassoLars\n",
       "LassoCV\n",
       "LassoLarsCV\n",
       "sklearn.decomposition.sparse_encode\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The algorithm used to fit the model is coordinate descent.\n",
       "\n",
       "To avoid unnecessary memory duplication the X argument of the fit method\n",
       "should be directly passed as a Fortran-contiguous numpy array.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\toshiba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     MultiTaskElasticNet\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net Regression Regression Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mElasticNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0ml1_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfit_intercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mprecompute\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcopy_X\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mselection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cyclic'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Linear regression with combined L1 and L2 priors as regularizer.\n",
       "\n",
       "Minimizes the objective function::\n",
       "\n",
       "        1 / (2 * n_samples) * ||y - Xw||^2_2\n",
       "        + alpha * l1_ratio * ||w||_1\n",
       "        + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
       "\n",
       "If you are interested in controlling the L1 and L2 penalty\n",
       "separately, keep in mind that this is equivalent to::\n",
       "\n",
       "        a * L1 + b * L2\n",
       "\n",
       "where::\n",
       "\n",
       "        alpha = a + b and l1_ratio = a / (a + b)\n",
       "\n",
       "The parameter l1_ratio corresponds to alpha in the glmnet R package while\n",
       "alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio\n",
       "= 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable,\n",
       "unless you supply your own sequence of alpha.\n",
       "\n",
       "Read more in the :ref:`User Guide <elastic_net>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "alpha : float, default=1.0\n",
       "    Constant that multiplies the penalty terms. Defaults to 1.0.\n",
       "    See the notes for the exact mathematical meaning of this\n",
       "    parameter. ``alpha = 0`` is equivalent to an ordinary least square,\n",
       "    solved by the :class:`LinearRegression` object. For numerical\n",
       "    reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n",
       "    Given this, you should use the :class:`LinearRegression` object.\n",
       "\n",
       "l1_ratio : float, default=0.5\n",
       "    The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For\n",
       "    ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it\n",
       "    is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a\n",
       "    combination of L1 and L2.\n",
       "\n",
       "fit_intercept : bool, default=True\n",
       "    Whether the intercept should be estimated or not. If ``False``, the\n",
       "    data is assumed to be already centered.\n",
       "\n",
       "normalize : bool, default=False\n",
       "    This parameter is ignored when ``fit_intercept`` is set to False.\n",
       "    If True, the regressors X will be normalized before regression by\n",
       "    subtracting the mean and dividing by the l2-norm.\n",
       "    If you wish to standardize, please use\n",
       "    :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
       "    on an estimator with ``normalize=False``.\n",
       "\n",
       "precompute : bool or array-like of shape (n_features, n_features),                 default=False\n",
       "    Whether to use a precomputed Gram matrix to speed up\n",
       "    calculations. The Gram matrix can also be passed as argument.\n",
       "    For sparse input this option is always ``True`` to preserve sparsity.\n",
       "\n",
       "max_iter : int, default=1000\n",
       "    The maximum number of iterations\n",
       "\n",
       "copy_X : bool, default=True\n",
       "    If ``True``, X will be copied; else, it may be overwritten.\n",
       "\n",
       "tol : float, default=1e-4\n",
       "    The tolerance for the optimization: if the updates are\n",
       "    smaller than ``tol``, the optimization code checks the\n",
       "    dual gap for optimality and continues until it is smaller\n",
       "    than ``tol``.\n",
       "\n",
       "warm_start : bool, default=False\n",
       "    When set to ``True``, reuse the solution of the previous call to fit as\n",
       "    initialization, otherwise, just erase the previous solution.\n",
       "    See :term:`the Glossary <warm_start>`.\n",
       "\n",
       "positive : bool, default=False\n",
       "    When set to ``True``, forces the coefficients to be positive.\n",
       "\n",
       "random_state : int, RandomState instance, default=None\n",
       "    The seed of the pseudo random number generator that selects a random\n",
       "    feature to update. Used when ``selection`` == 'random'.\n",
       "    Pass an int for reproducible output across multiple function calls.\n",
       "    See :term:`Glossary <random_state>`.\n",
       "\n",
       "selection : {'cyclic', 'random'}, default='cyclic'\n",
       "    If set to 'random', a random coefficient is updated every iteration\n",
       "    rather than looping over features sequentially by default. This\n",
       "    (setting to 'random') often leads to significantly faster convergence\n",
       "    especially when tol is higher than 1e-4.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
       "    parameter vector (w in the cost function formula)\n",
       "\n",
       "sparse_coef_ : sparse matrix of shape (n_features, 1) or             (n_targets, n_features)\n",
       "    ``sparse_coef_`` is a readonly property derived from ``coef_``\n",
       "\n",
       "intercept_ : float or ndarray of shape (n_targets,)\n",
       "    independent term in decision function.\n",
       "\n",
       "n_iter_ : list of int\n",
       "    number of iterations run by the coordinate descent solver to reach\n",
       "    the specified tolerance.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.linear_model import ElasticNet\n",
       ">>> from sklearn.datasets import make_regression\n",
       "\n",
       ">>> X, y = make_regression(n_features=2, random_state=0)\n",
       ">>> regr = ElasticNet(random_state=0)\n",
       ">>> regr.fit(X, y)\n",
       "ElasticNet(random_state=0)\n",
       ">>> print(regr.coef_)\n",
       "[18.83816048 64.55968825]\n",
       ">>> print(regr.intercept_)\n",
       "1.451...\n",
       ">>> print(regr.predict([[0, 0]]))\n",
       "[1.451...]\n",
       "\n",
       "\n",
       "Notes\n",
       "-----\n",
       "To avoid unnecessary memory duplication the X argument of the fit method\n",
       "should be directly passed as a Fortran-contiguous numpy array.\n",
       "\n",
       "See also\n",
       "--------\n",
       "ElasticNetCV : Elastic net model with best model selection by\n",
       "    cross-validation.\n",
       "SGDRegressor: implements elastic net regression with incremental training.\n",
       "SGDClassifier: implements logistic regression with elastic net penalty\n",
       "    (``SGDClassifier(loss=\"log\", penalty=\"elasticnet\")``).\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\toshiba\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     Lasso\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295.9805666186622"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Çapraz doğrulama işleviyle hiper parametre optimizasyonu.\n",
    "# Yeni alfa değerleri atayarak modeli ayarlamaya çalışacağız.\n",
    "# ElesticNet regresyonunda varsayılan alfa değeri 1.0 ve varsayılan l1_ratio 0,5'tir. Farklı değerler deneyeceğiz.\n",
    "# En uygun değerler veya parametreler nihai modelde kullanılacaktır.\n",
    "\n",
    "enet_params = {\"l1_ratio\": [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n",
    "              \"alpha\":[0.1,0.01,0.001,0.2,0.3,0.5,0.8,0.9,1]}\n",
    "enet = ElasticNet()\n",
    "enet_model = enet.fit(X_train,y_train)\n",
    "enet_cv = GridSearchCV(enet_model, enet_params, cv = 10).fit(X, y)\n",
    "enet_cv.best_params_\n",
    "\n",
    "#Final Model \n",
    "\n",
    "enet_tuned = ElasticNet(**enet_cv.best_params_).fit(X_train,y_train)\n",
    "y_pred = enet_tuned.predict(X_test)\n",
    "df_enet_tuned_rmse = np.sqrt(mean_squared_error(y_test,y_pred))\n",
    "df_enet_tuned_rmse "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN (K-Nearest Neighbors) Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        KNeighborsRegressor\n",
       "\u001b[1;31mString form:\u001b[0m KNeighborsRegressor()\n",
       "\u001b[1;31mFile:\u001b[0m        c:\\users\\toshiba\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_regression.py\n",
       "\u001b[1;31mDocstring:\u001b[0m  \n",
       "Regression based on k-nearest neighbors.\n",
       "\n",
       "The target is predicted by local interpolation of the targets\n",
       "associated of the nearest neighbors in the training set.\n",
       "\n",
       "Read more in the :ref:`User Guide <regression>`.\n",
       "\n",
       ".. versionadded:: 0.9\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "n_neighbors : int, default=5\n",
       "    Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
       "\n",
       "weights : {'uniform', 'distance'} or callable, default='uniform'\n",
       "    weight function used in prediction.  Possible values:\n",
       "\n",
       "    - 'uniform' : uniform weights.  All points in each neighborhood\n",
       "      are weighted equally.\n",
       "    - 'distance' : weight points by the inverse of their distance.\n",
       "      in this case, closer neighbors of a query point will have a\n",
       "      greater influence than neighbors which are further away.\n",
       "    - [callable] : a user-defined function which accepts an\n",
       "      array of distances, and returns an array of the same shape\n",
       "      containing the weights.\n",
       "\n",
       "    Uniform weights are used by default.\n",
       "\n",
       "algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
       "    Algorithm used to compute the nearest neighbors:\n",
       "\n",
       "    - 'ball_tree' will use :class:`BallTree`\n",
       "    - 'kd_tree' will use :class:`KDTree`\n",
       "    - 'brute' will use a brute-force search.\n",
       "    - 'auto' will attempt to decide the most appropriate algorithm\n",
       "      based on the values passed to :meth:`fit` method.\n",
       "\n",
       "    Note: fitting on sparse input will override the setting of\n",
       "    this parameter, using brute force.\n",
       "\n",
       "leaf_size : int, default=30\n",
       "    Leaf size passed to BallTree or KDTree.  This can affect the\n",
       "    speed of the construction and query, as well as the memory\n",
       "    required to store the tree.  The optimal value depends on the\n",
       "    nature of the problem.\n",
       "\n",
       "p : int, default=2\n",
       "    Power parameter for the Minkowski metric. When p = 1, this is\n",
       "    equivalent to using manhattan_distance (l1), and euclidean_distance\n",
       "    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
       "\n",
       "metric : str or callable, default='minkowski'\n",
       "    the distance metric to use for the tree.  The default metric is\n",
       "    minkowski, and with p=2 is equivalent to the standard Euclidean\n",
       "    metric. See the documentation of :class:`DistanceMetric` for a\n",
       "    list of available metrics.\n",
       "    If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
       "    must be square during fit. X may be a :term:`sparse graph`,\n",
       "    in which case only \"nonzero\" elements may be considered neighbors.\n",
       "\n",
       "metric_params : dict, default=None\n",
       "    Additional keyword arguments for the metric function.\n",
       "\n",
       "n_jobs : int, default=None\n",
       "    The number of parallel jobs to run for neighbors search.\n",
       "    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
       "    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
       "    for more details.\n",
       "    Doesn't affect :meth:`fit` method.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "effective_metric_ : str or callable\n",
       "    The distance metric to use. It will be same as the `metric` parameter\n",
       "    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
       "    'minkowski' and `p` parameter set to 2.\n",
       "\n",
       "effective_metric_params_ : dict\n",
       "    Additional keyword arguments for the metric function. For most metrics\n",
       "    will be same with `metric_params` parameter, but may also contain the\n",
       "    `p` parameter value if the `effective_metric_` attribute is set to\n",
       "    'minkowski'.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> X = [[0], [1], [2], [3]]\n",
       ">>> y = [0, 0, 1, 1]\n",
       ">>> from sklearn.neighbors import KNeighborsRegressor\n",
       ">>> neigh = KNeighborsRegressor(n_neighbors=2)\n",
       ">>> neigh.fit(X, y)\n",
       "KNeighborsRegressor(...)\n",
       ">>> print(neigh.predict([[1.5]]))\n",
       "[0.5]\n",
       "\n",
       "See also\n",
       "--------\n",
       "NearestNeighbors\n",
       "RadiusNeighborsRegressor\n",
       "KNeighborsClassifier\n",
       "RadiusNeighborsClassifier\n",
       "\n",
       "Notes\n",
       "-----\n",
       "See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
       "for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
       "\n",
       ".. warning::\n",
       "\n",
       "   Regarding the Nearest Neighbors algorithms, if it is found that two\n",
       "   neighbors, neighbor `k+1` and `k`, have identical distances but\n",
       "   different labels, the results will depend on the ordering of the\n",
       "   training data.\n",
       "\n",
       "https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "289.1071265271843"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_neighbors : int, default=5 Varsayılan olarak kullanılacak komşu sayısı :meth:`kneighbors` sorguları.\n",
    "\n",
    "knn_params = {\"n_neighbors\": np.arange(2,30,1)}\n",
    "knn_cv_model = GridSearchCV(knn_model, knn_params, cv = 10).fit(X_train, y_train)\n",
    "knn_cv_model.best_params_\n",
    "knn_tuned = KNeighborsRegressor(**knn_cv_model.best_params_).fit(X_train, y_train)\n",
    "\n",
    "# Final Model\n",
    "\n",
    "y_pred = knn_tuned.predict(X_test)\n",
    "df_knn_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "df_knn_tuned_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 9}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_cv_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(n_neighbors=9)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_cv_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR (Support Vector Regression) Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        SVR\n",
       "\u001b[1;31mString form:\u001b[0m SVR(kernel='linear')\n",
       "\u001b[1;31mFile:\u001b[0m        c:\\users\\toshiba\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\n",
       "\u001b[1;31mDocstring:\u001b[0m  \n",
       "Epsilon-Support Vector Regression.\n",
       "\n",
       "The free parameters in the model are C and epsilon.\n",
       "\n",
       "The implementation is based on libsvm. The fit time complexity\n",
       "is more than quadratic with the number of samples which makes it hard\n",
       "to scale to datasets with more than a couple of 10000 samples. For large\n",
       "datasets consider using :class:`sklearn.svm.LinearSVR` or\n",
       ":class:`sklearn.linear_model.SGDRegressor` instead, possibly after a\n",
       ":class:`sklearn.kernel_approximation.Nystroem` transformer.\n",
       "\n",
       "Read more in the :ref:`User Guide <svm_regression>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'\n",
       "     Specifies the kernel type to be used in the algorithm.\n",
       "     It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n",
       "     a callable.\n",
       "     If none is given, 'rbf' will be used. If a callable is given it is\n",
       "     used to precompute the kernel matrix.\n",
       "\n",
       "degree : int, default=3\n",
       "    Degree of the polynomial kernel function ('poly').\n",
       "    Ignored by all other kernels.\n",
       "\n",
       "gamma : {'scale', 'auto'} or float, default='scale'\n",
       "    Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
       "\n",
       "    - if ``gamma='scale'`` (default) is passed then it uses\n",
       "      1 / (n_features * X.var()) as value of gamma,\n",
       "    - if 'auto', uses 1 / n_features.\n",
       "\n",
       "    .. versionchanged:: 0.22\n",
       "       The default value of ``gamma`` changed from 'auto' to 'scale'.\n",
       "\n",
       "coef0 : float, default=0.0\n",
       "    Independent term in kernel function.\n",
       "    It is only significant in 'poly' and 'sigmoid'.\n",
       "\n",
       "tol : float, default=1e-3\n",
       "    Tolerance for stopping criterion.\n",
       "\n",
       "C : float, default=1.0\n",
       "    Regularization parameter. The strength of the regularization is\n",
       "    inversely proportional to C. Must be strictly positive.\n",
       "    The penalty is a squared l2 penalty.\n",
       "\n",
       "epsilon : float, default=0.1\n",
       "     Epsilon in the epsilon-SVR model. It specifies the epsilon-tube\n",
       "     within which no penalty is associated in the training loss function\n",
       "     with points predicted within a distance epsilon from the actual\n",
       "     value.\n",
       "\n",
       "shrinking : bool, default=True\n",
       "    Whether to use the shrinking heuristic.\n",
       "    See the :ref:`User Guide <shrinking_svm>`.\n",
       "\n",
       "cache_size : float, default=200\n",
       "    Specify the size of the kernel cache (in MB).\n",
       "\n",
       "verbose : bool, default=False\n",
       "    Enable verbose output. Note that this setting takes advantage of a\n",
       "    per-process runtime setting in libsvm that, if enabled, may not work\n",
       "    properly in a multithreaded context.\n",
       "\n",
       "max_iter : int, default=-1\n",
       "    Hard limit on iterations within solver, or -1 for no limit.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "support_ : ndarray of shape (n_SV,)\n",
       "    Indices of support vectors.\n",
       "\n",
       "support_vectors_ : ndarray of shape (n_SV, n_features)\n",
       "    Support vectors.\n",
       "\n",
       "dual_coef_ : ndarray of shape (1, n_SV)\n",
       "    Coefficients of the support vector in the decision function.\n",
       "\n",
       "coef_ : ndarray of shape (1, n_features)\n",
       "    Weights assigned to the features (coefficients in the primal\n",
       "    problem). This is only available in the case of a linear kernel.\n",
       "\n",
       "    `coef_` is readonly property derived from `dual_coef_` and\n",
       "    `support_vectors_`.\n",
       "\n",
       "fit_status_ : int\n",
       "    0 if correctly fitted, 1 otherwise (will raise warning)\n",
       "\n",
       "intercept_ : ndarray of shape (1,)\n",
       "    Constants in decision function.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.svm import SVR\n",
       ">>> from sklearn.pipeline import make_pipeline\n",
       ">>> from sklearn.preprocessing import StandardScaler\n",
       ">>> import numpy as np\n",
       ">>> n_samples, n_features = 10, 5\n",
       ">>> rng = np.random.RandomState(0)\n",
       ">>> y = rng.randn(n_samples)\n",
       ">>> X = rng.randn(n_samples, n_features)\n",
       ">>> regr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\n",
       ">>> regr.fit(X, y)\n",
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svr', SVR(epsilon=0.2))])\n",
       "\n",
       "\n",
       "See also\n",
       "--------\n",
       "NuSVR\n",
       "    Support Vector Machine for regression implemented using libsvm\n",
       "    using a parameter to control the number of support vectors.\n",
       "\n",
       "LinearSVR\n",
       "    Scalable Linear Support Vector Machine for regression\n",
       "    implemented using liblinear.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "**References:**\n",
       "`LIBSVM: A Library for Support Vector Machines\n",
       "<http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`__\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?svr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    3.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "278.85157251429393"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C : float, default=1.0 Düzenlileştirme parametresi. Düzenlileştirmenin gücü C ile ters orantılıdır. Kesinlikle pozitif olmalıdır.\n",
    "\n",
    "svr_params = {'C': [0.01,0.001, 0.2, 0.1,0.5,0.8,0.9,1]}\n",
    "svr_cv_model = GridSearchCV(svr_model, svr_params, cv = 5, n_jobs = -1, verbose =  2).fit(X_train, y_train)\n",
    "svr_tuned = SVR('linear', **svr_cv_model.best_params_).fit(X_train, y_train)\n",
    "y_pred = svr_tuned.predict(X_test)\n",
    "df_svr_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "df_svr_tuned_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr_cv_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1, kernel='linear')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr_cv_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP (Multilayer Perceptron) Model Tuning\n",
    "Yapay sinir ağı modellerinden biri(ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        MLPRegressor\n",
       "\u001b[1;31mString form:\u001b[0m MLPRegressor()\n",
       "\u001b[1;31mFile:\u001b[0m        c:\\users\\toshiba\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\n",
       "\u001b[1;31mDocstring:\u001b[0m  \n",
       "Multi-layer Perceptron regressor.\n",
       "\n",
       "This model optimizes the squared-loss using LBFGS or stochastic gradient\n",
       "descent.\n",
       "\n",
       ".. versionadded:: 0.18\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "hidden_layer_sizes : tuple, length = n_layers - 2, default=(100,)\n",
       "    The ith element represents the number of neurons in the ith\n",
       "    hidden layer.\n",
       "\n",
       "activation : {'identity', 'logistic', 'tanh', 'relu'}, default='relu'\n",
       "    Activation function for the hidden layer.\n",
       "\n",
       "    - 'identity', no-op activation, useful to implement linear bottleneck,\n",
       "      returns f(x) = x\n",
       "\n",
       "    - 'logistic', the logistic sigmoid function,\n",
       "      returns f(x) = 1 / (1 + exp(-x)).\n",
       "\n",
       "    - 'tanh', the hyperbolic tan function,\n",
       "      returns f(x) = tanh(x).\n",
       "\n",
       "    - 'relu', the rectified linear unit function,\n",
       "      returns f(x) = max(0, x)\n",
       "\n",
       "solver : {'lbfgs', 'sgd', 'adam'}, default='adam'\n",
       "    The solver for weight optimization.\n",
       "\n",
       "    - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\n",
       "\n",
       "    - 'sgd' refers to stochastic gradient descent.\n",
       "\n",
       "    - 'adam' refers to a stochastic gradient-based optimizer proposed by\n",
       "      Kingma, Diederik, and Jimmy Ba\n",
       "\n",
       "    Note: The default solver 'adam' works pretty well on relatively\n",
       "    large datasets (with thousands of training samples or more) in terms of\n",
       "    both training time and validation score.\n",
       "    For small datasets, however, 'lbfgs' can converge faster and perform\n",
       "    better.\n",
       "\n",
       "alpha : float, default=0.0001\n",
       "    L2 penalty (regularization term) parameter.\n",
       "\n",
       "batch_size : int, default='auto'\n",
       "    Size of minibatches for stochastic optimizers.\n",
       "    If the solver is 'lbfgs', the classifier will not use minibatch.\n",
       "    When set to \"auto\", `batch_size=min(200, n_samples)`\n",
       "\n",
       "learning_rate : {'constant', 'invscaling', 'adaptive'}, default='constant'\n",
       "    Learning rate schedule for weight updates.\n",
       "\n",
       "    - 'constant' is a constant learning rate given by\n",
       "      'learning_rate_init'.\n",
       "\n",
       "    - 'invscaling' gradually decreases the learning rate ``learning_rate_``\n",
       "      at each time step 't' using an inverse scaling exponent of 'power_t'.\n",
       "      effective_learning_rate = learning_rate_init / pow(t, power_t)\n",
       "\n",
       "    - 'adaptive' keeps the learning rate constant to\n",
       "      'learning_rate_init' as long as training loss keeps decreasing.\n",
       "      Each time two consecutive epochs fail to decrease training loss by at\n",
       "      least tol, or fail to increase validation score by at least tol if\n",
       "      'early_stopping' is on, the current learning rate is divided by 5.\n",
       "\n",
       "    Only used when solver='sgd'.\n",
       "\n",
       "learning_rate_init : double, default=0.001\n",
       "    The initial learning rate used. It controls the step-size\n",
       "    in updating the weights. Only used when solver='sgd' or 'adam'.\n",
       "\n",
       "power_t : double, default=0.5\n",
       "    The exponent for inverse scaling learning rate.\n",
       "    It is used in updating effective learning rate when the learning_rate\n",
       "    is set to 'invscaling'. Only used when solver='sgd'.\n",
       "\n",
       "max_iter : int, default=200\n",
       "    Maximum number of iterations. The solver iterates until convergence\n",
       "    (determined by 'tol') or this number of iterations. For stochastic\n",
       "    solvers ('sgd', 'adam'), note that this determines the number of epochs\n",
       "    (how many times each data point will be used), not the number of\n",
       "    gradient steps.\n",
       "\n",
       "shuffle : bool, default=True\n",
       "    Whether to shuffle samples in each iteration. Only used when\n",
       "    solver='sgd' or 'adam'.\n",
       "\n",
       "random_state : int, RandomState instance, default=None\n",
       "    Determines random number generation for weights and bias\n",
       "    initialization, train-test split if early stopping is used, and batch\n",
       "    sampling when solver='sgd' or 'adam'.\n",
       "    Pass an int for reproducible results across multiple function calls.\n",
       "    See :term:`Glossary <random_state>`.\n",
       "\n",
       "tol : float, default=1e-4\n",
       "    Tolerance for the optimization. When the loss or score is not improving\n",
       "    by at least ``tol`` for ``n_iter_no_change`` consecutive iterations,\n",
       "    unless ``learning_rate`` is set to 'adaptive', convergence is\n",
       "    considered to be reached and training stops.\n",
       "\n",
       "verbose : bool, default=False\n",
       "    Whether to print progress messages to stdout.\n",
       "\n",
       "warm_start : bool, default=False\n",
       "    When set to True, reuse the solution of the previous\n",
       "    call to fit as initialization, otherwise, just erase the\n",
       "    previous solution. See :term:`the Glossary <warm_start>`.\n",
       "\n",
       "momentum : float, default=0.9\n",
       "    Momentum for gradient descent update.  Should be between 0 and 1. Only\n",
       "    used when solver='sgd'.\n",
       "\n",
       "nesterovs_momentum : boolean, default=True\n",
       "    Whether to use Nesterov's momentum. Only used when solver='sgd' and\n",
       "    momentum > 0.\n",
       "\n",
       "early_stopping : bool, default=False\n",
       "    Whether to use early stopping to terminate training when validation\n",
       "    score is not improving. If set to true, it will automatically set\n",
       "    aside 10% of training data as validation and terminate training when\n",
       "    validation score is not improving by at least ``tol`` for\n",
       "    ``n_iter_no_change`` consecutive epochs.\n",
       "    Only effective when solver='sgd' or 'adam'\n",
       "\n",
       "validation_fraction : float, default=0.1\n",
       "    The proportion of training data to set aside as validation set for\n",
       "    early stopping. Must be between 0 and 1.\n",
       "    Only used if early_stopping is True\n",
       "\n",
       "beta_1 : float, default=0.9\n",
       "    Exponential decay rate for estimates of first moment vector in adam,\n",
       "    should be in [0, 1). Only used when solver='adam'\n",
       "\n",
       "beta_2 : float, default=0.999\n",
       "    Exponential decay rate for estimates of second moment vector in adam,\n",
       "    should be in [0, 1). Only used when solver='adam'\n",
       "\n",
       "epsilon : float, default=1e-8\n",
       "    Value for numerical stability in adam. Only used when solver='adam'\n",
       "\n",
       "n_iter_no_change : int, default=10\n",
       "    Maximum number of epochs to not meet ``tol`` improvement.\n",
       "    Only effective when solver='sgd' or 'adam'\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "max_fun : int, default=15000\n",
       "    Only used when solver='lbfgs'. Maximum number of function calls.\n",
       "    The solver iterates until convergence (determined by 'tol'), number\n",
       "    of iterations reaches max_iter, or this number of function calls.\n",
       "    Note that number of function calls will be greater than or equal to\n",
       "    the number of iterations for the MLPRegressor.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "loss_ : float\n",
       "    The current loss computed with the loss function.\n",
       "\n",
       "coefs_ : list, length n_layers - 1\n",
       "    The ith element in the list represents the weight matrix corresponding\n",
       "    to layer i.\n",
       "\n",
       "intercepts_ : list, length n_layers - 1\n",
       "    The ith element in the list represents the bias vector corresponding to\n",
       "    layer i + 1.\n",
       "\n",
       "n_iter_ : int,\n",
       "    The number of iterations the solver has ran.\n",
       "\n",
       "n_layers_ : int\n",
       "    Number of layers.\n",
       "\n",
       "n_outputs_ : int\n",
       "    Number of outputs.\n",
       "\n",
       "out_activation_ : string\n",
       "    Name of the output activation function.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.neural_network import MLPRegressor\n",
       ">>> from sklearn.datasets import make_regression\n",
       ">>> from sklearn.model_selection import train_test_split\n",
       ">>> X, y = make_regression(n_samples=200, random_state=1)\n",
       ">>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
       "...                                                     random_state=1)\n",
       ">>> regr = MLPRegressor(random_state=1, max_iter=500).fit(X_train, y_train)\n",
       ">>> regr.predict(X_test[:2])\n",
       "array([-0.9..., -7.1...])\n",
       ">>> regr.score(X_test, y_test)\n",
       "0.4...\n",
       "\n",
       "Notes\n",
       "-----\n",
       "MLPRegressor trains iteratively since at each time step\n",
       "the partial derivatives of the loss function with respect to the model\n",
       "parameters are computed to update the parameters.\n",
       "\n",
       "It can also have a regularization term added to the loss function\n",
       "that shrinks model parameters to prevent overfitting.\n",
       "\n",
       "This implementation works with data represented as dense and sparse numpy\n",
       "arrays of floating point values.\n",
       "\n",
       "References\n",
       "----------\n",
       "Hinton, Geoffrey E.\n",
       "    \"Connectionist learning procedures.\" Artificial intelligence 40.1\n",
       "    (1989): 185-234.\n",
       "\n",
       "Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of\n",
       "    training deep feedforward neural networks.\" International Conference\n",
       "    on Artificial Intelligence and Statistics. 2010.\n",
       "\n",
       "He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level\n",
       "    performance on imagenet classification.\" arXiv preprint\n",
       "    arXiv:1502.01852 (2015).\n",
       "\n",
       "Kingma, Diederik, and Jimmy Ba. \"Adam: A method for stochastic\n",
       "    optimization.\" arXiv preprint arXiv:1412.6980 (2014).\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "419.6322576217184"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hidden_layer_sizes : tuple, length = n_layers - 2, default=(100,) \n",
    "# alpha : float, default=0.0001\n",
    "\n",
    "mlp_params = {\"alpha\": [0.1, 0.01, 0.02, 0.001, 0.0001], \n",
    "             \"hidden_layer_sizes\": [(10,20), (5,5), (100,100), (1000,100,10)]}\n",
    "mlp_cv_model = GridSearchCV(mlp_model, mlp_params, cv = 10, verbose = 2, n_jobs = -1).fit(X_train, y_train)\n",
    "mlp_tuned = MLPRegressor(**mlp_cv_model.best_params_).fit(X_train, y_train)\n",
    "y_pred = mlp_tuned.predict(X_test)\n",
    "df_mlp_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "df_mlp_tuned_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.02, 'hidden_layer_sizes': (1000, 100, 10)}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_cv_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(alpha=0.02, hidden_layer_sizes=(1000, 100, 10))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_cv_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CART (Classification and Regression Trees) Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        DecisionTreeRegressor\n",
       "\u001b[1;31mString form:\u001b[0m DecisionTreeRegressor()\n",
       "\u001b[1;31mFile:\u001b[0m        c:\\users\\toshiba\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\n",
       "\u001b[1;31mDocstring:\u001b[0m  \n",
       "A decision tree regressor.\n",
       "\n",
       "Read more in the :ref:`User Guide <tree>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "criterion : {\"mse\", \"friedman_mse\", \"mae\"}, default=\"mse\"\n",
       "    The function to measure the quality of a split. Supported criteria\n",
       "    are \"mse\" for the mean squared error, which is equal to variance\n",
       "    reduction as feature selection criterion and minimizes the L2 loss\n",
       "    using the mean of each terminal node, \"friedman_mse\", which uses mean\n",
       "    squared error with Friedman's improvement score for potential splits,\n",
       "    and \"mae\" for the mean absolute error, which minimizes the L1 loss\n",
       "    using the median of each terminal node.\n",
       "\n",
       "    .. versionadded:: 0.18\n",
       "       Mean Absolute Error (MAE) criterion.\n",
       "\n",
       "splitter : {\"best\", \"random\"}, default=\"best\"\n",
       "    The strategy used to choose the split at each node. Supported\n",
       "    strategies are \"best\" to choose the best split and \"random\" to choose\n",
       "    the best random split.\n",
       "\n",
       "max_depth : int, default=None\n",
       "    The maximum depth of the tree. If None, then nodes are expanded until\n",
       "    all leaves are pure or until all leaves contain less than\n",
       "    min_samples_split samples.\n",
       "\n",
       "min_samples_split : int or float, default=2\n",
       "    The minimum number of samples required to split an internal node:\n",
       "\n",
       "    - If int, then consider `min_samples_split` as the minimum number.\n",
       "    - If float, then `min_samples_split` is a fraction and\n",
       "      `ceil(min_samples_split * n_samples)` are the minimum\n",
       "      number of samples for each split.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_samples_leaf : int or float, default=1\n",
       "    The minimum number of samples required to be at a leaf node.\n",
       "    A split point at any depth will only be considered if it leaves at\n",
       "    least ``min_samples_leaf`` training samples in each of the left and\n",
       "    right branches.  This may have the effect of smoothing the model,\n",
       "    especially in regression.\n",
       "\n",
       "    - If int, then consider `min_samples_leaf` as the minimum number.\n",
       "    - If float, then `min_samples_leaf` is a fraction and\n",
       "      `ceil(min_samples_leaf * n_samples)` are the minimum\n",
       "      number of samples for each node.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_weight_fraction_leaf : float, default=0.0\n",
       "    The minimum weighted fraction of the sum total of weights (of all\n",
       "    the input samples) required to be at a leaf node. Samples have\n",
       "    equal weight when sample_weight is not provided.\n",
       "\n",
       "max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n",
       "    The number of features to consider when looking for the best split:\n",
       "\n",
       "    - If int, then consider `max_features` features at each split.\n",
       "    - If float, then `max_features` is a fraction and\n",
       "      `int(max_features * n_features)` features are considered at each\n",
       "      split.\n",
       "    - If \"auto\", then `max_features=n_features`.\n",
       "    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
       "    - If \"log2\", then `max_features=log2(n_features)`.\n",
       "    - If None, then `max_features=n_features`.\n",
       "\n",
       "    Note: the search for a split does not stop until at least one\n",
       "    valid partition of the node samples is found, even if it requires to\n",
       "    effectively inspect more than ``max_features`` features.\n",
       "\n",
       "random_state : int, RandomState instance, default=None\n",
       "    Controls the randomness of the estimator. The features are always\n",
       "    randomly permuted at each split, even if ``splitter`` is set to\n",
       "    ``\"best\"``. When ``max_features < n_features``, the algorithm will\n",
       "    select ``max_features`` at random at each split before finding the best\n",
       "    split among them. But the best found split may vary across different\n",
       "    runs, even if ``max_features=n_features``. That is the case, if the\n",
       "    improvement of the criterion is identical for several splits and one\n",
       "    split has to be selected at random. To obtain a deterministic behaviour\n",
       "    during fitting, ``random_state`` has to be fixed to an integer.\n",
       "    See :term:`Glossary <random_state>` for details.\n",
       "\n",
       "max_leaf_nodes : int, default=None\n",
       "    Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
       "    Best nodes are defined as relative reduction in impurity.\n",
       "    If None then unlimited number of leaf nodes.\n",
       "\n",
       "min_impurity_decrease : float, default=0.0\n",
       "    A node will be split if this split induces a decrease of the impurity\n",
       "    greater than or equal to this value.\n",
       "\n",
       "    The weighted impurity decrease equation is the following::\n",
       "\n",
       "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
       "                            - N_t_L / N_t * left_impurity)\n",
       "\n",
       "    where ``N`` is the total number of samples, ``N_t`` is the number of\n",
       "    samples at the current node, ``N_t_L`` is the number of samples in the\n",
       "    left child, and ``N_t_R`` is the number of samples in the right child.\n",
       "\n",
       "    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
       "    if ``sample_weight`` is passed.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "\n",
       "min_impurity_split : float, (default=0)\n",
       "    Threshold for early stopping in tree growth. A node will split\n",
       "    if its impurity is above the threshold, otherwise it is a leaf.\n",
       "\n",
       "    .. deprecated:: 0.19\n",
       "       ``min_impurity_split`` has been deprecated in favor of\n",
       "       ``min_impurity_decrease`` in 0.19. The default value of\n",
       "       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n",
       "       will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
       "\n",
       "presort : deprecated, default='deprecated'\n",
       "    This parameter is deprecated and will be removed in v0.24.\n",
       "\n",
       "    .. deprecated:: 0.22\n",
       "\n",
       "ccp_alpha : non-negative float, default=0.0\n",
       "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
       "    subtree with the largest cost complexity that is smaller than\n",
       "    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
       "    :ref:`minimal_cost_complexity_pruning` for details.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "feature_importances_ : ndarray of shape (n_features,)\n",
       "    The feature importances.\n",
       "    The higher, the more important the feature.\n",
       "    The importance of a feature is computed as the\n",
       "    (normalized) total reduction of the criterion brought\n",
       "    by that feature. It is also known as the Gini importance [4]_.\n",
       "\n",
       "    Warning: impurity-based feature importances can be misleading for\n",
       "    high cardinality features (many unique values). See\n",
       "    :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
       "\n",
       "max_features_ : int\n",
       "    The inferred value of max_features.\n",
       "\n",
       "n_features_ : int\n",
       "    The number of features when ``fit`` is performed.\n",
       "\n",
       "n_outputs_ : int\n",
       "    The number of outputs when ``fit`` is performed.\n",
       "\n",
       "tree_ : Tree\n",
       "    The underlying Tree object. Please refer to\n",
       "    ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
       "    :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
       "    for basic usage of these attributes.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "DecisionTreeClassifier : A decision tree classifier.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The default values for the parameters controlling the size of the trees\n",
       "(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
       "unpruned trees which can potentially be very large on some data sets. To\n",
       "reduce memory consumption, the complexity and size of the trees should be\n",
       "controlled by setting those parameter values.\n",
       "\n",
       "References\n",
       "----------\n",
       "\n",
       ".. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
       "\n",
       ".. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
       "       and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
       "\n",
       ".. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
       "       Learning\", Springer, 2009.\n",
       "\n",
       ".. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
       "       https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.datasets import load_diabetes\n",
       ">>> from sklearn.model_selection import cross_val_score\n",
       ">>> from sklearn.tree import DecisionTreeRegressor\n",
       ">>> X, y = load_diabetes(return_X_y=True)\n",
       ">>> regressor = DecisionTreeRegressor(random_state=0)\n",
       ">>> cross_val_score(regressor, X, y, cv=10)\n",
       "...                    # doctest: +SKIP\n",
       "...\n",
       "array([-0.39..., -0.46...,  0.02...,  0.06..., -0.50...,\n",
       "       0.16...,  0.11..., -0.73..., -0.30..., -0.00...])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?cart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- max_depth : int, default=None Ağacın maksimum derinliği. Hiçbiri ise, tüm yapraklar saf olana veya tüm yapraklar min_samples_split örneklerinden daha azını içerene kadar düğümler genişletilir.\n",
    "- min_samples_split : int or float, default=2 Bir dahili düğümü bölmek için gereken minimum örnek sayısı:\n",
    "       *İnt ise, min_samples_split'i minimum sayı olarak kabul edin.\n",
    "       *Float ise, min_samples_split bir kesirdir ve ceil(min_samples_split * n_samples) her bölme için minimum örnek sayısıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399.6353867178006"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart_params = {\"max_depth\": [2,3,4,5,10,20, 100, 1000],\n",
    "              \"min_samples_split\": [2,10,5,30,50,10]}\n",
    "cart_cv_model = GridSearchCV(cart_model, cart_params, cv = 10).fit(X_train, y_train)\n",
    "cart_tuned = DecisionTreeRegressor(**cart_cv_model.best_params_).fit(X_train, y_train)\n",
    "y_pred = cart_tuned.predict(X_test)\n",
    "df_cart_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "df_cart_tuned_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5, 'min_samples_split': 30}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart_cv_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=5, min_samples_split=30)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart_cv_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests  Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_samples_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_weight_fraction_leaf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_leaf_nodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_impurity_decrease\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_impurity_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mbootstrap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0moob_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mccp_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "A random forest regressor.\n",
       "\n",
       "A random forest is a meta estimator that fits a number of classifying\n",
       "decision trees on various sub-samples of the dataset and uses averaging\n",
       "to improve the predictive accuracy and control over-fitting.\n",
       "The sub-sample size is controlled with the `max_samples` parameter if\n",
       "`bootstrap=True` (default), otherwise the whole dataset is used to build\n",
       "each tree.\n",
       "\n",
       "Read more in the :ref:`User Guide <forest>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "n_estimators : int, default=100\n",
       "    The number of trees in the forest.\n",
       "\n",
       "    .. versionchanged:: 0.22\n",
       "       The default value of ``n_estimators`` changed from 10 to 100\n",
       "       in 0.22.\n",
       "\n",
       "criterion : {\"mse\", \"mae\"}, default=\"mse\"\n",
       "    The function to measure the quality of a split. Supported criteria\n",
       "    are \"mse\" for the mean squared error, which is equal to variance\n",
       "    reduction as feature selection criterion, and \"mae\" for the mean\n",
       "    absolute error.\n",
       "\n",
       "    .. versionadded:: 0.18\n",
       "       Mean Absolute Error (MAE) criterion.\n",
       "\n",
       "max_depth : int, default=None\n",
       "    The maximum depth of the tree. If None, then nodes are expanded until\n",
       "    all leaves are pure or until all leaves contain less than\n",
       "    min_samples_split samples.\n",
       "\n",
       "min_samples_split : int or float, default=2\n",
       "    The minimum number of samples required to split an internal node:\n",
       "\n",
       "    - If int, then consider `min_samples_split` as the minimum number.\n",
       "    - If float, then `min_samples_split` is a fraction and\n",
       "      `ceil(min_samples_split * n_samples)` are the minimum\n",
       "      number of samples for each split.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_samples_leaf : int or float, default=1\n",
       "    The minimum number of samples required to be at a leaf node.\n",
       "    A split point at any depth will only be considered if it leaves at\n",
       "    least ``min_samples_leaf`` training samples in each of the left and\n",
       "    right branches.  This may have the effect of smoothing the model,\n",
       "    especially in regression.\n",
       "\n",
       "    - If int, then consider `min_samples_leaf` as the minimum number.\n",
       "    - If float, then `min_samples_leaf` is a fraction and\n",
       "      `ceil(min_samples_leaf * n_samples)` are the minimum\n",
       "      number of samples for each node.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_weight_fraction_leaf : float, default=0.0\n",
       "    The minimum weighted fraction of the sum total of weights (of all\n",
       "    the input samples) required to be at a leaf node. Samples have\n",
       "    equal weight when sample_weight is not provided.\n",
       "\n",
       "max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n",
       "    The number of features to consider when looking for the best split:\n",
       "\n",
       "    - If int, then consider `max_features` features at each split.\n",
       "    - If float, then `max_features` is a fraction and\n",
       "      `int(max_features * n_features)` features are considered at each\n",
       "      split.\n",
       "    - If \"auto\", then `max_features=n_features`.\n",
       "    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
       "    - If \"log2\", then `max_features=log2(n_features)`.\n",
       "    - If None, then `max_features=n_features`.\n",
       "\n",
       "    Note: the search for a split does not stop until at least one\n",
       "    valid partition of the node samples is found, even if it requires to\n",
       "    effectively inspect more than ``max_features`` features.\n",
       "\n",
       "max_leaf_nodes : int, default=None\n",
       "    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
       "    Best nodes are defined as relative reduction in impurity.\n",
       "    If None then unlimited number of leaf nodes.\n",
       "\n",
       "min_impurity_decrease : float, default=0.0\n",
       "    A node will be split if this split induces a decrease of the impurity\n",
       "    greater than or equal to this value.\n",
       "\n",
       "    The weighted impurity decrease equation is the following::\n",
       "\n",
       "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
       "                            - N_t_L / N_t * left_impurity)\n",
       "\n",
       "    where ``N`` is the total number of samples, ``N_t`` is the number of\n",
       "    samples at the current node, ``N_t_L`` is the number of samples in the\n",
       "    left child, and ``N_t_R`` is the number of samples in the right child.\n",
       "\n",
       "    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
       "    if ``sample_weight`` is passed.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "\n",
       "min_impurity_split : float, default=None\n",
       "    Threshold for early stopping in tree growth. A node will split\n",
       "    if its impurity is above the threshold, otherwise it is a leaf.\n",
       "\n",
       "    .. deprecated:: 0.19\n",
       "       ``min_impurity_split`` has been deprecated in favor of\n",
       "       ``min_impurity_decrease`` in 0.19. The default value of\n",
       "       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n",
       "       will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
       "\n",
       "bootstrap : bool, default=True\n",
       "    Whether bootstrap samples are used when building trees. If False, the\n",
       "    whole dataset is used to build each tree.\n",
       "\n",
       "oob_score : bool, default=False\n",
       "    whether to use out-of-bag samples to estimate\n",
       "    the R^2 on unseen data.\n",
       "\n",
       "n_jobs : int, default=None\n",
       "    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
       "    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
       "    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
       "    context. ``-1`` means using all processors. See :term:`Glossary\n",
       "    <n_jobs>` for more details.\n",
       "\n",
       "random_state : int or RandomState, default=None\n",
       "    Controls both the randomness of the bootstrapping of the samples used\n",
       "    when building trees (if ``bootstrap=True``) and the sampling of the\n",
       "    features to consider when looking for the best split at each node\n",
       "    (if ``max_features < n_features``).\n",
       "    See :term:`Glossary <random_state>` for details.\n",
       "\n",
       "verbose : int, default=0\n",
       "    Controls the verbosity when fitting and predicting.\n",
       "\n",
       "warm_start : bool, default=False\n",
       "    When set to ``True``, reuse the solution of the previous call to fit\n",
       "    and add more estimators to the ensemble, otherwise, just fit a whole\n",
       "    new forest. See :term:`the Glossary <warm_start>`.\n",
       "\n",
       "ccp_alpha : non-negative float, default=0.0\n",
       "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
       "    subtree with the largest cost complexity that is smaller than\n",
       "    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
       "    :ref:`minimal_cost_complexity_pruning` for details.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "max_samples : int or float, default=None\n",
       "    If bootstrap is True, the number of samples to draw from X\n",
       "    to train each base estimator.\n",
       "\n",
       "    - If None (default), then draw `X.shape[0]` samples.\n",
       "    - If int, then draw `max_samples` samples.\n",
       "    - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
       "      `max_samples` should be in the interval `(0, 1)`.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "base_estimator_ : DecisionTreeRegressor\n",
       "    The child estimator template used to create the collection of fitted\n",
       "    sub-estimators.\n",
       "\n",
       "estimators_ : list of DecisionTreeRegressor\n",
       "    The collection of fitted sub-estimators.\n",
       "\n",
       "feature_importances_ : ndarray of shape (n_features,)\n",
       "    The impurity-based feature importances.\n",
       "    The higher, the more important the feature.\n",
       "    The importance of a feature is computed as the (normalized)\n",
       "    total reduction of the criterion brought by that feature.  It is also\n",
       "    known as the Gini importance.\n",
       "\n",
       "    Warning: impurity-based feature importances can be misleading for\n",
       "    high cardinality features (many unique values). See\n",
       "    :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
       "\n",
       "n_features_ : int\n",
       "    The number of features when ``fit`` is performed.\n",
       "\n",
       "n_outputs_ : int\n",
       "    The number of outputs when ``fit`` is performed.\n",
       "\n",
       "oob_score_ : float\n",
       "    Score of the training dataset obtained using an out-of-bag estimate.\n",
       "    This attribute exists only when ``oob_score`` is True.\n",
       "\n",
       "oob_prediction_ : ndarray of shape (n_samples,)\n",
       "    Prediction computed with out-of-bag estimate on the training set.\n",
       "    This attribute exists only when ``oob_score`` is True.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "DecisionTreeRegressor, ExtraTreesRegressor\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The default values for the parameters controlling the size of the trees\n",
       "(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
       "unpruned trees which can potentially be very large on some data sets. To\n",
       "reduce memory consumption, the complexity and size of the trees should be\n",
       "controlled by setting those parameter values.\n",
       "\n",
       "The features are always randomly permuted at each split. Therefore,\n",
       "the best found split may vary, even with the same training data,\n",
       "``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
       "of the criterion is identical for several splits enumerated during the\n",
       "search of the best split. To obtain a deterministic behaviour during\n",
       "fitting, ``random_state`` has to be fixed.\n",
       "\n",
       "The default value ``max_features=\"auto\"`` uses ``n_features``\n",
       "rather than ``n_features / 3``. The latter was originally suggested in\n",
       "[1], whereas the former was more recently justified empirically in [2].\n",
       "\n",
       "References\n",
       "----------\n",
       ".. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
       "\n",
       ".. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n",
       "       trees\", Machine Learning, 63(1), 3-42, 2006.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.ensemble import RandomForestRegressor\n",
       ">>> from sklearn.datasets import make_regression\n",
       ">>> X, y = make_regression(n_features=4, n_informative=2,\n",
       "...                        random_state=0, shuffle=False)\n",
       ">>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
       ">>> regr.fit(X, y)\n",
       "RandomForestRegressor(...)\n",
       ">>> print(regr.predict([[0, 0, 0, 0]]))\n",
       "[-8.32987858]\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\toshiba\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RandomForestRegressor varsayılan değerleri:\n",
    "\n",
    "-n_estimators=100,\n",
    "\n",
    "* criterion='mse',\n",
    "* max_depth=None,\n",
    "* min_samples_split=2\n",
    "* n_jobs=None,\n",
    "* verbose=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 400 candidates, totalling 4000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   24.4s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4000 out of 4000 | elapsed: 13.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "261.882609744023"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_params = {\"max_depth\": [5,8,10,None],\n",
    "            \"max_features\": [2,5,10,15,17],\n",
    "            \"n_estimators\": [100,200, 500, 1000],\n",
    "            \"min_samples_split\": [2,5,10,20,30]}\n",
    "rf_cv_model = GridSearchCV(rf_model, rf_params, cv = 10, n_jobs = -1, verbose = 2).fit(X_train, y_train)\n",
    "rf_tuned = RandomForestRegressor(**rf_cv_model.best_params_).fit(X_train, y_train)\n",
    "y_pred = rf_tuned.predict(X_test)\n",
    "df_rf_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "df_rf_tuned_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 10,\n",
       " 'max_features': 2,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 200}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_cv_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=10, max_features=2, n_estimators=200)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_cv_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBM (Gradient Boosting Machines) Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        GradientBoostingRegressor\n",
       "\u001b[1;31mString form:\u001b[0m GradientBoostingRegressor()\n",
       "\u001b[1;31mLength:\u001b[0m      100\n",
       "\u001b[1;31mFile:\u001b[0m        c:\\users\\toshiba\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\n",
       "\u001b[1;31mDocstring:\u001b[0m  \n",
       "Gradient Boosting for regression.\n",
       "\n",
       "GB builds an additive model in a forward stage-wise fashion;\n",
       "it allows for the optimization of arbitrary differentiable loss functions.\n",
       "In each stage a regression tree is fit on the negative gradient of the\n",
       "given loss function.\n",
       "\n",
       "Read more in the :ref:`User Guide <gradient_boosting>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "loss : {'ls', 'lad', 'huber', 'quantile'}, default='ls'\n",
       "    loss function to be optimized. 'ls' refers to least squares\n",
       "    regression. 'lad' (least absolute deviation) is a highly robust\n",
       "    loss function solely based on order information of the input\n",
       "    variables. 'huber' is a combination of the two. 'quantile'\n",
       "    allows quantile regression (use `alpha` to specify the quantile).\n",
       "\n",
       "learning_rate : float, default=0.1\n",
       "    learning rate shrinks the contribution of each tree by `learning_rate`.\n",
       "    There is a trade-off between learning_rate and n_estimators.\n",
       "\n",
       "n_estimators : int, default=100\n",
       "    The number of boosting stages to perform. Gradient boosting\n",
       "    is fairly robust to over-fitting so a large number usually\n",
       "    results in better performance.\n",
       "\n",
       "subsample : float, default=1.0\n",
       "    The fraction of samples to be used for fitting the individual base\n",
       "    learners. If smaller than 1.0 this results in Stochastic Gradient\n",
       "    Boosting. `subsample` interacts with the parameter `n_estimators`.\n",
       "    Choosing `subsample < 1.0` leads to a reduction of variance\n",
       "    and an increase in bias.\n",
       "\n",
       "criterion : {'friedman_mse', 'mse', 'mae'}, default='friedman_mse'\n",
       "    The function to measure the quality of a split. Supported criteria\n",
       "    are \"friedman_mse\" for the mean squared error with improvement\n",
       "    score by Friedman, \"mse\" for mean squared error, and \"mae\" for\n",
       "    the mean absolute error. The default value of \"friedman_mse\" is\n",
       "    generally the best as it can provide a better approximation in\n",
       "    some cases.\n",
       "\n",
       "    .. versionadded:: 0.18\n",
       "\n",
       "min_samples_split : int or float, default=2\n",
       "    The minimum number of samples required to split an internal node:\n",
       "\n",
       "    - If int, then consider `min_samples_split` as the minimum number.\n",
       "    - If float, then `min_samples_split` is a fraction and\n",
       "      `ceil(min_samples_split * n_samples)` are the minimum\n",
       "      number of samples for each split.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_samples_leaf : int or float, default=1\n",
       "    The minimum number of samples required to be at a leaf node.\n",
       "    A split point at any depth will only be considered if it leaves at\n",
       "    least ``min_samples_leaf`` training samples in each of the left and\n",
       "    right branches.  This may have the effect of smoothing the model,\n",
       "    especially in regression.\n",
       "\n",
       "    - If int, then consider `min_samples_leaf` as the minimum number.\n",
       "    - If float, then `min_samples_leaf` is a fraction and\n",
       "      `ceil(min_samples_leaf * n_samples)` are the minimum\n",
       "      number of samples for each node.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_weight_fraction_leaf : float, default=0.0\n",
       "    The minimum weighted fraction of the sum total of weights (of all\n",
       "    the input samples) required to be at a leaf node. Samples have\n",
       "    equal weight when sample_weight is not provided.\n",
       "\n",
       "max_depth : int, default=3\n",
       "    maximum depth of the individual regression estimators. The maximum\n",
       "    depth limits the number of nodes in the tree. Tune this parameter\n",
       "    for best performance; the best value depends on the interaction\n",
       "    of the input variables.\n",
       "\n",
       "min_impurity_decrease : float, default=0.0\n",
       "    A node will be split if this split induces a decrease of the impurity\n",
       "    greater than or equal to this value.\n",
       "\n",
       "    The weighted impurity decrease equation is the following::\n",
       "\n",
       "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
       "                            - N_t_L / N_t * left_impurity)\n",
       "\n",
       "    where ``N`` is the total number of samples, ``N_t`` is the number of\n",
       "    samples at the current node, ``N_t_L`` is the number of samples in the\n",
       "    left child, and ``N_t_R`` is the number of samples in the right child.\n",
       "\n",
       "    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
       "    if ``sample_weight`` is passed.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "\n",
       "min_impurity_split : float, default=None\n",
       "    Threshold for early stopping in tree growth. A node will split\n",
       "    if its impurity is above the threshold, otherwise it is a leaf.\n",
       "\n",
       "    .. deprecated:: 0.19\n",
       "       ``min_impurity_split`` has been deprecated in favor of\n",
       "       ``min_impurity_decrease`` in 0.19. The default value of\n",
       "       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n",
       "       will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
       "\n",
       "init : estimator or 'zero', default=None\n",
       "    An estimator object that is used to compute the initial predictions.\n",
       "    ``init`` has to provide :term:`fit` and :term:`predict`. If 'zero', the\n",
       "    initial raw predictions are set to zero. By default a\n",
       "    ``DummyEstimator`` is used, predicting either the average target value\n",
       "    (for loss='ls'), or a quantile for the other losses.\n",
       "\n",
       "random_state : int or RandomState, default=None\n",
       "    Controls the random seed given to each Tree estimator at each\n",
       "    boosting iteration.\n",
       "    In addition, it controls the random permutation of the features at\n",
       "    each split (see Notes for more details).\n",
       "    It also controls the random spliting of the training data to obtain a\n",
       "    validation set if `n_iter_no_change` is not None.\n",
       "    Pass an int for reproducible output across multiple function calls.\n",
       "    See :term:`Glossary <random_state>`.\n",
       "\n",
       "max_features : {'auto', 'sqrt', 'log2'}, int or float, default=None\n",
       "    The number of features to consider when looking for the best split:\n",
       "\n",
       "    - If int, then consider `max_features` features at each split.\n",
       "    - If float, then `max_features` is a fraction and\n",
       "      `int(max_features * n_features)` features are considered at each\n",
       "      split.\n",
       "    - If \"auto\", then `max_features=n_features`.\n",
       "    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
       "    - If \"log2\", then `max_features=log2(n_features)`.\n",
       "    - If None, then `max_features=n_features`.\n",
       "\n",
       "    Choosing `max_features < n_features` leads to a reduction of variance\n",
       "    and an increase in bias.\n",
       "\n",
       "    Note: the search for a split does not stop until at least one\n",
       "    valid partition of the node samples is found, even if it requires to\n",
       "    effectively inspect more than ``max_features`` features.\n",
       "\n",
       "alpha : float, default=0.9\n",
       "    The alpha-quantile of the huber loss function and the quantile\n",
       "    loss function. Only if ``loss='huber'`` or ``loss='quantile'``.\n",
       "\n",
       "verbose : int, default=0\n",
       "    Enable verbose output. If 1 then it prints progress and performance\n",
       "    once in a while (the more trees the lower the frequency). If greater\n",
       "    than 1 then it prints progress and performance for every tree.\n",
       "\n",
       "max_leaf_nodes : int, default=None\n",
       "    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
       "    Best nodes are defined as relative reduction in impurity.\n",
       "    If None then unlimited number of leaf nodes.\n",
       "\n",
       "warm_start : bool, default=False\n",
       "    When set to ``True``, reuse the solution of the previous call to fit\n",
       "    and add more estimators to the ensemble, otherwise, just erase the\n",
       "    previous solution. See :term:`the Glossary <warm_start>`.\n",
       "\n",
       "presort : deprecated, default='deprecated'\n",
       "    This parameter is deprecated and will be removed in v0.24.\n",
       "\n",
       "    .. deprecated :: 0.22\n",
       "\n",
       "validation_fraction : float, default=0.1\n",
       "    The proportion of training data to set aside as validation set for\n",
       "    early stopping. Must be between 0 and 1.\n",
       "    Only used if ``n_iter_no_change`` is set to an integer.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "n_iter_no_change : int, default=None\n",
       "    ``n_iter_no_change`` is used to decide if early stopping will be used\n",
       "    to terminate training when validation score is not improving. By\n",
       "    default it is set to None to disable early stopping. If set to a\n",
       "    number, it will set aside ``validation_fraction`` size of the training\n",
       "    data as validation and terminate training when validation score is not\n",
       "    improving in all of the previous ``n_iter_no_change`` numbers of\n",
       "    iterations.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "tol : float, default=1e-4\n",
       "    Tolerance for the early stopping. When the loss is not improving\n",
       "    by at least tol for ``n_iter_no_change`` iterations (if set to a\n",
       "    number), the training stops.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "ccp_alpha : non-negative float, default=0.0\n",
       "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
       "    subtree with the largest cost complexity that is smaller than\n",
       "    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
       "    :ref:`minimal_cost_complexity_pruning` for details.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "feature_importances_ : ndarray of shape (n_features,)\n",
       "    The impurity-based feature importances.\n",
       "    The higher, the more important the feature.\n",
       "    The importance of a feature is computed as the (normalized)\n",
       "    total reduction of the criterion brought by that feature.  It is also\n",
       "    known as the Gini importance.\n",
       "\n",
       "    Warning: impurity-based feature importances can be misleading for\n",
       "    high cardinality features (many unique values). See\n",
       "    :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
       "\n",
       "oob_improvement_ : ndarray of shape (n_estimators,)\n",
       "    The improvement in loss (= deviance) on the out-of-bag samples\n",
       "    relative to the previous iteration.\n",
       "    ``oob_improvement_[0]`` is the improvement in\n",
       "    loss of the first stage over the ``init`` estimator.\n",
       "    Only available if ``subsample < 1.0``\n",
       "\n",
       "train_score_ : ndarray of shape (n_estimators,)\n",
       "    The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n",
       "    model at iteration ``i`` on the in-bag sample.\n",
       "    If ``subsample == 1`` this is the deviance on the training data.\n",
       "\n",
       "loss_ : LossFunction\n",
       "    The concrete ``LossFunction`` object.\n",
       "\n",
       "init_ : estimator\n",
       "    The estimator that provides the initial predictions.\n",
       "    Set via the ``init`` argument or ``loss.init_estimator``.\n",
       "\n",
       "estimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, 1)\n",
       "    The collection of fitted sub-estimators.\n",
       "\n",
       "n_features_ : int\n",
       "    The number of data features.\n",
       "\n",
       "max_features_ : int\n",
       "    The inferred value of max_features.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The features are always randomly permuted at each split. Therefore,\n",
       "the best found split may vary, even with the same training data and\n",
       "``max_features=n_features``, if the improvement of the criterion is\n",
       "identical for several splits enumerated during the search of the best\n",
       "split. To obtain a deterministic behaviour during fitting,\n",
       "``random_state`` has to be fixed.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.datasets import make_regression\n",
       ">>> from sklearn.ensemble import GradientBoostingRegressor\n",
       ">>> from sklearn.model_selection import train_test_split\n",
       ">>> X, y = make_regression(random_state=0)\n",
       ">>> X_train, X_test, y_train, y_test = train_test_split(\n",
       "...     X, y, random_state=0)\n",
       ">>> reg = GradientBoostingRegressor(random_state=0)\n",
       ">>> reg.fit(X_train, y_train)\n",
       "GradientBoostingRegressor(random_state=0)\n",
       ">>> reg.predict(X_test[1:2])\n",
       "array([-61...])\n",
       ">>> reg.score(X_test, y_test)\n",
       "0.4...\n",
       "\n",
       "See also\n",
       "--------\n",
       "sklearn.ensemble.HistGradientBoostingRegressor,\n",
       "sklearn.tree.DecisionTreeRegressor, RandomForestRegressor\n",
       "\n",
       "References\n",
       "----------\n",
       "J. Friedman, Greedy Function Approximation: A Gradient Boosting\n",
       "Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n",
       "\n",
       "J. Friedman, Stochastic Gradient Boosting, 1999\n",
       "\n",
       "T. Hastie, R. Tibshirani and J. Friedman.\n",
       "Elements of Statistical Learning Ed. 2, Springer, 2009.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?gbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~ Parametreler\n",
    "\n",
    "* kayıp: {'ls', 'lad', 'huber', 'quantile'}, default = 'ls'\n",
    "\n",
    "Kayıp fonksiyonu optimize edilecek. 'ls' en küçük kareler regresyonunu ifade eder.'lad' (en küçük mutlak sapma), yalnızca girdi değişkenlerinin sıra bilgisine dayanan oldukça sağlam bir kayıp işlevidir. 'huber' ikisinin birleşimidir. 'quantile'' niceliksel regresyona izin verir (quantile' belirtmek için alfa kullanın).\n",
    "* learning_rate : float, default=0.1\n",
    "\n",
    "Öğrenme oranı, öğrenme hızı ile her ağacın katkısını küçültür. Learning_rate ve n_estimators arasında bir değiş tokuş vardır.\n",
    "* n_estimators : int, default=100\n",
    "\n",
    "Gerçekleştirilecek güçlendirme aşamalarının sayısı.Gradyan artırma, aşırı takmaya karşı oldukça dayanıklıdır, bu nedenle çok sayıda olması genellikle daha iyi performans sağlar.\n",
    "* subsample : float, default=1.0\n",
    "\n",
    "Bireysel temel öğrencilere uydurmak için kullanılacak örneklerin oranı. 1.0'dan küçükse, bu Stokastik Gradyan Artışı ile sonuçlanır.alt örnek, n_estimators parametresi ile etkileşime girer. Alt numunenin < 1.0 seçilmesi, varyansın azalmasına ve sapmada bir artışa yol açar.\n",
    "* max_depth : int, default=3\n",
    "\n",
    "Bireysel regresyon tahmincilerinin maksimum derinliği. Maksimum derinlik, ağaçtaki düğüm sayısını sınırlar. En iyi performans için bu parametreyi ayarlayın; en iyi değer, girdi değişkenlerinin etkileşimine bağlıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 2520 candidates, totalling 25200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 399 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done 754 tasks      | elapsed:   20.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1200 tasks      | elapsed:   45.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1788 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2516 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3298 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3968 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4994 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done 5914 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done 7198 tasks      | elapsed: 11.6min\n",
      "[Parallel(n_jobs=-1)]: Done 8640 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=-1)]: Done 9925 tasks      | elapsed: 16.2min\n",
      "[Parallel(n_jobs=-1)]: Done 11348 tasks      | elapsed: 19.7min\n",
      "[Parallel(n_jobs=-1)]: Done 12731 tasks      | elapsed: 24.0min\n",
      "[Parallel(n_jobs=-1)]: Done 14984 tasks      | elapsed: 26.0min\n",
      "[Parallel(n_jobs=-1)]: Done 16628 tasks      | elapsed: 30.1min\n",
      "[Parallel(n_jobs=-1)]: Done 18474 tasks      | elapsed: 34.2min\n",
      "[Parallel(n_jobs=-1)]: Done 20965 tasks      | elapsed: 37.9min\n",
      "[Parallel(n_jobs=-1)]: Done 22976 tasks      | elapsed: 42.6min\n",
      "[Parallel(n_jobs=-1)]: Done 25181 tasks      | elapsed: 48.6min\n",
      "[Parallel(n_jobs=-1)]: Done 25200 out of 25200 | elapsed: 48.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "250.91525172248444"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bu işlem çok uzun sürüyor, bu nedenle kodu cv = 10 yerine cv =5 ile çalıştırmak daha iyi olur.\n",
    "\n",
    "gbm_params = {\"learning_rate\": [0.001,0.1,0.01, 0.05],\n",
    "             \"max_depth\": [1,2,3,5,8,9,10],\n",
    "             \"n_estimators\": [50,100,200,500,1000],\n",
    "             \"subsample\": [2,1.5,1,0.4,0.5,0.7],\n",
    "             \"loss\": [\"ls\",\"lad\",\"quantile\"]}                  \n",
    "gbm_cv_model = GridSearchCV(gbm_model, gbm_params, cv = 10, n_jobs = -1, verbose = 2).fit(X_train, y_train)\n",
    "gbm_tuned = GradientBoostingRegressor(**gbm_cv_model.best_params_).fit(X_train, y_train)                             \n",
    "y_pred = gbm_tuned.predict(X_test)                             \n",
    "df_gbm_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))                             \n",
    "df_gbm_tuned_rmse      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.05,\n",
       " 'loss': 'lad',\n",
       " 'max_depth': 2,\n",
       " 'n_estimators': 1000,\n",
       " 'subsample': 0.5}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_cv_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.05, loss='lad', max_depth=2,\n",
       "                          n_estimators=1000, subsample=0.5)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_cv_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost (Extreme Gradient Boosting) Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        XGBRegressor\n",
       "\u001b[1;31mString form:\u001b[0m\n",
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "           colsample_bynode <...> s_weight=1, subsample=1,\n",
       "           tree_method='exact', validate_parameters=1, verbosity=None)\n",
       "\u001b[1;31mFile:\u001b[0m        c:\\users\\toshiba\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\n",
       "\u001b[1;31mDocstring:\u001b[0m  \n",
       "Implementation of the scikit-learn API for XGBoost regression.\n",
       "\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "\n",
       "    n_estimators : int\n",
       "        Number of gradient boosted trees.  Equivalent to number of boosting\n",
       "        rounds.\n",
       "\n",
       "    max_depth : int\n",
       "        Maximum tree depth for base learners.\n",
       "    learning_rate : float\n",
       "        Boosting learning rate (xgb's \"eta\")\n",
       "    verbosity : int\n",
       "        The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
       "    objective : string or callable\n",
       "        Specify the learning task and the corresponding learning objective or\n",
       "        a custom objective function to be used (see note below).\n",
       "    booster: string\n",
       "        Specify which booster to use: gbtree, gblinear or dart.\n",
       "    tree_method: string\n",
       "        Specify which tree method to use.  Default to auto.  If this parameter\n",
       "        is set to default, XGBoost will choose the most conservative option\n",
       "        available.  It's recommended to study this option from parameters\n",
       "        document.\n",
       "    n_jobs : int\n",
       "        Number of parallel threads used to run xgboost.\n",
       "    gamma : float\n",
       "        Minimum loss reduction required to make a further partition on a leaf\n",
       "        node of the tree.\n",
       "    min_child_weight : float\n",
       "        Minimum sum of instance weight(hessian) needed in a child.\n",
       "    max_delta_step : int\n",
       "        Maximum delta step we allow each tree's weight estimation to be.\n",
       "    subsample : float\n",
       "        Subsample ratio of the training instance.\n",
       "    colsample_bytree : float\n",
       "        Subsample ratio of columns when constructing each tree.\n",
       "    colsample_bylevel : float\n",
       "        Subsample ratio of columns for each level.\n",
       "    colsample_bynode : float\n",
       "        Subsample ratio of columns for each split.\n",
       "    reg_alpha : float (xgb's alpha)\n",
       "        L1 regularization term on weights\n",
       "    reg_lambda : float (xgb's lambda)\n",
       "        L2 regularization term on weights\n",
       "    scale_pos_weight : float\n",
       "        Balancing of positive and negative weights.\n",
       "    base_score:\n",
       "        The initial prediction score of all instances, global bias.\n",
       "    random_state : int\n",
       "        Random number seed.\n",
       "\n",
       "        .. note::\n",
       "\n",
       "           Using gblinear booster with shotgun updater is nondeterministic as\n",
       "           it uses Hogwild algorithm.\n",
       "\n",
       "    missing : float, default np.nan\n",
       "        Value in the data which needs to be present as a missing value.\n",
       "    num_parallel_tree: int\n",
       "        Used for boosting random forest.\n",
       "    monotone_constraints : str\n",
       "        Constraint of variable monotonicity.  See tutorial for more\n",
       "        information.\n",
       "    interaction_constraints : str\n",
       "        Constraints for interaction representing permitted interactions.  The\n",
       "        constraints must be specified in the form of a nest list, e.g. [[0, 1],\n",
       "        [2, 3, 4]], where each inner list is a group of indices of features\n",
       "        that are allowed to interact with each other.  See tutorial for more\n",
       "        information\n",
       "    importance_type: string, default \"gain\"\n",
       "        The feature importance type for the feature_importances\\_ property:\n",
       "        either \"gain\", \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n",
       "\n",
       "    \\*\\*kwargs : dict, optional\n",
       "        Keyword arguments for XGBoost Booster object.  Full documentation of\n",
       "        parameters can be found here:\n",
       "        https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
       "        Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
       "        dict simultaneously will result in a TypeError.\n",
       "\n",
       "        .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
       "\n",
       "            \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
       "            that parameters passed via this argument will interact properly\n",
       "            with scikit-learn.\n",
       "\n",
       "        .. note::  Custom objective function\n",
       "\n",
       "            A custom objective function can be provided for the ``objective``\n",
       "            parameter. In this case, it should have the signature\n",
       "            ``objective(y_true, y_pred) -> grad, hess``:\n",
       "\n",
       "            y_true: array_like of shape [n_samples]\n",
       "                The target values\n",
       "            y_pred: array_like of shape [n_samples]\n",
       "                The predicted values\n",
       "\n",
       "            grad: array_like of shape [n_samples]\n",
       "                The value of the gradient for each sample point.\n",
       "            hess: array_like of shape [n_samples]\n",
       "                The value of the second derivative for each sample point\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 480 candidates, totalling 4800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:   21.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1122 tasks      | elapsed:   29.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1858 tasks      | elapsed:   53.5s\n",
      "[Parallel(n_jobs=-1)]: Done 2385 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2992 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3681 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4450 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4800 out of 4800 | elapsed:  2.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "277.1273319664687"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "xgb_params = {\"learning_rate\": [0.1,0.01,0.5,0.7,0.8],\n",
    "             \"max_depth\": [3,4,5,6,7,8],\n",
    "             \"n_estimators\": [100,200,500,1000],\n",
    "             \"colsample_bytree\": [0.5,0.7,0.8,0.9]}\n",
    "xgb_cv_model  = GridSearchCV(xgb,xgb_params, cv = 10, n_jobs = -1, verbose = 2).fit(X_train, y_train)\n",
    "xgb_tuned = XGBRegressor(**xgb_cv_model.best_params_).fit(X_train, y_train)\n",
    "y_pred = xgb_tuned.predict(X_test)\n",
    "df_xgb_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "df_xgb_tuned_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.5,\n",
       " 'learning_rate': 0.01,\n",
       " 'max_depth': 5,\n",
       " 'n_estimators': 500}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_cv_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.01, max_delta_step=0, max_depth=5,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=500, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_cv_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m           LGBMRegressor\n",
       "\u001b[1;31mString form:\u001b[0m    LGBMRegressor()\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\toshiba\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\n",
       "\u001b[1;31mDocstring:\u001b[0m      LightGBM regressor.\n",
       "\u001b[1;31mInit docstring:\u001b[0m\n",
       "Construct a gradient boosting model.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "boosting_type : string, optional (default='gbdt')\n",
       "    'gbdt', traditional Gradient Boosting Decision Tree.\n",
       "    'dart', Dropouts meet Multiple Additive Regression Trees.\n",
       "    'goss', Gradient-based One-Side Sampling.\n",
       "    'rf', Random Forest.\n",
       "num_leaves : int, optional (default=31)\n",
       "    Maximum tree leaves for base learners.\n",
       "max_depth : int, optional (default=-1)\n",
       "    Maximum tree depth for base learners, <=0 means no limit.\n",
       "learning_rate : float, optional (default=0.1)\n",
       "    Boosting learning rate.\n",
       "    You can use ``callbacks`` parameter of ``fit`` method to shrink/adapt learning rate\n",
       "    in training using ``reset_parameter`` callback.\n",
       "    Note, that this will ignore the ``learning_rate`` argument in training.\n",
       "n_estimators : int, optional (default=100)\n",
       "    Number of boosted trees to fit.\n",
       "subsample_for_bin : int, optional (default=200000)\n",
       "    Number of samples for constructing bins.\n",
       "objective : string, callable or None, optional (default=None)\n",
       "    Specify the learning task and the corresponding learning objective or\n",
       "    a custom objective function to be used (see note below).\n",
       "    Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n",
       "class_weight : dict, 'balanced' or None, optional (default=None)\n",
       "    Weights associated with classes in the form ``{class_label: weight}``.\n",
       "    Use this parameter only for multi-class classification task;\n",
       "    for binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters.\n",
       "    Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities.\n",
       "    You may want to consider performing probability calibration\n",
       "    (https://scikit-learn.org/stable/modules/calibration.html) of your model.\n",
       "    The 'balanced' mode uses the values of y to automatically adjust weights\n",
       "    inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.\n",
       "    If None, all classes are supposed to have weight one.\n",
       "    Note, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method)\n",
       "    if ``sample_weight`` is specified.\n",
       "min_split_gain : float, optional (default=0.)\n",
       "    Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
       "min_child_weight : float, optional (default=1e-3)\n",
       "    Minimum sum of instance weight (hessian) needed in a child (leaf).\n",
       "min_child_samples : int, optional (default=20)\n",
       "    Minimum number of data needed in a child (leaf).\n",
       "subsample : float, optional (default=1.)\n",
       "    Subsample ratio of the training instance.\n",
       "subsample_freq : int, optional (default=0)\n",
       "    Frequence of subsample, <=0 means no enable.\n",
       "colsample_bytree : float, optional (default=1.)\n",
       "    Subsample ratio of columns when constructing each tree.\n",
       "reg_alpha : float, optional (default=0.)\n",
       "    L1 regularization term on weights.\n",
       "reg_lambda : float, optional (default=0.)\n",
       "    L2 regularization term on weights.\n",
       "random_state : int, RandomState object or None, optional (default=None)\n",
       "    Random number seed.\n",
       "    If int, this number is used to seed the C++ code.\n",
       "    If RandomState object (numpy), a random integer is picked based on its state to seed the C++ code.\n",
       "    If None, default seeds in C++ code are used.\n",
       "n_jobs : int, optional (default=-1)\n",
       "    Number of parallel threads.\n",
       "silent : bool, optional (default=True)\n",
       "    Whether to print messages while running boosting.\n",
       "importance_type : string, optional (default='split')\n",
       "    The type of feature importance to be filled into ``feature_importances_``.\n",
       "    If 'split', result contains numbers of times the feature is used in a model.\n",
       "    If 'gain', result contains total gains of splits which use the feature.\n",
       "**kwargs\n",
       "    Other parameters for the model.\n",
       "    Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.\n",
       "\n",
       "    .. warning::\n",
       "\n",
       "        \\*\\*kwargs is not supported in sklearn, it may cause unexpected issues.\n",
       "\n",
       "Note\n",
       "----\n",
       "A custom objective function can be provided for the ``objective`` parameter.\n",
       "In this case, it should have the signature\n",
       "``objective(y_true, y_pred) -> grad, hess`` or\n",
       "``objective(y_true, y_pred, group) -> grad, hess``:\n",
       "\n",
       "    y_true : array-like of shape = [n_samples]\n",
       "        The target values.\n",
       "    y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
       "        The predicted values.\n",
       "    group : array-like\n",
       "        Group/query data, used for ranking task.\n",
       "    grad : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
       "        The value of the first order derivative (gradient) for each sample point.\n",
       "    hess : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
       "        The value of the second order derivative (Hessian) for each sample point.\n",
       "\n",
       "For binary task, the y_pred is margin.\n",
       "For multi-class task, the y_pred is group by class_id first, then group by row_id.\n",
       "If you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i]\n",
       "and you should group grad and hess in this way as well.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning_rate default = 0.1, n_estimators default = 100, colsample_bytree default = 1, max_depth default = -1, n_jobs default=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 2310 candidates, totalling 23100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1072 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 2344 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=-1)]: Done 4608 tasks      | elapsed:   28.9s\n",
      "[Parallel(n_jobs=-1)]: Done 7528 tasks      | elapsed:   49.4s\n",
      "[Parallel(n_jobs=-1)]: Done 11088 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 13376 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 16596 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 23100 out of 23100 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "275.41846202275894"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_params = {\"learning_rate\": [0.01,0.001, 0.1, 0.5, 1],\n",
    "              \"n_estimators\": [50,80,100,200,500,1000],\n",
    "              \"max_depth\": [-1.5, -1.3, -1, 0.3, 0.5,0.7,2,4,6,7,10],\n",
    "              \"colsample_bytree\": [0.1,0.3,0.5,0.7,1,1.3,1.5]}\n",
    "lgbm_cv_model = GridSearchCV(lgbm_model, lgbm_params, cv = 10, n_jobs = -1, verbose =2).fit(X_train, y_train)\n",
    "lgbm_tuned = LGBMRegressor(**lgbm_cv_model.best_params_).fit(X_train, y_train)                              \n",
    "y_pred = lgbm_tuned.predict(X_test)                              \n",
    "df_lgbm_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))                              \n",
    "df_lgbm_tuned_rmse  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.3,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': -1,\n",
       " 'n_estimators': 50}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_cv_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(colsample_bytree=0.3, n_estimators=50)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_cv_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost (Category Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m            CatBoostRegressor\n",
       "\u001b[1;31mString form:\u001b[0m     <catboost.core.CatBoostRegressor object at 0x000001DD0AD0BB50>\n",
       "\u001b[1;31mFile:\u001b[0m            c:\\users\\toshiba\\anaconda3\\lib\\site-packages\\catboost\\core.py\n",
       "\u001b[1;31mDocstring:\u001b[0m       <no docstring>\n",
       "\u001b[1;31mClass docstring:\u001b[0m CatBoost model. Contains training, prediction and evaluation methods.\n",
       "\u001b[1;31mInit docstring:\u001b[0m \n",
       "Initialize the CatBoost.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "params : dict\n",
       "    Parameters for CatBoost.\n",
       "    If  None, all params are set to their defaults.\n",
       "    If  dict, overriding parameters present in dict.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?catb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hem 10 hem de 5 kat denedim. 5 kat (cv =5) ile yerleştirme (düşük) daha iyi sonuç verdi. 10 kat ile rmse değeri 255 iken, 5 kat ile 240 olmuştur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:   53.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 458.4632952\ttotal: 808us\tremaining: 403ms\n",
      "1:\tlearn: 439.1158680\ttotal: 1.47ms\tremaining: 365ms\n",
      "2:\tlearn: 421.8718599\ttotal: 2.04ms\tremaining: 338ms\n",
      "3:\tlearn: 406.8068268\ttotal: 2.68ms\tremaining: 333ms\n",
      "4:\tlearn: 391.1204598\ttotal: 3.27ms\tremaining: 324ms\n",
      "5:\tlearn: 381.8107190\ttotal: 3.84ms\tremaining: 316ms\n",
      "6:\tlearn: 370.0274807\ttotal: 4.4ms\tremaining: 310ms\n",
      "7:\tlearn: 358.7262922\ttotal: 4.97ms\tremaining: 306ms\n",
      "8:\tlearn: 348.5480164\ttotal: 5.5ms\tremaining: 300ms\n",
      "9:\tlearn: 338.6785326\ttotal: 6.05ms\tremaining: 297ms\n",
      "10:\tlearn: 330.6497064\ttotal: 6.58ms\tremaining: 292ms\n",
      "11:\tlearn: 326.1322469\ttotal: 7.14ms\tremaining: 290ms\n",
      "12:\tlearn: 319.6098815\ttotal: 7.66ms\tremaining: 287ms\n",
      "13:\tlearn: 315.2128779\ttotal: 8.21ms\tremaining: 285ms\n",
      "14:\tlearn: 310.7437324\ttotal: 8.73ms\tremaining: 282ms\n",
      "15:\tlearn: 304.3962710\ttotal: 9.29ms\tremaining: 281ms\n",
      "16:\tlearn: 299.6595157\ttotal: 9.99ms\tremaining: 284ms\n",
      "17:\tlearn: 295.4833647\ttotal: 10.6ms\tremaining: 283ms\n",
      "18:\tlearn: 292.0286526\ttotal: 11.1ms\tremaining: 282ms\n",
      "19:\tlearn: 286.9359339\ttotal: 11.7ms\tremaining: 280ms\n",
      "20:\tlearn: 283.9572393\ttotal: 12.2ms\tremaining: 279ms\n",
      "21:\tlearn: 282.1883106\ttotal: 12.8ms\tremaining: 278ms\n",
      "22:\tlearn: 276.2922708\ttotal: 13.3ms\tremaining: 276ms\n",
      "23:\tlearn: 272.9955999\ttotal: 13.8ms\tremaining: 274ms\n",
      "24:\tlearn: 269.0677373\ttotal: 14.3ms\tremaining: 272ms\n",
      "25:\tlearn: 264.6011235\ttotal: 14.9ms\tremaining: 271ms\n",
      "26:\tlearn: 261.2525266\ttotal: 15.4ms\tremaining: 269ms\n",
      "27:\tlearn: 258.3172867\ttotal: 15.9ms\tremaining: 268ms\n",
      "28:\tlearn: 257.3931227\ttotal: 16.4ms\tremaining: 267ms\n",
      "29:\tlearn: 254.3587319\ttotal: 17ms\tremaining: 266ms\n",
      "30:\tlearn: 253.1187064\ttotal: 17.5ms\tremaining: 264ms\n",
      "31:\tlearn: 250.1605144\ttotal: 18ms\tremaining: 263ms\n",
      "32:\tlearn: 246.9878250\ttotal: 18.6ms\tremaining: 263ms\n",
      "33:\tlearn: 244.1152284\ttotal: 19.1ms\tremaining: 262ms\n",
      "34:\tlearn: 240.9548489\ttotal: 19.7ms\tremaining: 262ms\n",
      "35:\tlearn: 238.4775299\ttotal: 20.3ms\tremaining: 261ms\n",
      "36:\tlearn: 237.7583773\ttotal: 20.9ms\tremaining: 261ms\n",
      "37:\tlearn: 235.9677358\ttotal: 21.4ms\tremaining: 260ms\n",
      "38:\tlearn: 234.8607233\ttotal: 22ms\tremaining: 260ms\n",
      "39:\tlearn: 230.1758898\ttotal: 22.6ms\tremaining: 259ms\n",
      "40:\tlearn: 226.6629182\ttotal: 23.2ms\tremaining: 259ms\n",
      "41:\tlearn: 224.5708957\ttotal: 23.7ms\tremaining: 259ms\n",
      "42:\tlearn: 222.3393611\ttotal: 24.3ms\tremaining: 258ms\n",
      "43:\tlearn: 219.1627890\ttotal: 24.9ms\tremaining: 258ms\n",
      "44:\tlearn: 218.3430285\ttotal: 25.6ms\tremaining: 258ms\n",
      "45:\tlearn: 217.7979747\ttotal: 26.1ms\tremaining: 258ms\n",
      "46:\tlearn: 216.9993590\ttotal: 26.8ms\tremaining: 258ms\n",
      "47:\tlearn: 216.3126751\ttotal: 27.4ms\tremaining: 258ms\n",
      "48:\tlearn: 214.3483764\ttotal: 28ms\tremaining: 258ms\n",
      "49:\tlearn: 211.4449372\ttotal: 28.6ms\tremaining: 258ms\n",
      "50:\tlearn: 210.7276313\ttotal: 29.2ms\tremaining: 257ms\n",
      "51:\tlearn: 210.0977608\ttotal: 29.8ms\tremaining: 257ms\n",
      "52:\tlearn: 208.3644692\ttotal: 30.4ms\tremaining: 256ms\n",
      "53:\tlearn: 207.6731175\ttotal: 31ms\tremaining: 256ms\n",
      "54:\tlearn: 206.6803761\ttotal: 31.6ms\tremaining: 255ms\n",
      "55:\tlearn: 203.7193833\ttotal: 32.1ms\tremaining: 255ms\n",
      "56:\tlearn: 200.5401230\ttotal: 32.7ms\tremaining: 254ms\n",
      "57:\tlearn: 199.9186526\ttotal: 33.5ms\tremaining: 255ms\n",
      "58:\tlearn: 197.4586309\ttotal: 34.1ms\tremaining: 255ms\n",
      "59:\tlearn: 196.9141833\ttotal: 34.7ms\tremaining: 255ms\n",
      "60:\tlearn: 194.5755964\ttotal: 35.3ms\tremaining: 254ms\n",
      "61:\tlearn: 193.8772810\ttotal: 35.9ms\tremaining: 254ms\n",
      "62:\tlearn: 193.3528538\ttotal: 36.6ms\tremaining: 254ms\n",
      "63:\tlearn: 191.8727274\ttotal: 37.2ms\tremaining: 253ms\n",
      "64:\tlearn: 191.3923695\ttotal: 37.7ms\tremaining: 253ms\n",
      "65:\tlearn: 189.3688797\ttotal: 38.3ms\tremaining: 252ms\n",
      "66:\tlearn: 187.0491869\ttotal: 38.9ms\tremaining: 251ms\n",
      "67:\tlearn: 186.5255017\ttotal: 39.5ms\tremaining: 251ms\n",
      "68:\tlearn: 186.0309669\ttotal: 40ms\tremaining: 250ms\n",
      "69:\tlearn: 185.4741789\ttotal: 40.6ms\tremaining: 249ms\n",
      "70:\tlearn: 182.9355077\ttotal: 41.1ms\tremaining: 248ms\n",
      "71:\tlearn: 182.4114459\ttotal: 41.6ms\tremaining: 248ms\n",
      "72:\tlearn: 182.0401402\ttotal: 42.2ms\tremaining: 247ms\n",
      "73:\tlearn: 179.8551994\ttotal: 42.7ms\tremaining: 246ms\n",
      "74:\tlearn: 178.3608256\ttotal: 43.3ms\tremaining: 245ms\n",
      "75:\tlearn: 176.8384550\ttotal: 43.9ms\tremaining: 245ms\n",
      "76:\tlearn: 175.6137364\ttotal: 44.4ms\tremaining: 244ms\n",
      "77:\tlearn: 175.2290115\ttotal: 45ms\tremaining: 243ms\n",
      "78:\tlearn: 174.8636660\ttotal: 45.6ms\tremaining: 243ms\n",
      "79:\tlearn: 172.4990113\ttotal: 46.2ms\tremaining: 243ms\n",
      "80:\tlearn: 172.0945311\ttotal: 46.9ms\tremaining: 243ms\n",
      "81:\tlearn: 171.7615625\ttotal: 47.4ms\tremaining: 242ms\n",
      "82:\tlearn: 171.4452463\ttotal: 48ms\tremaining: 241ms\n",
      "83:\tlearn: 169.7947494\ttotal: 48.6ms\tremaining: 241ms\n",
      "84:\tlearn: 169.4322881\ttotal: 49.2ms\tremaining: 240ms\n",
      "85:\tlearn: 169.1366194\ttotal: 49.8ms\tremaining: 240ms\n",
      "86:\tlearn: 168.4555028\ttotal: 50.6ms\tremaining: 240ms\n",
      "87:\tlearn: 167.0784374\ttotal: 51.2ms\tremaining: 240ms\n",
      "88:\tlearn: 166.8063716\ttotal: 51.7ms\tremaining: 239ms\n",
      "89:\tlearn: 164.6646024\ttotal: 52.3ms\tremaining: 238ms\n",
      "90:\tlearn: 164.4186839\ttotal: 52.8ms\tremaining: 237ms\n",
      "91:\tlearn: 163.8792184\ttotal: 53.4ms\tremaining: 237ms\n",
      "92:\tlearn: 162.8086346\ttotal: 53.9ms\tremaining: 236ms\n",
      "93:\tlearn: 160.5035254\ttotal: 54.4ms\tremaining: 235ms\n",
      "94:\tlearn: 159.0582987\ttotal: 54.9ms\tremaining: 234ms\n",
      "95:\tlearn: 157.9880138\ttotal: 55.3ms\tremaining: 233ms\n",
      "96:\tlearn: 157.0573934\ttotal: 55.8ms\tremaining: 232ms\n",
      "97:\tlearn: 156.6835352\ttotal: 56.2ms\tremaining: 231ms\n",
      "98:\tlearn: 155.7240986\ttotal: 56.7ms\tremaining: 230ms\n",
      "99:\tlearn: 155.4951899\ttotal: 57.1ms\tremaining: 228ms\n",
      "100:\tlearn: 154.3880679\ttotal: 57.6ms\tremaining: 227ms\n",
      "101:\tlearn: 153.4872645\ttotal: 58ms\tremaining: 226ms\n",
      "102:\tlearn: 152.9489753\ttotal: 58.5ms\tremaining: 225ms\n",
      "103:\tlearn: 151.0838349\ttotal: 59ms\tremaining: 224ms\n",
      "104:\tlearn: 150.2453783\ttotal: 59.4ms\tremaining: 223ms\n",
      "105:\tlearn: 149.2158762\ttotal: 59.8ms\tremaining: 222ms\n",
      "106:\tlearn: 148.3354199\ttotal: 60.3ms\tremaining: 221ms\n",
      "107:\tlearn: 147.5948534\ttotal: 60.7ms\tremaining: 220ms\n",
      "108:\tlearn: 146.0391470\ttotal: 61.2ms\tremaining: 219ms\n",
      "109:\tlearn: 145.3532434\ttotal: 61.6ms\tremaining: 218ms\n",
      "110:\tlearn: 144.7260629\ttotal: 62.1ms\tremaining: 218ms\n",
      "111:\tlearn: 143.6806013\ttotal: 62.5ms\tremaining: 217ms\n",
      "112:\tlearn: 142.7871037\ttotal: 63ms\tremaining: 216ms\n",
      "113:\tlearn: 141.4071481\ttotal: 63.4ms\tremaining: 215ms\n",
      "114:\tlearn: 140.6782701\ttotal: 63.9ms\tremaining: 214ms\n",
      "115:\tlearn: 139.7106755\ttotal: 64.3ms\tremaining: 213ms\n",
      "116:\tlearn: 138.8183202\ttotal: 64.8ms\tremaining: 212ms\n",
      "117:\tlearn: 137.6478223\ttotal: 65.2ms\tremaining: 211ms\n",
      "118:\tlearn: 137.0545031\ttotal: 65.7ms\tremaining: 210ms\n",
      "119:\tlearn: 136.7299885\ttotal: 66.1ms\tremaining: 209ms\n",
      "120:\tlearn: 136.3853366\ttotal: 66.6ms\tremaining: 208ms\n",
      "121:\tlearn: 135.9390981\ttotal: 67ms\tremaining: 208ms\n",
      "122:\tlearn: 135.3363775\ttotal: 67.5ms\tremaining: 207ms\n",
      "123:\tlearn: 135.1863620\ttotal: 67.9ms\tremaining: 206ms\n",
      "124:\tlearn: 135.0448037\ttotal: 68.3ms\tremaining: 205ms\n",
      "125:\tlearn: 134.3401928\ttotal: 68.8ms\tremaining: 204ms\n",
      "126:\tlearn: 134.0060916\ttotal: 69.3ms\tremaining: 204ms\n",
      "127:\tlearn: 133.3161414\ttotal: 69.8ms\tremaining: 203ms\n",
      "128:\tlearn: 132.1048227\ttotal: 70.2ms\tremaining: 202ms\n",
      "129:\tlearn: 130.3599593\ttotal: 70.7ms\tremaining: 201ms\n",
      "130:\tlearn: 129.2619841\ttotal: 71.1ms\tremaining: 200ms\n",
      "131:\tlearn: 128.9862415\ttotal: 71.6ms\tremaining: 200ms\n",
      "132:\tlearn: 128.1331931\ttotal: 72ms\tremaining: 199ms\n",
      "133:\tlearn: 127.3969328\ttotal: 72.5ms\tremaining: 198ms\n",
      "134:\tlearn: 126.8411727\ttotal: 72.9ms\tremaining: 197ms\n",
      "135:\tlearn: 126.3974574\ttotal: 73.3ms\tremaining: 196ms\n",
      "136:\tlearn: 125.8068414\ttotal: 73.8ms\tremaining: 196ms\n",
      "137:\tlearn: 124.8260231\ttotal: 74.2ms\tremaining: 195ms\n",
      "138:\tlearn: 123.7889993\ttotal: 74.7ms\tremaining: 194ms\n",
      "139:\tlearn: 123.0368640\ttotal: 75.1ms\tremaining: 193ms\n",
      "140:\tlearn: 122.8817539\ttotal: 75.6ms\tremaining: 192ms\n",
      "141:\tlearn: 122.0488603\ttotal: 76ms\tremaining: 192ms\n",
      "142:\tlearn: 121.3541433\ttotal: 76.5ms\tremaining: 191ms\n",
      "143:\tlearn: 120.9445291\ttotal: 76.9ms\tremaining: 190ms\n",
      "144:\tlearn: 120.4189590\ttotal: 77.4ms\tremaining: 189ms\n",
      "145:\tlearn: 120.1997253\ttotal: 77.8ms\tremaining: 189ms\n",
      "146:\tlearn: 120.0947121\ttotal: 78.3ms\tremaining: 188ms\n",
      "147:\tlearn: 119.9690680\ttotal: 78.7ms\tremaining: 187ms\n",
      "148:\tlearn: 119.3315711\ttotal: 79.2ms\tremaining: 187ms\n",
      "149:\tlearn: 119.2149248\ttotal: 79.6ms\tremaining: 186ms\n",
      "150:\tlearn: 118.6975708\ttotal: 80.1ms\tremaining: 185ms\n",
      "151:\tlearn: 118.5053099\ttotal: 80.5ms\tremaining: 184ms\n",
      "152:\tlearn: 117.4431955\ttotal: 81ms\tremaining: 184ms\n",
      "153:\tlearn: 117.1562845\ttotal: 81.4ms\tremaining: 183ms\n",
      "154:\tlearn: 116.5208908\ttotal: 81.9ms\tremaining: 182ms\n",
      "155:\tlearn: 115.8211733\ttotal: 82.3ms\tremaining: 182ms\n",
      "156:\tlearn: 114.8910748\ttotal: 82.8ms\tremaining: 181ms\n",
      "157:\tlearn: 113.9673548\ttotal: 83.3ms\tremaining: 180ms\n",
      "158:\tlearn: 113.7184395\ttotal: 83.7ms\tremaining: 180ms\n",
      "159:\tlearn: 112.9662222\ttotal: 84.2ms\tremaining: 179ms\n",
      "160:\tlearn: 112.5101529\ttotal: 84.6ms\tremaining: 178ms\n",
      "161:\tlearn: 111.5723935\ttotal: 85.1ms\tremaining: 177ms\n",
      "162:\tlearn: 111.0396684\ttotal: 85.5ms\tremaining: 177ms\n",
      "163:\tlearn: 110.4398177\ttotal: 86ms\tremaining: 176ms\n",
      "164:\tlearn: 110.0259931\ttotal: 86.4ms\tremaining: 175ms\n",
      "165:\tlearn: 109.3093251\ttotal: 86.8ms\tremaining: 175ms\n",
      "166:\tlearn: 108.7588589\ttotal: 87.3ms\tremaining: 174ms\n",
      "167:\tlearn: 108.1911397\ttotal: 87.7ms\tremaining: 173ms\n",
      "168:\tlearn: 108.0176156\ttotal: 88.2ms\tremaining: 173ms\n",
      "169:\tlearn: 107.0154187\ttotal: 88.6ms\tremaining: 172ms\n",
      "170:\tlearn: 106.5664542\ttotal: 89.1ms\tremaining: 171ms\n",
      "171:\tlearn: 106.2406968\ttotal: 89.6ms\tremaining: 171ms\n",
      "172:\tlearn: 105.6068920\ttotal: 90ms\tremaining: 170ms\n",
      "173:\tlearn: 105.4452553\ttotal: 90.5ms\tremaining: 169ms\n",
      "174:\tlearn: 105.1613691\ttotal: 90.9ms\tremaining: 169ms\n",
      "175:\tlearn: 104.5269850\ttotal: 91.3ms\tremaining: 168ms\n",
      "176:\tlearn: 104.3198336\ttotal: 91.8ms\tremaining: 168ms\n",
      "177:\tlearn: 103.9791706\ttotal: 92.4ms\tremaining: 167ms\n",
      "178:\tlearn: 103.2038043\ttotal: 93ms\tremaining: 167ms\n",
      "179:\tlearn: 102.9116795\ttotal: 93.5ms\tremaining: 166ms\n",
      "180:\tlearn: 102.6969663\ttotal: 94ms\tremaining: 166ms\n",
      "181:\tlearn: 102.4951361\ttotal: 94.5ms\tremaining: 165ms\n",
      "182:\tlearn: 102.1700597\ttotal: 94.9ms\tremaining: 164ms\n",
      "183:\tlearn: 102.0186047\ttotal: 95.4ms\tremaining: 164ms\n",
      "184:\tlearn: 101.0570342\ttotal: 95.8ms\tremaining: 163ms\n",
      "185:\tlearn: 100.1798718\ttotal: 96.3ms\tremaining: 163ms\n",
      "186:\tlearn: 99.8458834\ttotal: 96.7ms\tremaining: 162ms\n",
      "187:\tlearn: 99.5344789\ttotal: 97.2ms\tremaining: 161ms\n",
      "188:\tlearn: 98.9230829\ttotal: 97.6ms\tremaining: 161ms\n",
      "189:\tlearn: 98.1204824\ttotal: 98.1ms\tremaining: 160ms\n",
      "190:\tlearn: 97.5657519\ttotal: 98.5ms\tremaining: 159ms\n",
      "191:\tlearn: 96.3654910\ttotal: 99ms\tremaining: 159ms\n",
      "192:\tlearn: 95.8819824\ttotal: 99.4ms\tremaining: 158ms\n",
      "193:\tlearn: 95.5524035\ttotal: 99.9ms\tremaining: 158ms\n",
      "194:\tlearn: 94.9081376\ttotal: 100ms\tremaining: 157ms\n",
      "195:\tlearn: 94.6932813\ttotal: 101ms\tremaining: 156ms\n",
      "196:\tlearn: 94.2848928\ttotal: 101ms\tremaining: 156ms\n",
      "197:\tlearn: 93.5710330\ttotal: 102ms\tremaining: 155ms\n",
      "198:\tlearn: 93.4413212\ttotal: 102ms\tremaining: 154ms\n",
      "199:\tlearn: 92.8625251\ttotal: 103ms\tremaining: 154ms\n",
      "200:\tlearn: 92.3477410\ttotal: 103ms\tremaining: 153ms\n",
      "201:\tlearn: 91.9088113\ttotal: 103ms\tremaining: 153ms\n",
      "202:\tlearn: 91.7117673\ttotal: 104ms\tremaining: 152ms\n",
      "203:\tlearn: 91.6546543\ttotal: 104ms\tremaining: 151ms\n",
      "204:\tlearn: 90.9670266\ttotal: 105ms\tremaining: 151ms\n",
      "205:\tlearn: 90.4584059\ttotal: 105ms\tremaining: 150ms\n",
      "206:\tlearn: 89.9336188\ttotal: 106ms\tremaining: 150ms\n",
      "207:\tlearn: 89.2158358\ttotal: 106ms\tremaining: 149ms\n",
      "208:\tlearn: 88.4567817\ttotal: 107ms\tremaining: 148ms\n",
      "209:\tlearn: 88.3339772\ttotal: 107ms\tremaining: 148ms\n",
      "210:\tlearn: 88.2739231\ttotal: 107ms\tremaining: 147ms\n",
      "211:\tlearn: 87.8548698\ttotal: 108ms\tremaining: 147ms\n",
      "212:\tlearn: 87.7285992\ttotal: 108ms\tremaining: 146ms\n",
      "213:\tlearn: 87.6810489\ttotal: 109ms\tremaining: 145ms\n",
      "214:\tlearn: 87.4515552\ttotal: 109ms\tremaining: 145ms\n",
      "215:\tlearn: 87.4013250\ttotal: 110ms\tremaining: 144ms\n",
      "216:\tlearn: 87.0112982\ttotal: 110ms\tremaining: 144ms\n",
      "217:\tlearn: 86.9605679\ttotal: 111ms\tremaining: 143ms\n",
      "218:\tlearn: 86.9160818\ttotal: 111ms\tremaining: 142ms\n",
      "219:\tlearn: 86.7228875\ttotal: 112ms\tremaining: 142ms\n",
      "220:\tlearn: 86.2883163\ttotal: 112ms\tremaining: 141ms\n",
      "221:\tlearn: 86.0684751\ttotal: 112ms\tremaining: 141ms\n",
      "222:\tlearn: 85.6076700\ttotal: 113ms\tremaining: 140ms\n",
      "223:\tlearn: 85.4175574\ttotal: 113ms\tremaining: 140ms\n",
      "224:\tlearn: 84.6495993\ttotal: 114ms\tremaining: 139ms\n",
      "225:\tlearn: 84.4006938\ttotal: 114ms\tremaining: 138ms\n",
      "226:\tlearn: 84.0155095\ttotal: 115ms\tremaining: 138ms\n",
      "227:\tlearn: 83.7954465\ttotal: 115ms\tremaining: 137ms\n",
      "228:\tlearn: 82.9415089\ttotal: 116ms\tremaining: 137ms\n",
      "229:\tlearn: 82.9006422\ttotal: 116ms\tremaining: 136ms\n",
      "230:\tlearn: 82.4163896\ttotal: 116ms\tremaining: 136ms\n",
      "231:\tlearn: 81.8944316\ttotal: 117ms\tremaining: 135ms\n",
      "232:\tlearn: 81.7838096\ttotal: 117ms\tremaining: 134ms\n",
      "233:\tlearn: 81.7473185\ttotal: 118ms\tremaining: 134ms\n",
      "234:\tlearn: 81.5606813\ttotal: 118ms\tremaining: 133ms\n",
      "235:\tlearn: 81.2471221\ttotal: 119ms\tremaining: 133ms\n",
      "236:\tlearn: 81.2067687\ttotal: 119ms\tremaining: 132ms\n",
      "237:\tlearn: 80.9411812\ttotal: 120ms\tremaining: 132ms\n",
      "238:\tlearn: 80.7617620\ttotal: 120ms\tremaining: 131ms\n",
      "239:\tlearn: 80.3677473\ttotal: 121ms\tremaining: 131ms\n",
      "240:\tlearn: 80.3179806\ttotal: 121ms\tremaining: 130ms\n",
      "241:\tlearn: 80.2146612\ttotal: 122ms\tremaining: 130ms\n",
      "242:\tlearn: 79.9240955\ttotal: 122ms\tremaining: 129ms\n",
      "243:\tlearn: 79.6209806\ttotal: 123ms\tremaining: 129ms\n",
      "244:\tlearn: 79.2422376\ttotal: 123ms\tremaining: 129ms\n",
      "245:\tlearn: 79.1318921\ttotal: 124ms\tremaining: 128ms\n",
      "246:\tlearn: 79.0104427\ttotal: 124ms\tremaining: 128ms\n",
      "247:\tlearn: 78.8891208\ttotal: 125ms\tremaining: 127ms\n",
      "248:\tlearn: 78.8535285\ttotal: 125ms\tremaining: 126ms\n",
      "249:\tlearn: 78.4941099\ttotal: 126ms\tremaining: 126ms\n",
      "250:\tlearn: 77.9110594\ttotal: 126ms\tremaining: 125ms\n",
      "251:\tlearn: 77.7448974\ttotal: 127ms\tremaining: 125ms\n",
      "252:\tlearn: 77.5979549\ttotal: 127ms\tremaining: 124ms\n",
      "253:\tlearn: 77.1630690\ttotal: 128ms\tremaining: 124ms\n",
      "254:\tlearn: 77.1140131\ttotal: 128ms\tremaining: 123ms\n",
      "255:\tlearn: 76.5408998\ttotal: 129ms\tremaining: 123ms\n",
      "256:\tlearn: 76.0138469\ttotal: 129ms\tremaining: 122ms\n",
      "257:\tlearn: 75.5860669\ttotal: 130ms\tremaining: 122ms\n",
      "258:\tlearn: 75.2657650\ttotal: 130ms\tremaining: 121ms\n",
      "259:\tlearn: 75.0769658\ttotal: 131ms\tremaining: 121ms\n",
      "260:\tlearn: 74.5570164\ttotal: 131ms\tremaining: 120ms\n",
      "261:\tlearn: 74.0822122\ttotal: 132ms\tremaining: 120ms\n",
      "262:\tlearn: 73.8830700\ttotal: 132ms\tremaining: 119ms\n",
      "263:\tlearn: 73.7902500\ttotal: 133ms\tremaining: 119ms\n",
      "264:\tlearn: 73.6621953\ttotal: 134ms\tremaining: 118ms\n",
      "265:\tlearn: 73.4993223\ttotal: 134ms\tremaining: 118ms\n",
      "266:\tlearn: 73.2474944\ttotal: 135ms\tremaining: 117ms\n",
      "267:\tlearn: 73.1089128\ttotal: 136ms\tremaining: 117ms\n",
      "268:\tlearn: 72.4129261\ttotal: 136ms\tremaining: 117ms\n",
      "269:\tlearn: 72.3126234\ttotal: 137ms\tremaining: 117ms\n",
      "270:\tlearn: 71.9415310\ttotal: 138ms\tremaining: 116ms\n",
      "271:\tlearn: 71.4544532\ttotal: 138ms\tremaining: 116ms\n",
      "272:\tlearn: 71.3197428\ttotal: 139ms\tremaining: 116ms\n",
      "273:\tlearn: 71.0687558\ttotal: 139ms\tremaining: 115ms\n",
      "274:\tlearn: 70.8189982\ttotal: 140ms\tremaining: 115ms\n",
      "275:\tlearn: 70.6661812\ttotal: 141ms\tremaining: 114ms\n",
      "276:\tlearn: 70.1732740\ttotal: 141ms\tremaining: 114ms\n",
      "277:\tlearn: 70.0431766\ttotal: 142ms\tremaining: 113ms\n",
      "278:\tlearn: 69.7694103\ttotal: 142ms\tremaining: 113ms\n",
      "279:\tlearn: 69.6208862\ttotal: 143ms\tremaining: 112ms\n",
      "280:\tlearn: 69.5050135\ttotal: 144ms\tremaining: 112ms\n",
      "281:\tlearn: 69.0100859\ttotal: 144ms\tremaining: 111ms\n",
      "282:\tlearn: 68.6422009\ttotal: 145ms\tremaining: 111ms\n",
      "283:\tlearn: 68.3768057\ttotal: 145ms\tremaining: 111ms\n",
      "284:\tlearn: 68.3332496\ttotal: 146ms\tremaining: 110ms\n",
      "285:\tlearn: 68.1743413\ttotal: 147ms\tremaining: 110ms\n",
      "286:\tlearn: 67.8423595\ttotal: 148ms\tremaining: 110ms\n",
      "287:\tlearn: 67.3127183\ttotal: 149ms\tremaining: 109ms\n",
      "288:\tlearn: 66.9813147\ttotal: 149ms\tremaining: 109ms\n",
      "289:\tlearn: 66.4327557\ttotal: 150ms\tremaining: 108ms\n",
      "290:\tlearn: 66.3482532\ttotal: 150ms\tremaining: 108ms\n",
      "291:\tlearn: 65.9876837\ttotal: 151ms\tremaining: 107ms\n",
      "292:\tlearn: 65.8758275\ttotal: 151ms\tremaining: 107ms\n",
      "293:\tlearn: 65.5861847\ttotal: 152ms\tremaining: 106ms\n",
      "294:\tlearn: 65.4930432\ttotal: 152ms\tremaining: 106ms\n",
      "295:\tlearn: 65.1618533\ttotal: 153ms\tremaining: 105ms\n",
      "296:\tlearn: 64.6992356\ttotal: 153ms\tremaining: 105ms\n",
      "297:\tlearn: 64.3546576\ttotal: 154ms\tremaining: 104ms\n",
      "298:\tlearn: 64.0005344\ttotal: 154ms\tremaining: 104ms\n",
      "299:\tlearn: 63.6666452\ttotal: 155ms\tremaining: 103ms\n",
      "300:\tlearn: 63.6074089\ttotal: 155ms\tremaining: 103ms\n",
      "301:\tlearn: 63.2788674\ttotal: 156ms\tremaining: 102ms\n",
      "302:\tlearn: 63.0411169\ttotal: 156ms\tremaining: 102ms\n",
      "303:\tlearn: 62.9591410\ttotal: 157ms\tremaining: 101ms\n",
      "304:\tlearn: 62.7137415\ttotal: 157ms\tremaining: 101ms\n",
      "305:\tlearn: 62.5316010\ttotal: 158ms\tremaining: 100ms\n",
      "306:\tlearn: 62.3739026\ttotal: 159ms\tremaining: 99.7ms\n",
      "307:\tlearn: 62.0769174\ttotal: 159ms\tremaining: 99.3ms\n",
      "308:\tlearn: 61.9750676\ttotal: 160ms\tremaining: 98.8ms\n",
      "309:\tlearn: 61.8979034\ttotal: 160ms\tremaining: 98.2ms\n",
      "310:\tlearn: 61.7684190\ttotal: 161ms\tremaining: 97.7ms\n",
      "311:\tlearn: 61.6578513\ttotal: 161ms\tremaining: 97.2ms\n",
      "312:\tlearn: 61.3767528\ttotal: 162ms\tremaining: 96.7ms\n",
      "313:\tlearn: 61.1783833\ttotal: 162ms\tremaining: 96.2ms\n",
      "314:\tlearn: 61.0732310\ttotal: 163ms\tremaining: 95.6ms\n",
      "315:\tlearn: 61.0025169\ttotal: 163ms\tremaining: 95.1ms\n",
      "316:\tlearn: 60.7820500\ttotal: 164ms\tremaining: 94.6ms\n",
      "317:\tlearn: 60.6740259\ttotal: 164ms\tremaining: 94ms\n",
      "318:\tlearn: 60.2092818\ttotal: 165ms\tremaining: 93.5ms\n",
      "319:\tlearn: 59.9676607\ttotal: 165ms\tremaining: 92.9ms\n",
      "320:\tlearn: 59.6776466\ttotal: 166ms\tremaining: 92.4ms\n",
      "321:\tlearn: 59.4956730\ttotal: 166ms\tremaining: 91.9ms\n",
      "322:\tlearn: 59.4448335\ttotal: 167ms\tremaining: 91.3ms\n",
      "323:\tlearn: 59.0209835\ttotal: 167ms\tremaining: 90.8ms\n",
      "324:\tlearn: 58.7971974\ttotal: 168ms\tremaining: 90.3ms\n",
      "325:\tlearn: 58.7331483\ttotal: 168ms\tremaining: 89.7ms\n",
      "326:\tlearn: 58.4005758\ttotal: 169ms\tremaining: 89.2ms\n",
      "327:\tlearn: 58.2758467\ttotal: 169ms\tremaining: 88.7ms\n",
      "328:\tlearn: 58.0798282\ttotal: 170ms\tremaining: 88.2ms\n",
      "329:\tlearn: 57.8549200\ttotal: 170ms\tremaining: 87.7ms\n",
      "330:\tlearn: 57.7955768\ttotal: 171ms\tremaining: 87.2ms\n",
      "331:\tlearn: 57.5898722\ttotal: 171ms\tremaining: 86.7ms\n",
      "332:\tlearn: 57.4686106\ttotal: 172ms\tremaining: 86.1ms\n",
      "333:\tlearn: 57.3638604\ttotal: 172ms\tremaining: 85.6ms\n",
      "334:\tlearn: 57.0495438\ttotal: 173ms\tremaining: 85ms\n",
      "335:\tlearn: 56.9822903\ttotal: 173ms\tremaining: 84.5ms\n",
      "336:\tlearn: 56.5452024\ttotal: 174ms\tremaining: 84ms\n",
      "337:\tlearn: 56.3323491\ttotal: 174ms\tremaining: 83.4ms\n",
      "338:\tlearn: 56.0220510\ttotal: 175ms\tremaining: 82.9ms\n",
      "339:\tlearn: 55.7787096\ttotal: 175ms\tremaining: 82.4ms\n",
      "340:\tlearn: 55.6484232\ttotal: 176ms\tremaining: 81.8ms\n",
      "341:\tlearn: 55.4338148\ttotal: 176ms\tremaining: 81.3ms\n",
      "342:\tlearn: 55.0858638\ttotal: 176ms\tremaining: 80.8ms\n",
      "343:\tlearn: 54.9569587\ttotal: 177ms\tremaining: 80.3ms\n",
      "344:\tlearn: 54.7917456\ttotal: 178ms\tremaining: 79.8ms\n",
      "345:\tlearn: 54.4071369\ttotal: 178ms\tremaining: 79.2ms\n",
      "346:\tlearn: 54.3772820\ttotal: 178ms\tremaining: 78.7ms\n",
      "347:\tlearn: 54.3370623\ttotal: 179ms\tremaining: 78.2ms\n",
      "348:\tlearn: 54.3033083\ttotal: 179ms\tremaining: 77.6ms\n",
      "349:\tlearn: 54.1939384\ttotal: 180ms\tremaining: 77.1ms\n",
      "350:\tlearn: 54.1254773\ttotal: 180ms\tremaining: 76.5ms\n",
      "351:\tlearn: 53.8022825\ttotal: 181ms\tremaining: 76ms\n",
      "352:\tlearn: 53.7347188\ttotal: 181ms\tremaining: 75.5ms\n",
      "353:\tlearn: 53.5597464\ttotal: 182ms\tremaining: 75ms\n",
      "354:\tlearn: 53.4728881\ttotal: 182ms\tremaining: 74.4ms\n",
      "355:\tlearn: 53.2070513\ttotal: 183ms\tremaining: 73.9ms\n",
      "356:\tlearn: 53.1435385\ttotal: 183ms\tremaining: 73.4ms\n",
      "357:\tlearn: 52.9032102\ttotal: 184ms\tremaining: 72.9ms\n",
      "358:\tlearn: 52.5974077\ttotal: 184ms\tremaining: 72.3ms\n",
      "359:\tlearn: 52.5077032\ttotal: 185ms\tremaining: 71.8ms\n",
      "360:\tlearn: 52.3581504\ttotal: 185ms\tremaining: 71.3ms\n",
      "361:\tlearn: 52.1542910\ttotal: 186ms\tremaining: 70.8ms\n",
      "362:\tlearn: 52.1279770\ttotal: 186ms\tremaining: 70.3ms\n",
      "363:\tlearn: 51.9011332\ttotal: 187ms\tremaining: 69.7ms\n",
      "364:\tlearn: 51.7253802\ttotal: 187ms\tremaining: 69.2ms\n",
      "365:\tlearn: 51.5737297\ttotal: 188ms\tremaining: 68.7ms\n",
      "366:\tlearn: 51.3991199\ttotal: 188ms\tremaining: 68.1ms\n",
      "367:\tlearn: 51.2136778\ttotal: 189ms\tremaining: 67.6ms\n",
      "368:\tlearn: 50.9116194\ttotal: 189ms\tremaining: 67.1ms\n",
      "369:\tlearn: 50.5812184\ttotal: 189ms\tremaining: 66.6ms\n",
      "370:\tlearn: 50.3851623\ttotal: 190ms\tremaining: 66ms\n",
      "371:\tlearn: 50.3680766\ttotal: 191ms\tremaining: 65.6ms\n",
      "372:\tlearn: 50.2174626\ttotal: 191ms\tremaining: 65ms\n",
      "373:\tlearn: 50.0191275\ttotal: 191ms\tremaining: 64.5ms\n",
      "374:\tlearn: 49.7910846\ttotal: 192ms\tremaining: 64ms\n",
      "375:\tlearn: 49.7742294\ttotal: 192ms\tremaining: 63.4ms\n",
      "376:\tlearn: 49.5195849\ttotal: 193ms\tremaining: 63ms\n",
      "377:\tlearn: 49.3468361\ttotal: 193ms\tremaining: 62.4ms\n",
      "378:\tlearn: 49.1501504\ttotal: 194ms\tremaining: 61.9ms\n",
      "379:\tlearn: 49.1194514\ttotal: 195ms\tremaining: 61.4ms\n",
      "380:\tlearn: 48.7726669\ttotal: 195ms\tremaining: 60.9ms\n",
      "381:\tlearn: 48.5864231\ttotal: 196ms\tremaining: 60.5ms\n",
      "382:\tlearn: 48.2689765\ttotal: 196ms\tremaining: 60ms\n",
      "383:\tlearn: 48.2051342\ttotal: 197ms\tremaining: 59.6ms\n",
      "384:\tlearn: 47.9522633\ttotal: 198ms\tremaining: 59.1ms\n",
      "385:\tlearn: 47.7219703\ttotal: 198ms\tremaining: 58.5ms\n",
      "386:\tlearn: 47.5299709\ttotal: 199ms\tremaining: 58ms\n",
      "387:\tlearn: 47.3351233\ttotal: 199ms\tremaining: 57.5ms\n",
      "388:\tlearn: 47.0934093\ttotal: 200ms\tremaining: 56.9ms\n",
      "389:\tlearn: 46.8149383\ttotal: 200ms\tremaining: 56.4ms\n",
      "390:\tlearn: 46.6212648\ttotal: 200ms\tremaining: 55.9ms\n",
      "391:\tlearn: 46.5770370\ttotal: 201ms\tremaining: 55.4ms\n",
      "392:\tlearn: 46.3118327\ttotal: 201ms\tremaining: 54.8ms\n",
      "393:\tlearn: 46.1542966\ttotal: 202ms\tremaining: 54.3ms\n",
      "394:\tlearn: 45.9975827\ttotal: 202ms\tremaining: 53.8ms\n",
      "395:\tlearn: 45.8583890\ttotal: 203ms\tremaining: 53.2ms\n",
      "396:\tlearn: 45.7478313\ttotal: 203ms\tremaining: 52.7ms\n",
      "397:\tlearn: 45.4602926\ttotal: 204ms\tremaining: 52.2ms\n",
      "398:\tlearn: 45.3934359\ttotal: 204ms\tremaining: 51.7ms\n",
      "399:\tlearn: 45.2550581\ttotal: 205ms\tremaining: 51.1ms\n",
      "400:\tlearn: 45.2412958\ttotal: 205ms\tremaining: 50.6ms\n",
      "401:\tlearn: 45.0844973\ttotal: 206ms\tremaining: 50.1ms\n",
      "402:\tlearn: 45.0278248\ttotal: 206ms\tremaining: 49.6ms\n",
      "403:\tlearn: 44.9089215\ttotal: 207ms\tremaining: 49.1ms\n",
      "404:\tlearn: 44.7926454\ttotal: 207ms\tremaining: 48.6ms\n",
      "405:\tlearn: 44.6260040\ttotal: 208ms\tremaining: 48ms\n",
      "406:\tlearn: 44.4428807\ttotal: 208ms\tremaining: 47.5ms\n",
      "407:\tlearn: 44.3463031\ttotal: 208ms\tremaining: 47ms\n",
      "408:\tlearn: 44.1955654\ttotal: 209ms\tremaining: 46.5ms\n",
      "409:\tlearn: 44.1292127\ttotal: 209ms\tremaining: 46ms\n",
      "410:\tlearn: 43.8504857\ttotal: 210ms\tremaining: 45.4ms\n",
      "411:\tlearn: 43.7811415\ttotal: 210ms\tremaining: 44.9ms\n",
      "412:\tlearn: 43.7110419\ttotal: 211ms\tremaining: 44.4ms\n",
      "413:\tlearn: 43.3414580\ttotal: 211ms\tremaining: 43.9ms\n",
      "414:\tlearn: 43.2247379\ttotal: 212ms\tremaining: 43.3ms\n",
      "415:\tlearn: 43.1877321\ttotal: 212ms\tremaining: 42.8ms\n",
      "416:\tlearn: 43.0227168\ttotal: 212ms\tremaining: 42.3ms\n",
      "417:\tlearn: 42.9704490\ttotal: 213ms\tremaining: 41.8ms\n",
      "418:\tlearn: 42.6970878\ttotal: 213ms\tremaining: 41.2ms\n",
      "419:\tlearn: 42.5722431\ttotal: 214ms\tremaining: 40.7ms\n",
      "420:\tlearn: 42.3328395\ttotal: 214ms\tremaining: 40.2ms\n",
      "421:\tlearn: 42.2273209\ttotal: 215ms\tremaining: 39.7ms\n",
      "422:\tlearn: 42.1223611\ttotal: 215ms\tremaining: 39.2ms\n",
      "423:\tlearn: 42.0050276\ttotal: 216ms\tremaining: 38.6ms\n",
      "424:\tlearn: 41.8083350\ttotal: 216ms\tremaining: 38.1ms\n",
      "425:\tlearn: 41.7120403\ttotal: 217ms\tremaining: 37.6ms\n",
      "426:\tlearn: 41.6577438\ttotal: 217ms\tremaining: 37.1ms\n",
      "427:\tlearn: 41.4851951\ttotal: 218ms\tremaining: 36.6ms\n",
      "428:\tlearn: 41.2854259\ttotal: 218ms\tremaining: 36.1ms\n",
      "429:\tlearn: 41.0273750\ttotal: 218ms\tremaining: 35.6ms\n",
      "430:\tlearn: 40.9686735\ttotal: 219ms\tremaining: 35ms\n",
      "431:\tlearn: 40.9166912\ttotal: 219ms\tremaining: 34.5ms\n",
      "432:\tlearn: 40.7316158\ttotal: 220ms\tremaining: 34ms\n",
      "433:\tlearn: 40.6026013\ttotal: 220ms\tremaining: 33.5ms\n",
      "434:\tlearn: 40.4972825\ttotal: 221ms\tremaining: 33ms\n",
      "435:\tlearn: 40.3560751\ttotal: 221ms\tremaining: 32.5ms\n",
      "436:\tlearn: 40.2746545\ttotal: 222ms\tremaining: 32ms\n",
      "437:\tlearn: 40.0657619\ttotal: 222ms\tremaining: 31.4ms\n",
      "438:\tlearn: 39.7902684\ttotal: 223ms\tremaining: 30.9ms\n",
      "439:\tlearn: 39.6444690\ttotal: 223ms\tremaining: 30.4ms\n",
      "440:\tlearn: 39.4605197\ttotal: 223ms\tremaining: 29.9ms\n",
      "441:\tlearn: 39.1422333\ttotal: 224ms\tremaining: 29.4ms\n",
      "442:\tlearn: 38.9665502\ttotal: 224ms\tremaining: 28.9ms\n",
      "443:\tlearn: 38.7832843\ttotal: 225ms\tremaining: 28.3ms\n",
      "444:\tlearn: 38.7390927\ttotal: 225ms\tremaining: 27.8ms\n",
      "445:\tlearn: 38.6860965\ttotal: 226ms\tremaining: 27.3ms\n",
      "446:\tlearn: 38.5844063\ttotal: 226ms\tremaining: 26.8ms\n",
      "447:\tlearn: 38.5389111\ttotal: 227ms\tremaining: 26.3ms\n",
      "448:\tlearn: 38.3759944\ttotal: 227ms\tremaining: 25.8ms\n",
      "449:\tlearn: 38.1665760\ttotal: 228ms\tremaining: 25.3ms\n",
      "450:\tlearn: 37.9451332\ttotal: 228ms\tremaining: 24.8ms\n",
      "451:\tlearn: 37.8993310\ttotal: 228ms\tremaining: 24.3ms\n",
      "452:\tlearn: 37.6861011\ttotal: 229ms\tremaining: 23.7ms\n",
      "453:\tlearn: 37.6039560\ttotal: 229ms\tremaining: 23.2ms\n",
      "454:\tlearn: 37.5653560\ttotal: 230ms\tremaining: 22.7ms\n",
      "455:\tlearn: 37.4697682\ttotal: 230ms\tremaining: 22.2ms\n",
      "456:\tlearn: 37.3055112\ttotal: 231ms\tremaining: 21.7ms\n",
      "457:\tlearn: 37.2268878\ttotal: 231ms\tremaining: 21.2ms\n",
      "458:\tlearn: 37.1198921\ttotal: 232ms\tremaining: 20.7ms\n",
      "459:\tlearn: 36.9202265\ttotal: 232ms\tremaining: 20.2ms\n",
      "460:\tlearn: 36.7772486\ttotal: 233ms\tremaining: 19.7ms\n",
      "461:\tlearn: 36.7110138\ttotal: 233ms\tremaining: 19.2ms\n",
      "462:\tlearn: 36.5340518\ttotal: 233ms\tremaining: 18.7ms\n",
      "463:\tlearn: 36.4899969\ttotal: 234ms\tremaining: 18.1ms\n",
      "464:\tlearn: 36.4473891\ttotal: 234ms\tremaining: 17.6ms\n",
      "465:\tlearn: 36.3373210\ttotal: 235ms\tremaining: 17.1ms\n",
      "466:\tlearn: 36.2492890\ttotal: 235ms\tremaining: 16.6ms\n",
      "467:\tlearn: 36.0007624\ttotal: 236ms\tremaining: 16.1ms\n",
      "468:\tlearn: 35.8370356\ttotal: 236ms\tremaining: 15.6ms\n",
      "469:\tlearn: 35.6884024\ttotal: 237ms\tremaining: 15.1ms\n",
      "470:\tlearn: 35.5418431\ttotal: 237ms\tremaining: 14.6ms\n",
      "471:\tlearn: 35.5139923\ttotal: 238ms\tremaining: 14.1ms\n",
      "472:\tlearn: 35.2819908\ttotal: 238ms\tremaining: 13.6ms\n",
      "473:\tlearn: 35.1124281\ttotal: 238ms\tremaining: 13.1ms\n",
      "474:\tlearn: 35.0019355\ttotal: 239ms\tremaining: 12.6ms\n",
      "475:\tlearn: 34.9665518\ttotal: 239ms\tremaining: 12.1ms\n",
      "476:\tlearn: 34.9310456\ttotal: 240ms\tremaining: 11.6ms\n",
      "477:\tlearn: 34.8000619\ttotal: 240ms\tremaining: 11.1ms\n",
      "478:\tlearn: 34.7113987\ttotal: 241ms\tremaining: 10.6ms\n",
      "479:\tlearn: 34.6544807\ttotal: 241ms\tremaining: 10ms\n",
      "480:\tlearn: 34.5276276\ttotal: 242ms\tremaining: 9.54ms\n",
      "481:\tlearn: 34.4828207\ttotal: 242ms\tremaining: 9.04ms\n",
      "482:\tlearn: 34.3210887\ttotal: 242ms\tremaining: 8.53ms\n",
      "483:\tlearn: 34.2394371\ttotal: 243ms\tremaining: 8.03ms\n",
      "484:\tlearn: 34.0874075\ttotal: 243ms\tremaining: 7.53ms\n",
      "485:\tlearn: 34.0613122\ttotal: 244ms\tremaining: 7.02ms\n",
      "486:\tlearn: 33.9974089\ttotal: 244ms\tremaining: 6.52ms\n",
      "487:\tlearn: 33.9541372\ttotal: 245ms\tremaining: 6.02ms\n",
      "488:\tlearn: 33.9290336\ttotal: 245ms\tremaining: 5.51ms\n",
      "489:\tlearn: 33.9026997\ttotal: 246ms\tremaining: 5.01ms\n",
      "490:\tlearn: 33.6838907\ttotal: 246ms\tremaining: 4.51ms\n",
      "491:\tlearn: 33.5171456\ttotal: 247ms\tremaining: 4.01ms\n",
      "492:\tlearn: 33.3789656\ttotal: 247ms\tremaining: 3.51ms\n",
      "493:\tlearn: 33.1952735\ttotal: 247ms\tremaining: 3ms\n",
      "494:\tlearn: 33.1622498\ttotal: 248ms\tremaining: 2.5ms\n",
      "495:\tlearn: 33.0263908\ttotal: 248ms\tremaining: 2ms\n",
      "496:\tlearn: 32.9513108\ttotal: 249ms\tremaining: 1.5ms\n",
      "497:\tlearn: 32.8539205\ttotal: 249ms\tremaining: 1ms\n",
      "498:\tlearn: 32.8194421\ttotal: 250ms\tremaining: 500us\n",
      "499:\tlearn: 32.7695204\ttotal: 250ms\tremaining: 0us\n",
      "0:\tlearn: 458.4632952\ttotal: 1.24ms\tremaining: 620ms\n",
      "1:\tlearn: 439.1158680\ttotal: 2.62ms\tremaining: 652ms\n",
      "2:\tlearn: 421.8718599\ttotal: 3.85ms\tremaining: 637ms\n",
      "3:\tlearn: 406.8068268\ttotal: 5.02ms\tremaining: 623ms\n",
      "4:\tlearn: 391.1204598\ttotal: 6.17ms\tremaining: 611ms\n",
      "5:\tlearn: 381.8107190\ttotal: 7.37ms\tremaining: 607ms\n",
      "6:\tlearn: 370.0274807\ttotal: 8.57ms\tremaining: 604ms\n",
      "7:\tlearn: 358.7262922\ttotal: 9.78ms\tremaining: 601ms\n",
      "8:\tlearn: 348.5480164\ttotal: 10.9ms\tremaining: 596ms\n",
      "9:\tlearn: 338.6785326\ttotal: 12.1ms\tremaining: 593ms\n",
      "10:\tlearn: 330.6497064\ttotal: 13.3ms\tremaining: 591ms\n",
      "11:\tlearn: 326.1322469\ttotal: 14.5ms\tremaining: 590ms\n",
      "12:\tlearn: 319.6098815\ttotal: 15.8ms\tremaining: 591ms\n",
      "13:\tlearn: 315.2128779\ttotal: 16.9ms\tremaining: 587ms\n",
      "14:\tlearn: 310.7437324\ttotal: 18.1ms\tremaining: 584ms\n",
      "15:\tlearn: 304.3962710\ttotal: 19.3ms\tremaining: 583ms\n",
      "16:\tlearn: 299.6595157\ttotal: 20.4ms\tremaining: 580ms\n",
      "17:\tlearn: 295.4833647\ttotal: 21.2ms\tremaining: 568ms\n",
      "18:\tlearn: 292.0286526\ttotal: 21.9ms\tremaining: 554ms\n",
      "19:\tlearn: 286.9359339\ttotal: 22.5ms\tremaining: 541ms\n",
      "20:\tlearn: 283.9572393\ttotal: 23.2ms\tremaining: 528ms\n",
      "21:\tlearn: 282.1883106\ttotal: 23.7ms\tremaining: 515ms\n",
      "22:\tlearn: 276.2922708\ttotal: 24.3ms\tremaining: 504ms\n",
      "23:\tlearn: 272.9955999\ttotal: 24.9ms\tremaining: 494ms\n",
      "24:\tlearn: 269.0677373\ttotal: 25.6ms\tremaining: 486ms\n",
      "25:\tlearn: 264.6011235\ttotal: 26.1ms\tremaining: 476ms\n",
      "26:\tlearn: 261.2525266\ttotal: 26.7ms\tremaining: 468ms\n",
      "27:\tlearn: 258.3172867\ttotal: 27.3ms\tremaining: 461ms\n",
      "28:\tlearn: 257.3931227\ttotal: 27.9ms\tremaining: 453ms\n",
      "29:\tlearn: 254.3587319\ttotal: 28.4ms\tremaining: 445ms\n",
      "30:\tlearn: 253.1187064\ttotal: 29.1ms\tremaining: 440ms\n",
      "31:\tlearn: 250.1605144\ttotal: 29.6ms\tremaining: 433ms\n",
      "32:\tlearn: 246.9878250\ttotal: 30.1ms\tremaining: 426ms\n",
      "33:\tlearn: 244.1152284\ttotal: 30.6ms\tremaining: 419ms\n",
      "34:\tlearn: 240.9548489\ttotal: 31.1ms\tremaining: 413ms\n",
      "35:\tlearn: 238.4775299\ttotal: 31.6ms\tremaining: 407ms\n",
      "36:\tlearn: 237.7583773\ttotal: 32.1ms\tremaining: 401ms\n",
      "37:\tlearn: 235.9677358\ttotal: 32.5ms\tremaining: 396ms\n",
      "38:\tlearn: 234.8607233\ttotal: 33ms\tremaining: 390ms\n",
      "39:\tlearn: 230.1758898\ttotal: 33.5ms\tremaining: 385ms\n",
      "40:\tlearn: 226.6629182\ttotal: 34ms\tremaining: 381ms\n",
      "41:\tlearn: 224.5708957\ttotal: 34.5ms\tremaining: 376ms\n",
      "42:\tlearn: 222.3393611\ttotal: 35ms\tremaining: 372ms\n",
      "43:\tlearn: 219.1627890\ttotal: 35.5ms\tremaining: 368ms\n",
      "44:\tlearn: 218.3430285\ttotal: 36ms\tremaining: 364ms\n",
      "45:\tlearn: 217.7979747\ttotal: 36.5ms\tremaining: 360ms\n",
      "46:\tlearn: 216.9993590\ttotal: 37ms\tremaining: 356ms\n",
      "47:\tlearn: 216.3126751\ttotal: 37.4ms\tremaining: 352ms\n",
      "48:\tlearn: 214.3483764\ttotal: 37.9ms\tremaining: 349ms\n",
      "49:\tlearn: 211.4449372\ttotal: 38.4ms\tremaining: 346ms\n",
      "50:\tlearn: 210.7276313\ttotal: 38.9ms\tremaining: 343ms\n",
      "51:\tlearn: 210.0977608\ttotal: 39.4ms\tremaining: 339ms\n",
      "52:\tlearn: 208.3644692\ttotal: 39.9ms\tremaining: 336ms\n",
      "53:\tlearn: 207.6731175\ttotal: 40.4ms\tremaining: 333ms\n",
      "54:\tlearn: 206.6803761\ttotal: 40.8ms\tremaining: 330ms\n",
      "55:\tlearn: 203.7193833\ttotal: 41.3ms\tremaining: 327ms\n",
      "56:\tlearn: 200.5401230\ttotal: 41.8ms\tremaining: 325ms\n",
      "57:\tlearn: 199.9186526\ttotal: 42.2ms\tremaining: 322ms\n",
      "58:\tlearn: 197.4586309\ttotal: 42.7ms\tremaining: 319ms\n",
      "59:\tlearn: 196.9141833\ttotal: 43.2ms\tremaining: 317ms\n",
      "60:\tlearn: 194.5755964\ttotal: 43.7ms\tremaining: 314ms\n",
      "61:\tlearn: 193.8772810\ttotal: 44.1ms\tremaining: 312ms\n",
      "62:\tlearn: 193.3528538\ttotal: 44.6ms\tremaining: 310ms\n",
      "63:\tlearn: 191.8727274\ttotal: 45.1ms\tremaining: 307ms\n",
      "64:\tlearn: 191.3923695\ttotal: 45.6ms\tremaining: 305ms\n",
      "65:\tlearn: 189.3688797\ttotal: 46.1ms\tremaining: 303ms\n",
      "66:\tlearn: 187.0491869\ttotal: 46.5ms\tremaining: 301ms\n",
      "67:\tlearn: 186.5255017\ttotal: 47ms\tremaining: 299ms\n",
      "68:\tlearn: 186.0309669\ttotal: 47.5ms\tremaining: 297ms\n",
      "69:\tlearn: 185.4741789\ttotal: 48ms\tremaining: 295ms\n",
      "70:\tlearn: 182.9355077\ttotal: 48.4ms\tremaining: 293ms\n",
      "71:\tlearn: 182.4114459\ttotal: 48.9ms\tremaining: 291ms\n",
      "72:\tlearn: 182.0401402\ttotal: 49.4ms\tremaining: 289ms\n",
      "73:\tlearn: 179.8551994\ttotal: 49.8ms\tremaining: 287ms\n",
      "74:\tlearn: 178.3608256\ttotal: 50.3ms\tremaining: 285ms\n",
      "75:\tlearn: 176.8384550\ttotal: 50.8ms\tremaining: 284ms\n",
      "76:\tlearn: 175.6137364\ttotal: 51.3ms\tremaining: 282ms\n",
      "77:\tlearn: 175.2290115\ttotal: 51.7ms\tremaining: 280ms\n",
      "78:\tlearn: 174.8636660\ttotal: 52.2ms\tremaining: 278ms\n",
      "79:\tlearn: 172.4990113\ttotal: 52.7ms\tremaining: 277ms\n",
      "80:\tlearn: 172.0945311\ttotal: 53.1ms\tremaining: 275ms\n",
      "81:\tlearn: 171.7615625\ttotal: 53.6ms\tremaining: 273ms\n",
      "82:\tlearn: 171.4452463\ttotal: 54ms\tremaining: 271ms\n",
      "83:\tlearn: 169.7947494\ttotal: 54.5ms\tremaining: 270ms\n",
      "84:\tlearn: 169.4322881\ttotal: 55ms\tremaining: 269ms\n",
      "85:\tlearn: 169.1366194\ttotal: 55.5ms\tremaining: 267ms\n",
      "86:\tlearn: 168.4555028\ttotal: 55.9ms\tremaining: 265ms\n",
      "87:\tlearn: 167.0784374\ttotal: 56.4ms\tremaining: 264ms\n",
      "88:\tlearn: 166.8063716\ttotal: 56.8ms\tremaining: 262ms\n",
      "89:\tlearn: 164.6646024\ttotal: 57.3ms\tremaining: 261ms\n",
      "90:\tlearn: 164.4186839\ttotal: 57.7ms\tremaining: 260ms\n",
      "91:\tlearn: 163.8792184\ttotal: 58.2ms\tremaining: 258ms\n",
      "92:\tlearn: 162.8086346\ttotal: 58.7ms\tremaining: 257ms\n",
      "93:\tlearn: 160.5035254\ttotal: 59.1ms\tremaining: 255ms\n",
      "94:\tlearn: 159.0582987\ttotal: 59.6ms\tremaining: 254ms\n",
      "95:\tlearn: 157.9880138\ttotal: 60ms\tremaining: 253ms\n",
      "96:\tlearn: 157.0573934\ttotal: 60.5ms\tremaining: 251ms\n",
      "97:\tlearn: 156.6835352\ttotal: 60.9ms\tremaining: 250ms\n",
      "98:\tlearn: 155.7240986\ttotal: 61.4ms\tremaining: 249ms\n",
      "99:\tlearn: 155.4951899\ttotal: 61.8ms\tremaining: 247ms\n",
      "100:\tlearn: 154.3880679\ttotal: 62.3ms\tremaining: 246ms\n",
      "101:\tlearn: 153.4872645\ttotal: 62.7ms\tremaining: 245ms\n",
      "102:\tlearn: 152.9489753\ttotal: 63.2ms\tremaining: 244ms\n",
      "103:\tlearn: 151.0838349\ttotal: 63.6ms\tremaining: 242ms\n",
      "104:\tlearn: 150.2453783\ttotal: 64.2ms\tremaining: 241ms\n",
      "105:\tlearn: 149.2158762\ttotal: 64.6ms\tremaining: 240ms\n",
      "106:\tlearn: 148.3354199\ttotal: 65.1ms\tremaining: 239ms\n",
      "107:\tlearn: 147.5948534\ttotal: 65.5ms\tremaining: 238ms\n",
      "108:\tlearn: 146.0391470\ttotal: 66ms\tremaining: 237ms\n",
      "109:\tlearn: 145.3532434\ttotal: 66.5ms\tremaining: 236ms\n",
      "110:\tlearn: 144.7260629\ttotal: 66.9ms\tremaining: 235ms\n",
      "111:\tlearn: 143.6806013\ttotal: 67.4ms\tremaining: 233ms\n",
      "112:\tlearn: 142.7871037\ttotal: 67.9ms\tremaining: 232ms\n",
      "113:\tlearn: 141.4071481\ttotal: 68.3ms\tremaining: 231ms\n",
      "114:\tlearn: 140.6782701\ttotal: 68.8ms\tremaining: 230ms\n",
      "115:\tlearn: 139.7106755\ttotal: 69.3ms\tremaining: 229ms\n",
      "116:\tlearn: 138.8183202\ttotal: 69.7ms\tremaining: 228ms\n",
      "117:\tlearn: 137.6478223\ttotal: 70.2ms\tremaining: 227ms\n",
      "118:\tlearn: 137.0545031\ttotal: 70.7ms\tremaining: 226ms\n",
      "119:\tlearn: 136.7299885\ttotal: 71.1ms\tremaining: 225ms\n",
      "120:\tlearn: 136.3853366\ttotal: 71.6ms\tremaining: 224ms\n",
      "121:\tlearn: 135.9390981\ttotal: 72ms\tremaining: 223ms\n",
      "122:\tlearn: 135.3363775\ttotal: 72.5ms\tremaining: 222ms\n",
      "123:\tlearn: 135.1863620\ttotal: 72.9ms\tremaining: 221ms\n",
      "124:\tlearn: 135.0448037\ttotal: 73.4ms\tremaining: 220ms\n",
      "125:\tlearn: 134.3401928\ttotal: 73.8ms\tremaining: 219ms\n",
      "126:\tlearn: 134.0060916\ttotal: 74.3ms\tremaining: 218ms\n",
      "127:\tlearn: 133.3161414\ttotal: 74.7ms\tremaining: 217ms\n",
      "128:\tlearn: 132.1048227\ttotal: 75.2ms\tremaining: 216ms\n",
      "129:\tlearn: 130.3599593\ttotal: 75.6ms\tremaining: 215ms\n",
      "130:\tlearn: 129.2619841\ttotal: 76.1ms\tremaining: 214ms\n",
      "131:\tlearn: 128.9862415\ttotal: 76.6ms\tremaining: 213ms\n",
      "132:\tlearn: 128.1331931\ttotal: 77ms\tremaining: 213ms\n",
      "133:\tlearn: 127.3969328\ttotal: 77.5ms\tremaining: 212ms\n",
      "134:\tlearn: 126.8411727\ttotal: 77.9ms\tremaining: 211ms\n",
      "135:\tlearn: 126.3974574\ttotal: 78.5ms\tremaining: 210ms\n",
      "136:\tlearn: 125.8068414\ttotal: 78.9ms\tremaining: 209ms\n",
      "137:\tlearn: 124.8260231\ttotal: 79.4ms\tremaining: 208ms\n",
      "138:\tlearn: 123.7889993\ttotal: 79.8ms\tremaining: 207ms\n",
      "139:\tlearn: 123.0368640\ttotal: 80.3ms\tremaining: 206ms\n",
      "140:\tlearn: 122.8817539\ttotal: 80.7ms\tremaining: 206ms\n",
      "141:\tlearn: 122.0488603\ttotal: 81.2ms\tremaining: 205ms\n",
      "142:\tlearn: 121.3541433\ttotal: 81.6ms\tremaining: 204ms\n",
      "143:\tlearn: 120.9445291\ttotal: 82.1ms\tremaining: 203ms\n",
      "144:\tlearn: 120.4189590\ttotal: 82.5ms\tremaining: 202ms\n",
      "145:\tlearn: 120.1997253\ttotal: 83ms\tremaining: 201ms\n",
      "146:\tlearn: 120.0947121\ttotal: 83.5ms\tremaining: 201ms\n",
      "147:\tlearn: 119.9690680\ttotal: 83.9ms\tremaining: 200ms\n",
      "148:\tlearn: 119.3315711\ttotal: 84.4ms\tremaining: 199ms\n",
      "149:\tlearn: 119.2149248\ttotal: 84.8ms\tremaining: 198ms\n",
      "150:\tlearn: 118.6975708\ttotal: 85.3ms\tremaining: 197ms\n",
      "151:\tlearn: 118.5053099\ttotal: 85.7ms\tremaining: 196ms\n",
      "152:\tlearn: 117.4431955\ttotal: 86.2ms\tremaining: 195ms\n",
      "153:\tlearn: 117.1562845\ttotal: 86.6ms\tremaining: 195ms\n",
      "154:\tlearn: 116.5208908\ttotal: 87.1ms\tremaining: 194ms\n",
      "155:\tlearn: 115.8211733\ttotal: 87.5ms\tremaining: 193ms\n",
      "156:\tlearn: 114.8910748\ttotal: 88ms\tremaining: 192ms\n",
      "157:\tlearn: 113.9673548\ttotal: 88.4ms\tremaining: 191ms\n",
      "158:\tlearn: 113.7184395\ttotal: 88.9ms\tremaining: 191ms\n",
      "159:\tlearn: 112.9662222\ttotal: 89.3ms\tremaining: 190ms\n",
      "160:\tlearn: 112.5101529\ttotal: 89.8ms\tremaining: 189ms\n",
      "161:\tlearn: 111.5723935\ttotal: 90.3ms\tremaining: 188ms\n",
      "162:\tlearn: 111.0396684\ttotal: 90.7ms\tremaining: 188ms\n",
      "163:\tlearn: 110.4398177\ttotal: 91.2ms\tremaining: 187ms\n",
      "164:\tlearn: 110.0259931\ttotal: 91.6ms\tremaining: 186ms\n",
      "165:\tlearn: 109.3093251\ttotal: 92.1ms\tremaining: 185ms\n",
      "166:\tlearn: 108.7588589\ttotal: 92.5ms\tremaining: 184ms\n",
      "167:\tlearn: 108.1911397\ttotal: 93ms\tremaining: 184ms\n",
      "168:\tlearn: 108.0176156\ttotal: 93.4ms\tremaining: 183ms\n",
      "169:\tlearn: 107.0154187\ttotal: 93.9ms\tremaining: 182ms\n",
      "170:\tlearn: 106.5664542\ttotal: 94.3ms\tremaining: 182ms\n",
      "171:\tlearn: 106.2406968\ttotal: 94.8ms\tremaining: 181ms\n",
      "172:\tlearn: 105.6068920\ttotal: 95.3ms\tremaining: 180ms\n",
      "173:\tlearn: 105.4452553\ttotal: 95.8ms\tremaining: 179ms\n",
      "174:\tlearn: 105.1613691\ttotal: 96.3ms\tremaining: 179ms\n",
      "175:\tlearn: 104.5269850\ttotal: 96.7ms\tremaining: 178ms\n",
      "176:\tlearn: 104.3198336\ttotal: 97.2ms\tremaining: 177ms\n",
      "177:\tlearn: 103.9791706\ttotal: 97.6ms\tremaining: 177ms\n",
      "178:\tlearn: 103.2038043\ttotal: 98.1ms\tremaining: 176ms\n",
      "179:\tlearn: 102.9116795\ttotal: 98.5ms\tremaining: 175ms\n",
      "180:\tlearn: 102.6969663\ttotal: 99ms\tremaining: 174ms\n",
      "181:\tlearn: 102.4951361\ttotal: 99.4ms\tremaining: 174ms\n",
      "182:\tlearn: 102.1700597\ttotal: 99.9ms\tremaining: 173ms\n",
      "183:\tlearn: 102.0186047\ttotal: 100ms\tremaining: 172ms\n",
      "184:\tlearn: 101.0570342\ttotal: 101ms\tremaining: 172ms\n",
      "185:\tlearn: 100.1798718\ttotal: 101ms\tremaining: 171ms\n",
      "186:\tlearn: 99.8458834\ttotal: 102ms\tremaining: 170ms\n",
      "187:\tlearn: 99.5344789\ttotal: 102ms\tremaining: 169ms\n",
      "188:\tlearn: 98.9230829\ttotal: 103ms\tremaining: 169ms\n",
      "189:\tlearn: 98.1204824\ttotal: 103ms\tremaining: 168ms\n",
      "190:\tlearn: 97.5657519\ttotal: 103ms\tremaining: 167ms\n",
      "191:\tlearn: 96.3654910\ttotal: 104ms\tremaining: 167ms\n",
      "192:\tlearn: 95.8819824\ttotal: 104ms\tremaining: 166ms\n",
      "193:\tlearn: 95.5524035\ttotal: 105ms\tremaining: 165ms\n",
      "194:\tlearn: 94.9081376\ttotal: 105ms\tremaining: 165ms\n",
      "195:\tlearn: 94.6932813\ttotal: 106ms\tremaining: 164ms\n",
      "196:\tlearn: 94.2848928\ttotal: 106ms\tremaining: 163ms\n",
      "197:\tlearn: 93.5710330\ttotal: 107ms\tremaining: 163ms\n",
      "198:\tlearn: 93.4413212\ttotal: 107ms\tremaining: 162ms\n",
      "199:\tlearn: 92.8625251\ttotal: 108ms\tremaining: 161ms\n",
      "200:\tlearn: 92.3477410\ttotal: 108ms\tremaining: 161ms\n",
      "201:\tlearn: 91.9088113\ttotal: 109ms\tremaining: 160ms\n",
      "202:\tlearn: 91.7117673\ttotal: 109ms\tremaining: 159ms\n",
      "203:\tlearn: 91.6546543\ttotal: 109ms\tremaining: 159ms\n",
      "204:\tlearn: 90.9670266\ttotal: 110ms\tremaining: 158ms\n",
      "205:\tlearn: 90.4584059\ttotal: 110ms\tremaining: 158ms\n",
      "206:\tlearn: 89.9336188\ttotal: 111ms\tremaining: 157ms\n",
      "207:\tlearn: 89.2158358\ttotal: 111ms\tremaining: 156ms\n",
      "208:\tlearn: 88.4567817\ttotal: 112ms\tremaining: 156ms\n",
      "209:\tlearn: 88.3339772\ttotal: 112ms\tremaining: 155ms\n",
      "210:\tlearn: 88.2739231\ttotal: 113ms\tremaining: 154ms\n",
      "211:\tlearn: 87.8548698\ttotal: 113ms\tremaining: 154ms\n",
      "212:\tlearn: 87.7285992\ttotal: 114ms\tremaining: 153ms\n",
      "213:\tlearn: 87.6810489\ttotal: 114ms\tremaining: 152ms\n",
      "214:\tlearn: 87.4515552\ttotal: 114ms\tremaining: 152ms\n",
      "215:\tlearn: 87.4013250\ttotal: 115ms\tremaining: 151ms\n",
      "216:\tlearn: 87.0112982\ttotal: 115ms\tremaining: 150ms\n",
      "217:\tlearn: 86.9605679\ttotal: 116ms\tremaining: 150ms\n",
      "218:\tlearn: 86.9160818\ttotal: 116ms\tremaining: 149ms\n",
      "219:\tlearn: 86.7228875\ttotal: 117ms\tremaining: 149ms\n",
      "220:\tlearn: 86.2883163\ttotal: 117ms\tremaining: 148ms\n",
      "221:\tlearn: 86.0684751\ttotal: 118ms\tremaining: 147ms\n",
      "222:\tlearn: 85.6076700\ttotal: 118ms\tremaining: 147ms\n",
      "223:\tlearn: 85.4175574\ttotal: 119ms\tremaining: 146ms\n",
      "224:\tlearn: 84.6495993\ttotal: 119ms\tremaining: 145ms\n",
      "225:\tlearn: 84.4006938\ttotal: 119ms\tremaining: 145ms\n",
      "226:\tlearn: 84.0155095\ttotal: 120ms\tremaining: 144ms\n",
      "227:\tlearn: 83.7954465\ttotal: 120ms\tremaining: 144ms\n",
      "228:\tlearn: 82.9415089\ttotal: 121ms\tremaining: 143ms\n",
      "229:\tlearn: 82.9006422\ttotal: 121ms\tremaining: 142ms\n",
      "230:\tlearn: 82.4163896\ttotal: 122ms\tremaining: 142ms\n",
      "231:\tlearn: 81.8944316\ttotal: 122ms\tremaining: 141ms\n",
      "232:\tlearn: 81.7838096\ttotal: 123ms\tremaining: 140ms\n",
      "233:\tlearn: 81.7473185\ttotal: 123ms\tremaining: 140ms\n",
      "234:\tlearn: 81.5606813\ttotal: 124ms\tremaining: 139ms\n",
      "235:\tlearn: 81.2471221\ttotal: 124ms\tremaining: 139ms\n",
      "236:\tlearn: 81.2067687\ttotal: 124ms\tremaining: 138ms\n",
      "237:\tlearn: 80.9411812\ttotal: 125ms\tremaining: 138ms\n",
      "238:\tlearn: 80.7617620\ttotal: 125ms\tremaining: 137ms\n",
      "239:\tlearn: 80.3677473\ttotal: 126ms\tremaining: 136ms\n",
      "240:\tlearn: 80.3179806\ttotal: 126ms\tremaining: 136ms\n",
      "241:\tlearn: 80.2146612\ttotal: 127ms\tremaining: 135ms\n",
      "242:\tlearn: 79.9240955\ttotal: 127ms\tremaining: 135ms\n",
      "243:\tlearn: 79.6209806\ttotal: 128ms\tremaining: 134ms\n",
      "244:\tlearn: 79.2422376\ttotal: 128ms\tremaining: 133ms\n",
      "245:\tlearn: 79.1318921\ttotal: 129ms\tremaining: 133ms\n",
      "246:\tlearn: 79.0104427\ttotal: 129ms\tremaining: 132ms\n",
      "247:\tlearn: 78.8891208\ttotal: 129ms\tremaining: 132ms\n",
      "248:\tlearn: 78.8535285\ttotal: 130ms\tremaining: 131ms\n",
      "249:\tlearn: 78.4941099\ttotal: 131ms\tremaining: 131ms\n",
      "250:\tlearn: 77.9110594\ttotal: 131ms\tremaining: 130ms\n",
      "251:\tlearn: 77.7448974\ttotal: 132ms\tremaining: 130ms\n",
      "252:\tlearn: 77.5979549\ttotal: 132ms\tremaining: 129ms\n",
      "253:\tlearn: 77.1630690\ttotal: 133ms\tremaining: 129ms\n",
      "254:\tlearn: 77.1140131\ttotal: 133ms\tremaining: 128ms\n",
      "255:\tlearn: 76.5408998\ttotal: 134ms\tremaining: 128ms\n",
      "256:\tlearn: 76.0138469\ttotal: 134ms\tremaining: 127ms\n",
      "257:\tlearn: 75.5860669\ttotal: 135ms\tremaining: 126ms\n",
      "258:\tlearn: 75.2657650\ttotal: 135ms\tremaining: 126ms\n",
      "259:\tlearn: 75.0769658\ttotal: 136ms\tremaining: 125ms\n",
      "260:\tlearn: 74.5570164\ttotal: 136ms\tremaining: 125ms\n",
      "261:\tlearn: 74.0822122\ttotal: 137ms\tremaining: 124ms\n",
      "262:\tlearn: 73.8830700\ttotal: 137ms\tremaining: 124ms\n",
      "263:\tlearn: 73.7902500\ttotal: 138ms\tremaining: 123ms\n",
      "264:\tlearn: 73.6621953\ttotal: 138ms\tremaining: 123ms\n",
      "265:\tlearn: 73.4993223\ttotal: 139ms\tremaining: 122ms\n",
      "266:\tlearn: 73.2474944\ttotal: 139ms\tremaining: 122ms\n",
      "267:\tlearn: 73.1089128\ttotal: 140ms\tremaining: 121ms\n",
      "268:\tlearn: 72.4129261\ttotal: 141ms\tremaining: 121ms\n",
      "269:\tlearn: 72.3126234\ttotal: 141ms\tremaining: 120ms\n",
      "270:\tlearn: 71.9415310\ttotal: 142ms\tremaining: 120ms\n",
      "271:\tlearn: 71.4544532\ttotal: 142ms\tremaining: 119ms\n",
      "272:\tlearn: 71.3197428\ttotal: 143ms\tremaining: 119ms\n",
      "273:\tlearn: 71.0687558\ttotal: 143ms\tremaining: 118ms\n",
      "274:\tlearn: 70.8189982\ttotal: 144ms\tremaining: 118ms\n",
      "275:\tlearn: 70.6661812\ttotal: 144ms\tremaining: 117ms\n",
      "276:\tlearn: 70.1732740\ttotal: 145ms\tremaining: 117ms\n",
      "277:\tlearn: 70.0431766\ttotal: 145ms\tremaining: 116ms\n",
      "278:\tlearn: 69.7694103\ttotal: 146ms\tremaining: 116ms\n",
      "279:\tlearn: 69.6208862\ttotal: 146ms\tremaining: 115ms\n",
      "280:\tlearn: 69.5050135\ttotal: 147ms\tremaining: 115ms\n",
      "281:\tlearn: 69.0100859\ttotal: 148ms\tremaining: 114ms\n",
      "282:\tlearn: 68.6422009\ttotal: 148ms\tremaining: 114ms\n",
      "283:\tlearn: 68.3768057\ttotal: 149ms\tremaining: 113ms\n",
      "284:\tlearn: 68.3332496\ttotal: 149ms\tremaining: 113ms\n",
      "285:\tlearn: 68.1743413\ttotal: 150ms\tremaining: 112ms\n",
      "286:\tlearn: 67.8423595\ttotal: 150ms\tremaining: 111ms\n",
      "287:\tlearn: 67.3127183\ttotal: 151ms\tremaining: 111ms\n",
      "288:\tlearn: 66.9813147\ttotal: 151ms\tremaining: 110ms\n",
      "289:\tlearn: 66.4327557\ttotal: 152ms\tremaining: 110ms\n",
      "290:\tlearn: 66.3482532\ttotal: 152ms\tremaining: 109ms\n",
      "291:\tlearn: 65.9876837\ttotal: 153ms\tremaining: 109ms\n",
      "292:\tlearn: 65.8758275\ttotal: 153ms\tremaining: 108ms\n",
      "293:\tlearn: 65.5861847\ttotal: 154ms\tremaining: 108ms\n",
      "294:\tlearn: 65.4930432\ttotal: 154ms\tremaining: 107ms\n",
      "295:\tlearn: 65.1618533\ttotal: 155ms\tremaining: 107ms\n",
      "296:\tlearn: 64.6992356\ttotal: 155ms\tremaining: 106ms\n",
      "297:\tlearn: 64.3546576\ttotal: 156ms\tremaining: 106ms\n",
      "298:\tlearn: 64.0005344\ttotal: 156ms\tremaining: 105ms\n",
      "299:\tlearn: 63.6666452\ttotal: 157ms\tremaining: 104ms\n",
      "300:\tlearn: 63.6074089\ttotal: 157ms\tremaining: 104ms\n",
      "301:\tlearn: 63.2788674\ttotal: 158ms\tremaining: 103ms\n",
      "302:\tlearn: 63.0411169\ttotal: 158ms\tremaining: 103ms\n",
      "303:\tlearn: 62.9591410\ttotal: 159ms\tremaining: 102ms\n",
      "304:\tlearn: 62.7137415\ttotal: 159ms\tremaining: 102ms\n",
      "305:\tlearn: 62.5316010\ttotal: 160ms\tremaining: 101ms\n",
      "306:\tlearn: 62.3739026\ttotal: 160ms\tremaining: 101ms\n",
      "307:\tlearn: 62.0769174\ttotal: 161ms\tremaining: 100ms\n",
      "308:\tlearn: 61.9750676\ttotal: 161ms\tremaining: 99.6ms\n",
      "309:\tlearn: 61.8979034\ttotal: 162ms\tremaining: 99.1ms\n",
      "310:\tlearn: 61.7684190\ttotal: 162ms\tremaining: 98.6ms\n",
      "311:\tlearn: 61.6578513\ttotal: 163ms\tremaining: 98ms\n",
      "312:\tlearn: 61.3767528\ttotal: 163ms\tremaining: 97.5ms\n",
      "313:\tlearn: 61.1783833\ttotal: 164ms\tremaining: 96.9ms\n",
      "314:\tlearn: 61.0732310\ttotal: 164ms\tremaining: 96.4ms\n",
      "315:\tlearn: 61.0025169\ttotal: 165ms\tremaining: 95.9ms\n",
      "316:\tlearn: 60.7820500\ttotal: 165ms\tremaining: 95.3ms\n",
      "317:\tlearn: 60.6740259\ttotal: 166ms\tremaining: 94.8ms\n",
      "318:\tlearn: 60.2092818\ttotal: 166ms\tremaining: 94.2ms\n",
      "319:\tlearn: 59.9676607\ttotal: 167ms\tremaining: 93.7ms\n",
      "320:\tlearn: 59.6776466\ttotal: 167ms\tremaining: 93.2ms\n",
      "321:\tlearn: 59.4956730\ttotal: 168ms\tremaining: 92.6ms\n",
      "322:\tlearn: 59.4448335\ttotal: 168ms\tremaining: 92.1ms\n",
      "323:\tlearn: 59.0209835\ttotal: 169ms\tremaining: 91.5ms\n",
      "324:\tlearn: 58.7971974\ttotal: 169ms\tremaining: 91ms\n",
      "325:\tlearn: 58.7331483\ttotal: 170ms\tremaining: 90.5ms\n",
      "326:\tlearn: 58.4005758\ttotal: 170ms\tremaining: 90ms\n",
      "327:\tlearn: 58.2758467\ttotal: 171ms\tremaining: 89.4ms\n",
      "328:\tlearn: 58.0798282\ttotal: 171ms\tremaining: 88.9ms\n",
      "329:\tlearn: 57.8549200\ttotal: 171ms\tremaining: 88.3ms\n",
      "330:\tlearn: 57.7955768\ttotal: 172ms\tremaining: 87.8ms\n",
      "331:\tlearn: 57.5898722\ttotal: 172ms\tremaining: 87.2ms\n",
      "332:\tlearn: 57.4686106\ttotal: 173ms\tremaining: 86.7ms\n",
      "333:\tlearn: 57.3638604\ttotal: 173ms\tremaining: 86.1ms\n",
      "334:\tlearn: 57.0495438\ttotal: 174ms\tremaining: 85.6ms\n",
      "335:\tlearn: 56.9822903\ttotal: 174ms\tremaining: 85.1ms\n",
      "336:\tlearn: 56.5452024\ttotal: 175ms\tremaining: 84.5ms\n",
      "337:\tlearn: 56.3323491\ttotal: 175ms\tremaining: 84ms\n",
      "338:\tlearn: 56.0220510\ttotal: 176ms\tremaining: 83.4ms\n",
      "339:\tlearn: 55.7787096\ttotal: 176ms\tremaining: 82.9ms\n",
      "340:\tlearn: 55.6484232\ttotal: 177ms\tremaining: 82.4ms\n",
      "341:\tlearn: 55.4338148\ttotal: 177ms\tremaining: 81.8ms\n",
      "342:\tlearn: 55.0858638\ttotal: 178ms\tremaining: 81.3ms\n",
      "343:\tlearn: 54.9569587\ttotal: 178ms\tremaining: 80.8ms\n",
      "344:\tlearn: 54.7917456\ttotal: 179ms\tremaining: 80.3ms\n",
      "345:\tlearn: 54.4071369\ttotal: 179ms\tremaining: 79.7ms\n",
      "346:\tlearn: 54.3772820\ttotal: 180ms\tremaining: 79.2ms\n",
      "347:\tlearn: 54.3370623\ttotal: 180ms\tremaining: 78.7ms\n",
      "348:\tlearn: 54.3033083\ttotal: 181ms\tremaining: 78.2ms\n",
      "349:\tlearn: 54.1939384\ttotal: 181ms\tremaining: 77.7ms\n",
      "350:\tlearn: 54.1254773\ttotal: 182ms\tremaining: 77.1ms\n",
      "351:\tlearn: 53.8022825\ttotal: 182ms\tremaining: 76.6ms\n",
      "352:\tlearn: 53.7347188\ttotal: 183ms\tremaining: 76.1ms\n",
      "353:\tlearn: 53.5597464\ttotal: 183ms\tremaining: 75.5ms\n",
      "354:\tlearn: 53.4728881\ttotal: 184ms\tremaining: 75ms\n",
      "355:\tlearn: 53.2070513\ttotal: 184ms\tremaining: 74.4ms\n",
      "356:\tlearn: 53.1435385\ttotal: 185ms\tremaining: 73.9ms\n",
      "357:\tlearn: 52.9032102\ttotal: 185ms\tremaining: 73.4ms\n",
      "358:\tlearn: 52.5974077\ttotal: 185ms\tremaining: 72.8ms\n",
      "359:\tlearn: 52.5077032\ttotal: 186ms\tremaining: 72.3ms\n",
      "360:\tlearn: 52.3581504\ttotal: 186ms\tremaining: 71.8ms\n",
      "361:\tlearn: 52.1542910\ttotal: 187ms\tremaining: 71.3ms\n",
      "362:\tlearn: 52.1279770\ttotal: 187ms\tremaining: 70.7ms\n",
      "363:\tlearn: 51.9011332\ttotal: 188ms\tremaining: 70.2ms\n",
      "364:\tlearn: 51.7253802\ttotal: 188ms\tremaining: 69.7ms\n",
      "365:\tlearn: 51.5737297\ttotal: 189ms\tremaining: 69.1ms\n",
      "366:\tlearn: 51.3991199\ttotal: 189ms\tremaining: 68.6ms\n",
      "367:\tlearn: 51.2136778\ttotal: 190ms\tremaining: 68.2ms\n",
      "368:\tlearn: 50.9116194\ttotal: 191ms\tremaining: 67.7ms\n",
      "369:\tlearn: 50.5812184\ttotal: 199ms\tremaining: 69.8ms\n",
      "370:\tlearn: 50.3851623\ttotal: 199ms\tremaining: 69.2ms\n",
      "371:\tlearn: 50.3680766\ttotal: 200ms\tremaining: 68.7ms\n",
      "372:\tlearn: 50.2174626\ttotal: 200ms\tremaining: 68.1ms\n",
      "373:\tlearn: 50.0191275\ttotal: 200ms\tremaining: 67.5ms\n",
      "374:\tlearn: 49.7910846\ttotal: 201ms\tremaining: 67ms\n",
      "375:\tlearn: 49.7742294\ttotal: 201ms\tremaining: 66.4ms\n",
      "376:\tlearn: 49.5195849\ttotal: 202ms\tremaining: 65.8ms\n",
      "377:\tlearn: 49.3468361\ttotal: 202ms\tremaining: 65.3ms\n",
      "378:\tlearn: 49.1501504\ttotal: 203ms\tremaining: 64.7ms\n",
      "379:\tlearn: 49.1194514\ttotal: 203ms\tremaining: 64.2ms\n",
      "380:\tlearn: 48.7726669\ttotal: 204ms\tremaining: 63.6ms\n",
      "381:\tlearn: 48.5864231\ttotal: 204ms\tremaining: 63ms\n",
      "382:\tlearn: 48.2689765\ttotal: 205ms\tremaining: 62.5ms\n",
      "383:\tlearn: 48.2051342\ttotal: 205ms\tremaining: 61.9ms\n",
      "384:\tlearn: 47.9522633\ttotal: 205ms\tremaining: 61.4ms\n",
      "385:\tlearn: 47.7219703\ttotal: 206ms\tremaining: 60.8ms\n",
      "386:\tlearn: 47.5299709\ttotal: 206ms\tremaining: 60.3ms\n",
      "387:\tlearn: 47.3351233\ttotal: 207ms\tremaining: 59.7ms\n",
      "388:\tlearn: 47.0934093\ttotal: 207ms\tremaining: 59.2ms\n",
      "389:\tlearn: 46.8149383\ttotal: 208ms\tremaining: 58.6ms\n",
      "390:\tlearn: 46.6212648\ttotal: 208ms\tremaining: 58.1ms\n",
      "391:\tlearn: 46.5770370\ttotal: 209ms\tremaining: 57.5ms\n",
      "392:\tlearn: 46.3118327\ttotal: 209ms\tremaining: 57ms\n",
      "393:\tlearn: 46.1542966\ttotal: 210ms\tremaining: 56.4ms\n",
      "394:\tlearn: 45.9975827\ttotal: 210ms\tremaining: 55.9ms\n",
      "395:\tlearn: 45.8583890\ttotal: 211ms\tremaining: 55.3ms\n",
      "396:\tlearn: 45.7478313\ttotal: 211ms\tremaining: 54.8ms\n",
      "397:\tlearn: 45.4602926\ttotal: 212ms\tremaining: 54.2ms\n",
      "398:\tlearn: 45.3934359\ttotal: 212ms\tremaining: 53.7ms\n",
      "399:\tlearn: 45.2550581\ttotal: 212ms\tremaining: 53.1ms\n",
      "400:\tlearn: 45.2412958\ttotal: 213ms\tremaining: 52.6ms\n",
      "401:\tlearn: 45.0844973\ttotal: 213ms\tremaining: 52ms\n",
      "402:\tlearn: 45.0278248\ttotal: 214ms\tremaining: 51.5ms\n",
      "403:\tlearn: 44.9089215\ttotal: 214ms\tremaining: 50.9ms\n",
      "404:\tlearn: 44.7926454\ttotal: 215ms\tremaining: 50.4ms\n",
      "405:\tlearn: 44.6260040\ttotal: 215ms\tremaining: 49.8ms\n",
      "406:\tlearn: 44.4428807\ttotal: 216ms\tremaining: 49.3ms\n",
      "407:\tlearn: 44.3463031\ttotal: 216ms\tremaining: 48.7ms\n",
      "408:\tlearn: 44.1955654\ttotal: 216ms\tremaining: 48.2ms\n",
      "409:\tlearn: 44.1292127\ttotal: 217ms\tremaining: 47.6ms\n",
      "410:\tlearn: 43.8504857\ttotal: 217ms\tremaining: 47.1ms\n",
      "411:\tlearn: 43.7811415\ttotal: 218ms\tremaining: 46.5ms\n",
      "412:\tlearn: 43.7110419\ttotal: 218ms\tremaining: 46ms\n",
      "413:\tlearn: 43.3414580\ttotal: 219ms\tremaining: 45.5ms\n",
      "414:\tlearn: 43.2247379\ttotal: 219ms\tremaining: 44.9ms\n",
      "415:\tlearn: 43.1877321\ttotal: 220ms\tremaining: 44.4ms\n",
      "416:\tlearn: 43.0227168\ttotal: 220ms\tremaining: 43.8ms\n",
      "417:\tlearn: 42.9704490\ttotal: 221ms\tremaining: 43.3ms\n",
      "418:\tlearn: 42.6970878\ttotal: 221ms\tremaining: 42.7ms\n",
      "419:\tlearn: 42.5722431\ttotal: 222ms\tremaining: 42.2ms\n",
      "420:\tlearn: 42.3328395\ttotal: 222ms\tremaining: 41.7ms\n",
      "421:\tlearn: 42.2273209\ttotal: 222ms\tremaining: 41.1ms\n",
      "422:\tlearn: 42.1223611\ttotal: 223ms\tremaining: 40.6ms\n",
      "423:\tlearn: 42.0050276\ttotal: 223ms\tremaining: 40ms\n",
      "424:\tlearn: 41.8083350\ttotal: 224ms\tremaining: 39.5ms\n",
      "425:\tlearn: 41.7120403\ttotal: 224ms\tremaining: 39ms\n",
      "426:\tlearn: 41.6577438\ttotal: 225ms\tremaining: 38.4ms\n",
      "427:\tlearn: 41.4851951\ttotal: 225ms\tremaining: 37.9ms\n",
      "428:\tlearn: 41.2854259\ttotal: 226ms\tremaining: 37.3ms\n",
      "429:\tlearn: 41.0273750\ttotal: 226ms\tremaining: 36.8ms\n",
      "430:\tlearn: 40.9686735\ttotal: 226ms\tremaining: 36.3ms\n",
      "431:\tlearn: 40.9166912\ttotal: 227ms\tremaining: 35.7ms\n",
      "432:\tlearn: 40.7316158\ttotal: 227ms\tremaining: 35.2ms\n",
      "433:\tlearn: 40.6026013\ttotal: 228ms\tremaining: 34.6ms\n",
      "434:\tlearn: 40.4972825\ttotal: 228ms\tremaining: 34.1ms\n",
      "435:\tlearn: 40.3560751\ttotal: 229ms\tremaining: 33.6ms\n",
      "436:\tlearn: 40.2746545\ttotal: 229ms\tremaining: 33ms\n",
      "437:\tlearn: 40.0657619\ttotal: 230ms\tremaining: 32.5ms\n",
      "438:\tlearn: 39.7902684\ttotal: 230ms\tremaining: 32ms\n",
      "439:\tlearn: 39.6444690\ttotal: 231ms\tremaining: 31.4ms\n",
      "440:\tlearn: 39.4605197\ttotal: 231ms\tremaining: 30.9ms\n",
      "441:\tlearn: 39.1422333\ttotal: 231ms\tremaining: 30.4ms\n",
      "442:\tlearn: 38.9665502\ttotal: 232ms\tremaining: 29.8ms\n",
      "443:\tlearn: 38.7832843\ttotal: 232ms\tremaining: 29.3ms\n",
      "444:\tlearn: 38.7390927\ttotal: 233ms\tremaining: 28.8ms\n",
      "445:\tlearn: 38.6860965\ttotal: 233ms\tremaining: 28.2ms\n",
      "446:\tlearn: 38.5844063\ttotal: 234ms\tremaining: 27.7ms\n",
      "447:\tlearn: 38.5389111\ttotal: 234ms\tremaining: 27.2ms\n",
      "448:\tlearn: 38.3759944\ttotal: 235ms\tremaining: 26.6ms\n",
      "449:\tlearn: 38.1665760\ttotal: 235ms\tremaining: 26.1ms\n",
      "450:\tlearn: 37.9451332\ttotal: 236ms\tremaining: 25.6ms\n",
      "451:\tlearn: 37.8993310\ttotal: 236ms\tremaining: 25.1ms\n",
      "452:\tlearn: 37.6861011\ttotal: 236ms\tremaining: 24.5ms\n",
      "453:\tlearn: 37.6039560\ttotal: 237ms\tremaining: 24ms\n",
      "454:\tlearn: 37.5653560\ttotal: 237ms\tremaining: 23.5ms\n",
      "455:\tlearn: 37.4697682\ttotal: 238ms\tremaining: 22.9ms\n",
      "456:\tlearn: 37.3055112\ttotal: 238ms\tremaining: 22.4ms\n",
      "457:\tlearn: 37.2268878\ttotal: 239ms\tremaining: 21.9ms\n",
      "458:\tlearn: 37.1198921\ttotal: 239ms\tremaining: 21.4ms\n",
      "459:\tlearn: 36.9202265\ttotal: 240ms\tremaining: 20.8ms\n",
      "460:\tlearn: 36.7772486\ttotal: 240ms\tremaining: 20.3ms\n",
      "461:\tlearn: 36.7110138\ttotal: 240ms\tremaining: 19.8ms\n",
      "462:\tlearn: 36.5340518\ttotal: 241ms\tremaining: 19.3ms\n",
      "463:\tlearn: 36.4899969\ttotal: 241ms\tremaining: 18.7ms\n",
      "464:\tlearn: 36.4473891\ttotal: 242ms\tremaining: 18.2ms\n",
      "465:\tlearn: 36.3373210\ttotal: 242ms\tremaining: 17.7ms\n",
      "466:\tlearn: 36.2492890\ttotal: 243ms\tremaining: 17.2ms\n",
      "467:\tlearn: 36.0007624\ttotal: 243ms\tremaining: 16.6ms\n",
      "468:\tlearn: 35.8370356\ttotal: 244ms\tremaining: 16.1ms\n",
      "469:\tlearn: 35.6884024\ttotal: 244ms\tremaining: 15.6ms\n",
      "470:\tlearn: 35.5418431\ttotal: 245ms\tremaining: 15.1ms\n",
      "471:\tlearn: 35.5139923\ttotal: 245ms\tremaining: 14.5ms\n",
      "472:\tlearn: 35.2819908\ttotal: 245ms\tremaining: 14ms\n",
      "473:\tlearn: 35.1124281\ttotal: 246ms\tremaining: 13.5ms\n",
      "474:\tlearn: 35.0019355\ttotal: 246ms\tremaining: 13ms\n",
      "475:\tlearn: 34.9665518\ttotal: 247ms\tremaining: 12.4ms\n",
      "476:\tlearn: 34.9310456\ttotal: 247ms\tremaining: 11.9ms\n",
      "477:\tlearn: 34.8000619\ttotal: 248ms\tremaining: 11.4ms\n",
      "478:\tlearn: 34.7113987\ttotal: 248ms\tremaining: 10.9ms\n",
      "479:\tlearn: 34.6544807\ttotal: 249ms\tremaining: 10.4ms\n",
      "480:\tlearn: 34.5276276\ttotal: 249ms\tremaining: 9.84ms\n",
      "481:\tlearn: 34.4828207\ttotal: 250ms\tremaining: 9.32ms\n",
      "482:\tlearn: 34.3210887\ttotal: 250ms\tremaining: 8.8ms\n",
      "483:\tlearn: 34.2394371\ttotal: 250ms\tremaining: 8.28ms\n",
      "484:\tlearn: 34.0874075\ttotal: 251ms\tremaining: 7.76ms\n",
      "485:\tlearn: 34.0613122\ttotal: 251ms\tremaining: 7.24ms\n",
      "486:\tlearn: 33.9974089\ttotal: 252ms\tremaining: 6.72ms\n",
      "487:\tlearn: 33.9541372\ttotal: 252ms\tremaining: 6.2ms\n",
      "488:\tlearn: 33.9290336\ttotal: 253ms\tremaining: 5.68ms\n",
      "489:\tlearn: 33.9026997\ttotal: 253ms\tremaining: 5.17ms\n",
      "490:\tlearn: 33.6838907\ttotal: 254ms\tremaining: 4.65ms\n",
      "491:\tlearn: 33.5171456\ttotal: 254ms\tremaining: 4.13ms\n",
      "492:\tlearn: 33.3789656\ttotal: 255ms\tremaining: 3.61ms\n",
      "493:\tlearn: 33.1952735\ttotal: 255ms\tremaining: 3.1ms\n",
      "494:\tlearn: 33.1622498\ttotal: 256ms\tremaining: 2.58ms\n",
      "495:\tlearn: 33.0263908\ttotal: 256ms\tremaining: 2.06ms\n",
      "496:\tlearn: 32.9513108\ttotal: 257ms\tremaining: 1.55ms\n",
      "497:\tlearn: 32.8539205\ttotal: 257ms\tremaining: 1.03ms\n",
      "498:\tlearn: 32.8194421\ttotal: 258ms\tremaining: 516us\n",
      "499:\tlearn: 32.7695204\ttotal: 258ms\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "240.56082365601893"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catb_params = {\"iterations\": [200,500,100],\n",
    "              \"learning_rate\": [0.01,0.1],\n",
    "              \"depth\": [3,6,8]}\n",
    "catb_cv_model = GridSearchCV(catb_model, catb_params, cv = 5, n_jobs = -1, verbose = 2).fit(X_train, y_train)\n",
    "catb_tuned = CatBoostRegressor(**catb_cv_model.best_params_).fit(X_train, y_train)                            \n",
    "y_pred = catb_tuned.predict(X_test) \n",
    "df_catb_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))                            \n",
    "df_catb_tuned_rmse                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'depth': 3, 'iterations': 500, 'learning_rate': 0.1}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catb_cv_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x1dd0ab86a90>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catb_cv_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dört Temel ve Ayarlı Modelin Karşılaştırılabilir Sonuçları"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LINEAR</th>\n",
       "      <th>RIDGE</th>\n",
       "      <th>RIDGE TUNED</th>\n",
       "      <th>LASSO</th>\n",
       "      <th>LASSO TUNED</th>\n",
       "      <th>ELASTIC NET</th>\n",
       "      <th>ELASTIC NET TUNED</th>\n",
       "      <th>KNN</th>\n",
       "      <th>KNN TUNED</th>\n",
       "      <th>SVR</th>\n",
       "      <th>...</th>\n",
       "      <th>RF</th>\n",
       "      <th>RF TUNED</th>\n",
       "      <th>GBM</th>\n",
       "      <th>GBM TUNED</th>\n",
       "      <th>XGBOOST</th>\n",
       "      <th>XGBOOST TUNED</th>\n",
       "      <th>LightGBM</th>\n",
       "      <th>LightGBM TUNED</th>\n",
       "      <th>CatBoost</th>\n",
       "      <th>CatBoost TUNED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>315.289765</td>\n",
       "      <td>315.289765</td>\n",
       "      <td>326.891053</td>\n",
       "      <td>318.680312</td>\n",
       "      <td>323.423125</td>\n",
       "      <td>302.78611</td>\n",
       "      <td>295.980567</td>\n",
       "      <td>315.07079</td>\n",
       "      <td>289.107127</td>\n",
       "      <td>278.851573</td>\n",
       "      <td>...</td>\n",
       "      <td>284.539468</td>\n",
       "      <td>261.88261</td>\n",
       "      <td>267.324796</td>\n",
       "      <td>250.915252</td>\n",
       "      <td>324.398425</td>\n",
       "      <td>277.127332</td>\n",
       "      <td>276.062536</td>\n",
       "      <td>275.418462</td>\n",
       "      <td>258.901978</td>\n",
       "      <td>240.560824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       LINEAR       RIDGE  RIDGE TUNED       LASSO  LASSO TUNED  ELASTIC NET  \\\n",
       "0  315.289765  315.289765   326.891053  318.680312   323.423125    302.78611   \n",
       "\n",
       "   ELASTIC NET TUNED        KNN   KNN TUNED         SVR  ...          RF  \\\n",
       "0         295.980567  315.07079  289.107127  278.851573  ...  284.539468   \n",
       "\n",
       "    RF TUNED         GBM   GBM TUNED     XGBOOST  XGBOOST TUNED    LightGBM  \\\n",
       "0  261.88261  267.324796  250.915252  324.398425     277.127332  276.062536   \n",
       "\n",
       "   LightGBM TUNED    CatBoost  CatBoost TUNED  \n",
       "0      275.418462  258.901978      240.560824  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ComparableResults_df = pd.DataFrame({\"LINEAR\":[df_ridreg_rmse],\"RIDGE\":[df_ridreg_rmse],\"RIDGE TUNED\":[df_ridge_tuned_rmse],\n",
    "                             \"LASSO\":[df_lasreg_rmse],\"LASSO TUNED\":[df_lasso_tuned_rmse], \n",
    "                             \"ELASTIC NET\":[df_enet_rmse], \"ELASTIC NET TUNED\":[df_enet_tuned_rmse],\n",
    "                             \"KNN\":[df_knn_rmse], \"KNN TUNED\":[df_knn_tuned_rmse],\n",
    "                             \"SVR\":[df_svr_rmse], \"SVR TUNED\":[df_svr_tuned_rmse],\n",
    "                             \"MLP\":[df_mlp_rmse], \"MLP TUNED\":[df_mlp_tuned_rmse],\n",
    "                             \"CART\":[df_cart_rmse], \"CART TUNED\":[df_cart_tuned_rmse],\n",
    "                             \"RF\":[df_rf_rmse], \"RF TUNED\":[df_rf_tuned_rmse],\n",
    "                             \"GBM\":[df_gbm_rmse], \"GBM TUNED\":[df_gbm_tuned_rmse],\n",
    "                             \"XGBOOST\":[df_xgb_rmse], \"XGBOOST TUNED\":[df_xgb_tuned_rmse],\n",
    "                             \"LightGBM\":[df_lgbm_rmse], \"LightGBM TUNED\":[df_lgbm_tuned_rmse],\n",
    "                             \"CatBoost\":[df_catb_rmse], \"CatBoost TUNED\":[df_catb_tuned_rmse]})\n",
    "\n",
    "ComparableResults_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    240.560824\n",
       "dtype: float64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ComparableResults_df.min(axis = 1, skipna = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    CatBoost TUNED\n",
       "dtype: object"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ComparableResults_df.idxmin(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LINEAR</th>\n",
       "      <td>315.289765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RIDGE</th>\n",
       "      <td>315.289765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RIDGE TUNED</th>\n",
       "      <td>326.891053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LASSO</th>\n",
       "      <td>318.680312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LASSO TUNED</th>\n",
       "      <td>323.423125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELASTIC NET</th>\n",
       "      <td>302.786110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELASTIC NET TUNED</th>\n",
       "      <td>295.980567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>315.070790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN TUNED</th>\n",
       "      <td>289.107127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>278.851573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR TUNED</th>\n",
       "      <td>278.851573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP</th>\n",
       "      <td>514.898355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP TUNED</th>\n",
       "      <td>419.632258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CART</th>\n",
       "      <td>428.180640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CART TUNED</th>\n",
       "      <td>399.635387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>284.539468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF TUNED</th>\n",
       "      <td>261.882610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBM</th>\n",
       "      <td>267.324796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBM TUNED</th>\n",
       "      <td>250.915252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBOOST</th>\n",
       "      <td>324.398425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBOOST TUNED</th>\n",
       "      <td>277.127332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBM</th>\n",
       "      <td>276.062536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBM TUNED</th>\n",
       "      <td>275.418462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoost</th>\n",
       "      <td>258.901978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoost TUNED</th>\n",
       "      <td>240.560824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0\n",
       "LINEAR             315.289765\n",
       "RIDGE              315.289765\n",
       "RIDGE TUNED        326.891053\n",
       "LASSO              318.680312\n",
       "LASSO TUNED        323.423125\n",
       "ELASTIC NET        302.786110\n",
       "ELASTIC NET TUNED  295.980567\n",
       "KNN                315.070790\n",
       "KNN TUNED          289.107127\n",
       "SVR                278.851573\n",
       "SVR TUNED          278.851573\n",
       "MLP                514.898355\n",
       "MLP TUNED          419.632258\n",
       "CART               428.180640\n",
       "CART TUNED         399.635387\n",
       "RF                 284.539468\n",
       "RF TUNED           261.882610\n",
       "GBM                267.324796\n",
       "GBM TUNED          250.915252\n",
       "XGBOOST            324.398425\n",
       "XGBOOST TUNED      277.127332\n",
       "LightGBM           276.062536\n",
       "LightGBM TUNED     275.418462\n",
       "CatBoost           258.901978\n",
       "CatBoost TUNED     240.560824"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ComparableResults_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Sonuçlar ve Çözüm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu projede, herhangi bir ABD Beyzbol Ligi oyuncusunun maaşını tahmin etmek için on üç farklı makine öğrenimi modeli kullanıldı. Doğrusal Regresyon, Ridge Regresyon, Kement Regresyon, ElasticNet Regresyon, KNN (K-En Yakın Komşular), SVR (Support Vector Regresyon), MLP (Çok Katmanlı Algılayıcı), CART (Sınıflandırma ve Regresyon Ağaçları), Random Forest (Rastgele Ormanlar), GBM (Gradient Boosting) kullanarak Machines), XGBoost (Extreme Gradient Boost), LightGBM ve CatBoost (Category Boosting) Machine Learning Modellerinde hata karelerinin kök ortalamaları (RMSE) değerleri hesaplandı. RMSE, tahminlerin gözlemlenen değerlerden ortalama sapmasının bir ölçüsüdür. Daha sonra hiperparametre optimizasyonları yardımıyla RMSE değerleri düşürülmeye çalışılmıştır. Tüm temel modeller ayarlandı. Sonuçlar, hem temel hem de ayarlanmış modelde, en düşük RMSE değerinin (258.901978 ve 240.560824), CatBoost (Kategori Yükseltme) Makine Öğrenimi modelinden elde edildiğini göstermiştir. En iyi Machine Learning modeli, 240.560824 RMSE değeri ile ayarlanmış CatBoost modeli oldu. Bu hata puanı, tahmin edilen değerin (539.2295992217898) ortalamasından oldukça uzaktır. Özetle, analizler ve tahmin sonuçları, ayarlanmış (optime edilmis) CatBoost (Kategori Arttırma) Makine Öğrenimi modelinin bir ABD Beyzbol Ligi oyuncusu maaşını tahmin etmek için en iyi model olduğunu açıkça ortaya koydu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
